{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcc0e3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8b584d",
   "metadata": {},
   "source": [
    "1) Alcohol\n",
    "2) Malic acid\n",
    "3) Ash\n",
    "4) Alcalinity of ash  \n",
    "5) Magnesium\n",
    "6) Total phenols\n",
    "7) Flavanoids\n",
    "8) Nonflavanoid phenols\n",
    "9) Proanthocyanins\n",
    "10) Color intensity\n",
    "11) Hue\n",
    "12) OD280/OD315 of diluted wines\n",
    "13) Proline            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74fffd01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2    3      4     5      6        7     8     9     10  \\\n",
       "0      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "1      7.8  0.88  0.00  2.6  0.098  25.0   67.0  0.99680  3.20  0.68   9.8   \n",
       "2      7.8  0.76  0.04  2.3  0.092  15.0   54.0  0.99700  3.26  0.65   9.8   \n",
       "3     11.2  0.28  0.56  1.9  0.075  17.0   60.0  0.99800  3.16  0.58   9.8   \n",
       "4      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "...    ...   ...   ...  ...    ...   ...    ...      ...   ...   ...   ...   \n",
       "6492   6.2  0.21  0.29  1.6  0.039  24.0   92.0  0.99114  3.27  0.50  11.2   \n",
       "6493   6.6  0.32  0.36  8.0  0.047  57.0  168.0  0.99490  3.15  0.46   9.6   \n",
       "6494   6.5  0.24  0.19  1.2  0.041  30.0  111.0  0.99254  2.99  0.46   9.4   \n",
       "6495   5.5  0.29  0.30  1.1  0.022  20.0  110.0  0.98869  3.34  0.38  12.8   \n",
       "6496   6.0  0.21  0.38  0.8  0.020  22.0   98.0  0.98941  3.26  0.32  11.8   \n",
       "\n",
       "      11  12  \n",
       "0      5   1  \n",
       "1      5   1  \n",
       "2      5   1  \n",
       "3      6   1  \n",
       "4      5   1  \n",
       "...   ..  ..  \n",
       "6492   6   0  \n",
       "6493   5   0  \n",
       "6494   6   0  \n",
       "6495   7   0  \n",
       "6496   6   0  \n",
       "\n",
       "[6497 rows x 13 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/haram4th/ADsP/main/wine.csv\", header=None)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feb8d625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6497 entries, 0 to 6496\n",
      "Data columns (total 13 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       6497 non-null   float64\n",
      " 1   1       6497 non-null   float64\n",
      " 2   2       6497 non-null   float64\n",
      " 3   3       6497 non-null   float64\n",
      " 4   4       6497 non-null   float64\n",
      " 5   5       6497 non-null   float64\n",
      " 6   6       6497 non-null   float64\n",
      " 7   7       6497 non-null   float64\n",
      " 8   8       6497 non-null   float64\n",
      " 9   9       6497 non-null   float64\n",
      " 10  10      6497 non-null   float64\n",
      " 11  11      6497 non-null   int64  \n",
      " 12  12      6497 non-null   int64  \n",
      "dtypes: float64(11), int64(2)\n",
      "memory usage: 660.0 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "864d5747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(data.iloc[:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f29150f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12\n",
       "0    4898\n",
       "1    1599\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[12].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac1fe001",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(12, axis=1)\n",
    "y = data[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15029dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5996e7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.30769231,  2.41176471, -2.21428571, ...,  0.29411765,\n",
       "        -0.5       , -1.        ],\n",
       "       [ 0.61538462,  3.47058824, -2.21428571, ...,  1.        ,\n",
       "        -0.27777778, -1.        ],\n",
       "       [ 0.61538462,  2.76470588, -1.92857143, ...,  0.82352941,\n",
       "        -0.27777778, -1.        ],\n",
       "       ...,\n",
       "       [-0.38461538, -0.29411765, -0.85714286, ..., -0.29411765,\n",
       "        -0.5       ,  0.        ],\n",
       "       [-1.15384615,  0.        , -0.07142857, ..., -0.76470588,\n",
       "         1.38888889,  1.        ],\n",
       "       [-0.76923077, -0.47058824,  0.5       , ..., -1.11764706,\n",
       "         0.83333333,  0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs = RobustScaler()\n",
    "X_scaled = rs.fit_transform(X)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f688b5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "6492    0\n",
       "6493    0\n",
       "6494    0\n",
       "6495    0\n",
       "6496    0\n",
       "Name: 12, Length: 6497, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02bb04ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e693adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_scaled, y, test_size=0.4, stratify=y, random_state=10)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=0.5, stratify=y_valid, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce0b51b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f026a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 09:08:56.002191: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-11 09:08:59.706138: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libdirectml.d6f03b303ac3c4f2eeb8ca631688c9757b361310.so\n",
      "2024-09-11 09:08:59.706310: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libdxcore.so\n",
      "2024-09-11 09:08:59.710817: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libd3d12.so\n",
      "2024-09-11 09:09:00.311922: I tensorflow/c/logging.cc:34] DirectML device enumeration: found 1 compatible adapters.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d715f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                416       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,089\n",
      "Trainable params: 1,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 09:10:25.031167: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-11 09:10:25.033317: I tensorflow/c/logging.cc:34] DirectML: creating device on adapter 0 (AMD Radeon(TM) RX Vega 11 Graphics)\n",
      "2024-09-11 09:10:25.165216: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-09-11 09:10:25.165274: W tensorflow/core/common_runtime/pluggable_device/pluggable_device_bfc_allocator.cc:28] Overriding allow_growth setting because force_memory_growth was requested by the device.\n",
      "2024-09-11 09:10:25.165301: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15603 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdfeb8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 5s - loss: 0.6446 - accuracy: 0.7380"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 09:11:10.382671: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-09-11 09:11:10.471794: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-09-11 09:11:10.471862: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15603 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 51ms/step - loss: 0.6244 - accuracy: 0.7981 - val_loss: 0.5982 - val_accuracy: 0.8676\n",
      "Epoch 2/100\n",
      "5/8 [=================>............] - ETA: 0s - loss: 0.5783 - accuracy: 0.8884"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 09:11:10.846350: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-09-11 09:11:10.882058: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-09-11 09:11:10.882126: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15603 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 22ms/step - loss: 0.5663 - accuracy: 0.9046 - val_loss: 0.5388 - val_accuracy: 0.9276\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.5030 - accuracy: 0.9569 - val_loss: 0.4720 - val_accuracy: 0.9554\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.4344 - accuracy: 0.9700 - val_loss: 0.4026 - val_accuracy: 0.9692\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.3657 - accuracy: 0.9772 - val_loss: 0.3355 - val_accuracy: 0.9769\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.3001 - accuracy: 0.9828 - val_loss: 0.2714 - val_accuracy: 0.9815\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.2387 - accuracy: 0.9859 - val_loss: 0.2131 - val_accuracy: 0.9846\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.1857 - accuracy: 0.9890 - val_loss: 0.1657 - val_accuracy: 0.9885\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.1441 - accuracy: 0.9910 - val_loss: 0.1297 - val_accuracy: 0.9915\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.1130 - accuracy: 0.9913 - val_loss: 0.1034 - val_accuracy: 0.9931\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0903 - accuracy: 0.9918 - val_loss: 0.0847 - val_accuracy: 0.9938\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0743 - accuracy: 0.9913 - val_loss: 0.0716 - val_accuracy: 0.9938\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0626 - accuracy: 0.9915 - val_loss: 0.0625 - val_accuracy: 0.9938\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0546 - accuracy: 0.9918 - val_loss: 0.0560 - val_accuracy: 0.9938\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0486 - accuracy: 0.9923 - val_loss: 0.0514 - val_accuracy: 0.9931\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0442 - accuracy: 0.9928 - val_loss: 0.0479 - val_accuracy: 0.9931\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0407 - accuracy: 0.9928 - val_loss: 0.0452 - val_accuracy: 0.9938\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.0380 - accuracy: 0.9933 - val_loss: 0.0430 - val_accuracy: 0.9938\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0358 - accuracy: 0.9936 - val_loss: 0.0414 - val_accuracy: 0.9938\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0339 - accuracy: 0.9936 - val_loss: 0.0401 - val_accuracy: 0.9938\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0324 - accuracy: 0.9938 - val_loss: 0.0390 - val_accuracy: 0.9938\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.0310 - accuracy: 0.9941 - val_loss: 0.0382 - val_accuracy: 0.9938\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0298 - accuracy: 0.9946 - val_loss: 0.0374 - val_accuracy: 0.9946\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.0288 - accuracy: 0.9946 - val_loss: 0.0367 - val_accuracy: 0.9938\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.0279 - accuracy: 0.9949 - val_loss: 0.0361 - val_accuracy: 0.9946\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0270 - accuracy: 0.9949 - val_loss: 0.0357 - val_accuracy: 0.9946\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0263 - accuracy: 0.9949 - val_loss: 0.0352 - val_accuracy: 0.9946\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0256 - accuracy: 0.9951 - val_loss: 0.0348 - val_accuracy: 0.9946\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0250 - accuracy: 0.9956 - val_loss: 0.0344 - val_accuracy: 0.9946\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0244 - accuracy: 0.9956 - val_loss: 0.0340 - val_accuracy: 0.9946\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0238 - accuracy: 0.9956 - val_loss: 0.0337 - val_accuracy: 0.9946\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0233 - accuracy: 0.9956 - val_loss: 0.0334 - val_accuracy: 0.9946\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0227 - accuracy: 0.9956 - val_loss: 0.0331 - val_accuracy: 0.9946\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0223 - accuracy: 0.9956 - val_loss: 0.0328 - val_accuracy: 0.9946\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0219 - accuracy: 0.9956 - val_loss: 0.0325 - val_accuracy: 0.9946\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0214 - accuracy: 0.9962 - val_loss: 0.0324 - val_accuracy: 0.9946\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0210 - accuracy: 0.9964 - val_loss: 0.0321 - val_accuracy: 0.9946\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0207 - accuracy: 0.9964 - val_loss: 0.0320 - val_accuracy: 0.9946\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0203 - accuracy: 0.9964 - val_loss: 0.0317 - val_accuracy: 0.9946\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0199 - accuracy: 0.9964 - val_loss: 0.0315 - val_accuracy: 0.9946\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0196 - accuracy: 0.9967 - val_loss: 0.0314 - val_accuracy: 0.9946\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0193 - accuracy: 0.9967 - val_loss: 0.0312 - val_accuracy: 0.9946\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.0190 - accuracy: 0.9967 - val_loss: 0.0311 - val_accuracy: 0.9946\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0187 - accuracy: 0.9967 - val_loss: 0.0309 - val_accuracy: 0.9946\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0185 - accuracy: 0.9969 - val_loss: 0.0308 - val_accuracy: 0.9946\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0182 - accuracy: 0.9969 - val_loss: 0.0306 - val_accuracy: 0.9946\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0179 - accuracy: 0.9969 - val_loss: 0.0304 - val_accuracy: 0.9946\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0177 - accuracy: 0.9969 - val_loss: 0.0303 - val_accuracy: 0.9946\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.0174 - accuracy: 0.9969 - val_loss: 0.0302 - val_accuracy: 0.9946\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0172 - accuracy: 0.9969 - val_loss: 0.0301 - val_accuracy: 0.9954\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.0170 - accuracy: 0.9969 - val_loss: 0.0300 - val_accuracy: 0.9954\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.0168 - accuracy: 0.9969 - val_loss: 0.0299 - val_accuracy: 0.9954\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0165 - accuracy: 0.9972 - val_loss: 0.0298 - val_accuracy: 0.9954\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.0163 - accuracy: 0.9972 - val_loss: 0.0296 - val_accuracy: 0.9954\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0162 - accuracy: 0.9972 - val_loss: 0.0294 - val_accuracy: 0.9954\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.0160 - accuracy: 0.9972 - val_loss: 0.0294 - val_accuracy: 0.9954\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0158 - accuracy: 0.9972 - val_loss: 0.0294 - val_accuracy: 0.9954\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0156 - accuracy: 0.9972 - val_loss: 0.0293 - val_accuracy: 0.9954\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.0154 - accuracy: 0.9972 - val_loss: 0.0292 - val_accuracy: 0.9954\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0152 - accuracy: 0.9972 - val_loss: 0.0291 - val_accuracy: 0.9954\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0150 - accuracy: 0.9972 - val_loss: 0.0289 - val_accuracy: 0.9954\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0149 - accuracy: 0.9972 - val_loss: 0.0288 - val_accuracy: 0.9954\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0147 - accuracy: 0.9972 - val_loss: 0.0288 - val_accuracy: 0.9954\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0145 - accuracy: 0.9972 - val_loss: 0.0287 - val_accuracy: 0.9954\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0143 - accuracy: 0.9972 - val_loss: 0.0286 - val_accuracy: 0.9954\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0142 - accuracy: 0.9972 - val_loss: 0.0285 - val_accuracy: 0.9954\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0141 - accuracy: 0.9972 - val_loss: 0.0286 - val_accuracy: 0.9954\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0139 - accuracy: 0.9972 - val_loss: 0.0286 - val_accuracy: 0.9954\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0138 - accuracy: 0.9972 - val_loss: 0.0285 - val_accuracy: 0.9954\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.0136 - accuracy: 0.9972 - val_loss: 0.0285 - val_accuracy: 0.9954\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0135 - accuracy: 0.9972 - val_loss: 0.0284 - val_accuracy: 0.9954\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0133 - accuracy: 0.9972 - val_loss: 0.0283 - val_accuracy: 0.9954\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0132 - accuracy: 0.9972 - val_loss: 0.0283 - val_accuracy: 0.9954\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0131 - accuracy: 0.9972 - val_loss: 0.0283 - val_accuracy: 0.9954\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0129 - accuracy: 0.9972 - val_loss: 0.0282 - val_accuracy: 0.9954\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0127 - accuracy: 0.9972 - val_loss: 0.0282 - val_accuracy: 0.9954\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0126 - accuracy: 0.9972 - val_loss: 0.0280 - val_accuracy: 0.9954\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0125 - accuracy: 0.9972 - val_loss: 0.0281 - val_accuracy: 0.9954\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0123 - accuracy: 0.9974 - val_loss: 0.0280 - val_accuracy: 0.9954\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0122 - accuracy: 0.9974 - val_loss: 0.0278 - val_accuracy: 0.9962\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0121 - accuracy: 0.9972 - val_loss: 0.0278 - val_accuracy: 0.9962\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0119 - accuracy: 0.9972 - val_loss: 0.0277 - val_accuracy: 0.9962\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0118 - accuracy: 0.9972 - val_loss: 0.0278 - val_accuracy: 0.9962\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0116 - accuracy: 0.9974 - val_loss: 0.0278 - val_accuracy: 0.9962\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0116 - accuracy: 0.9972 - val_loss: 0.0278 - val_accuracy: 0.9962\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0114 - accuracy: 0.9977 - val_loss: 0.0279 - val_accuracy: 0.9962\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0113 - accuracy: 0.9977 - val_loss: 0.0278 - val_accuracy: 0.9962\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0112 - accuracy: 0.9977 - val_loss: 0.0278 - val_accuracy: 0.9962\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.0110 - accuracy: 0.9977 - val_loss: 0.0277 - val_accuracy: 0.9962\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.0109 - accuracy: 0.9977 - val_loss: 0.0276 - val_accuracy: 0.9962\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.0108 - accuracy: 0.9977 - val_loss: 0.0275 - val_accuracy: 0.9962\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.0107 - accuracy: 0.9977 - val_loss: 0.0276 - val_accuracy: 0.9962\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.0105 - accuracy: 0.9977 - val_loss: 0.0277 - val_accuracy: 0.9962\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.0104 - accuracy: 0.9977 - val_loss: 0.0276 - val_accuracy: 0.9962\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0103 - accuracy: 0.9977 - val_loss: 0.0276 - val_accuracy: 0.9962\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0102 - accuracy: 0.9977 - val_loss: 0.0277 - val_accuracy: 0.9962\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0101 - accuracy: 0.9979 - val_loss: 0.0278 - val_accuracy: 0.9962\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0099 - accuracy: 0.9982 - val_loss: 0.0276 - val_accuracy: 0.9962\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0098 - accuracy: 0.9982 - val_loss: 0.0275 - val_accuracy: 0.9962\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0097 - accuracy: 0.9982 - val_loss: 0.0276 - val_accuracy: 0.9962\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=500, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab09803e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 09:12:30.070368: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-09-11 09:12:30.270932: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       980\n",
      "           1       0.99      0.99      0.99       320\n",
      "\n",
      "    accuracy                           1.00      1300\n",
      "   macro avg       0.99      0.99      0.99      1300\n",
      "weighted avg       1.00      1.00      1.00      1300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 09:12:30.271051: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15603 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-09-11 09:12:30.313366: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-09-11 09:12:30.313455: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15603 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-09-11 09:12:30.319240: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-09-11 09:12:30.319308: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15603 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred = pd.DataFrame(pred)\n",
    "pred = pred[0].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9ad6447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "786dafa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend(['train', 'valid'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da023ad7",
   "metadata": {},
   "source": [
    "# EarlyStopping으로 학습 조기 중단 및 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c95e43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01b8d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd4b0bf",
   "metadata": {},
   "source": [
    "# ModelCheckpoint\n",
    "* 모델을 중간에 저장하는 옵션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc53574e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /model already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.exists(\"./model\"):\n",
    "    os.makedirs(\"./model\")\n",
    "    print(f\"Directory /model created.\")\n",
    "else:\n",
    "    print(f\"Directory /model already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d889e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a04dae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = \"./model/{epoch:03d}--{val_loss:.4f}.keras\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=0, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28ab4eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.0095 - accuracy: 0.9982 - val_loss: 0.0276 - val_accuracy: 0.9962\n",
      "Epoch 2/1000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0095 - accuracy: 0.9982 - val_loss: 0.0276 - val_accuracy: 0.9962\n",
      "Epoch 3/1000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0093 - accuracy: 0.9982 - val_loss: 0.0276 - val_accuracy: 0.9962\n",
      "Epoch 4/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.0092 - accuracy: 0.9982 - val_loss: 0.0275 - val_accuracy: 0.9962\n",
      "Epoch 5/1000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0090 - accuracy: 0.9982 - val_loss: 0.0276 - val_accuracy: 0.9962\n",
      "Epoch 6/1000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0089 - accuracy: 0.9982 - val_loss: 0.0277 - val_accuracy: 0.9962\n",
      "Epoch 7/1000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0088 - accuracy: 0.9982 - val_loss: 0.0277 - val_accuracy: 0.9962\n",
      "Epoch 8/1000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0086 - accuracy: 0.9982 - val_loss: 0.0278 - val_accuracy: 0.9962\n",
      "Epoch 9/1000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0277 - val_accuracy: 0.9962\n",
      "Epoch 10/1000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0085 - accuracy: 0.9987 - val_loss: 0.0279 - val_accuracy: 0.9962\n",
      "Epoch 11/1000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0084 - accuracy: 0.9987 - val_loss: 0.0277 - val_accuracy: 0.9962\n",
      "Epoch 12/1000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0083 - accuracy: 0.9987 - val_loss: 0.0276 - val_accuracy: 0.9962\n",
      "Epoch 13/1000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0082 - accuracy: 0.9987 - val_loss: 0.0280 - val_accuracy: 0.9962\n",
      "Epoch 14/1000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0081 - accuracy: 0.9987 - val_loss: 0.0280 - val_accuracy: 0.9962\n",
      "Epoch 15/1000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0080 - accuracy: 0.9987 - val_loss: 0.0279 - val_accuracy: 0.9962\n",
      "Epoch 16/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0080 - accuracy: 0.9987 - val_loss: 0.0277 - val_accuracy: 0.9962\n",
      "Epoch 17/1000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0079 - accuracy: 0.9987 - val_loss: 0.0280 - val_accuracy: 0.9962\n",
      "Epoch 18/1000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0078 - accuracy: 0.9987 - val_loss: 0.0280 - val_accuracy: 0.9962\n",
      "Epoch 19/1000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0078 - accuracy: 0.9987 - val_loss: 0.0279 - val_accuracy: 0.9962\n",
      "Epoch 20/1000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0077 - accuracy: 0.9987 - val_loss: 0.0279 - val_accuracy: 0.9962\n",
      "Epoch 21/1000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0076 - accuracy: 0.9987 - val_loss: 0.0282 - val_accuracy: 0.9962\n",
      "Epoch 22/1000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0075 - accuracy: 0.9987 - val_loss: 0.0283 - val_accuracy: 0.9962\n",
      "Epoch 23/1000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0075 - accuracy: 0.9987 - val_loss: 0.0282 - val_accuracy: 0.9962\n",
      "Epoch 24/1000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0074 - accuracy: 0.9987 - val_loss: 0.0280 - val_accuracy: 0.9962\n",
      "Epoch 25/1000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0074 - accuracy: 0.9987 - val_loss: 0.0280 - val_accuracy: 0.9962\n",
      "Epoch 26/1000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0073 - accuracy: 0.9987 - val_loss: 0.0282 - val_accuracy: 0.9962\n",
      "Epoch 27/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0072 - accuracy: 0.9987 - val_loss: 0.0282 - val_accuracy: 0.9962\n",
      "Epoch 28/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0071 - accuracy: 0.9987 - val_loss: 0.0282 - val_accuracy: 0.9962\n",
      "Epoch 29/1000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0071 - accuracy: 0.9987 - val_loss: 0.0283 - val_accuracy: 0.9962\n",
      "Epoch 30/1000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0070 - accuracy: 0.9987 - val_loss: 0.0281 - val_accuracy: 0.9962\n",
      "Epoch 31/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.0070 - accuracy: 0.9987 - val_loss: 0.0282 - val_accuracy: 0.9962\n",
      "Epoch 32/1000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0069 - accuracy: 0.9987 - val_loss: 0.0283 - val_accuracy: 0.9962\n",
      "Epoch 33/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0068 - accuracy: 0.9987 - val_loss: 0.0283 - val_accuracy: 0.9962\n",
      "Epoch 34/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.0068 - accuracy: 0.9987 - val_loss: 0.0283 - val_accuracy: 0.9962\n",
      "Epoch 35/1000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0067 - accuracy: 0.9987 - val_loss: 0.0282 - val_accuracy: 0.9962\n",
      "Epoch 36/1000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0067 - accuracy: 0.9987 - val_loss: 0.0283 - val_accuracy: 0.9962\n",
      "Epoch 37/1000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0066 - accuracy: 0.9987 - val_loss: 0.0285 - val_accuracy: 0.9962\n",
      "Epoch 38/1000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0066 - accuracy: 0.9987 - val_loss: 0.0286 - val_accuracy: 0.9962\n",
      "Epoch 39/1000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0065 - accuracy: 0.9987 - val_loss: 0.0284 - val_accuracy: 0.9962\n",
      "Epoch 40/1000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0065 - accuracy: 0.9987 - val_loss: 0.0284 - val_accuracy: 0.9962\n",
      "Epoch 41/1000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0064 - accuracy: 0.9987 - val_loss: 0.0284 - val_accuracy: 0.9962\n",
      "Epoch 42/1000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0064 - accuracy: 0.9987 - val_loss: 0.0284 - val_accuracy: 0.9962\n",
      "Epoch 43/1000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0063 - accuracy: 0.9987 - val_loss: 0.0286 - val_accuracy: 0.9962\n",
      "Epoch 44/1000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0062 - accuracy: 0.9987 - val_loss: 0.0286 - val_accuracy: 0.9962\n",
      "Epoch 45/1000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0062 - accuracy: 0.9987 - val_loss: 0.0285 - val_accuracy: 0.9962\n",
      "Epoch 46/1000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0062 - accuracy: 0.9987 - val_loss: 0.0286 - val_accuracy: 0.9962\n",
      "Epoch 47/1000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0061 - accuracy: 0.9987 - val_loss: 0.0286 - val_accuracy: 0.9962\n",
      "Epoch 48/1000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0061 - accuracy: 0.9987 - val_loss: 0.0288 - val_accuracy: 0.9962\n",
      "Epoch 49/1000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0060 - accuracy: 0.9987 - val_loss: 0.0287 - val_accuracy: 0.9962\n",
      "Epoch 50/1000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0060 - accuracy: 0.9987 - val_loss: 0.0286 - val_accuracy: 0.9962\n",
      "Epoch 51/1000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0059 - accuracy: 0.9987 - val_loss: 0.0288 - val_accuracy: 0.9962\n",
      "Epoch 52/1000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0059 - accuracy: 0.9987 - val_loss: 0.0287 - val_accuracy: 0.9962\n",
      "Epoch 53/1000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.0058 - accuracy: 0.9987 - val_loss: 0.0286 - val_accuracy: 0.9962\n",
      "Epoch 54/1000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0058 - accuracy: 0.9987 - val_loss: 0.0288 - val_accuracy: 0.9962\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=1000, batch_size=500, validation_data=(X_valid, y_valid), \n",
    "                   callbacks=[early_stopping_callback, checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b7790f",
   "metadata": {},
   "source": [
    "# 저장된 베스트 모델을 불러와서 테스트 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a33e4706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3120afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model(\"./model/004--0.0275.keras\")  # .h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe24fdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 09:29:16.728086: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       980\n",
      "           1       0.99      0.99      0.99       320\n",
      "\n",
      "    accuracy                           1.00      1300\n",
      "   macro avg       0.99      0.99      0.99      1300\n",
      "weighted avg       1.00      1.00      1.00      1300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_pred = best_model.predict(X_test)\n",
    "best_pred = pd.DataFrame(best_pred)\n",
    "best_pred = best_pred[0].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "print(classification_report(y_test, best_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ae643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078aa075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ce6652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f09593e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = pd.read_csv(\"../06machine_learning/data/winequality-white.csv\", sep=\";\")\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3361084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quality\n",
       "6    2198\n",
       "5    1457\n",
       "7     880\n",
       "8     175\n",
       "4     163\n",
       "3      20\n",
       "9       5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e4c612e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine['quality'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f316f41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4898 entries, 0 to 4897\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         4898 non-null   float64\n",
      " 1   volatile acidity      4898 non-null   float64\n",
      " 2   citric acid           4898 non-null   float64\n",
      " 3   residual sugar        4898 non-null   float64\n",
      " 4   chlorides             4898 non-null   float64\n",
      " 5   free sulfur dioxide   4898 non-null   float64\n",
      " 6   total sulfur dioxide  4898 non-null   float64\n",
      " 7   density               4898 non-null   float64\n",
      " 8   pH                    4898 non-null   float64\n",
      " 9   sulphates             4898 non-null   float64\n",
      " 10  alcohol               4898 non-null   float64\n",
      " 11  quality               4898 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 459.3 KB\n"
     ]
    }
   ],
   "source": [
    "wine.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2110f494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(wine.iloc[:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7a1902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2dea8133",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wine.drop('quality', axis=1).copy()\n",
    "y = wine['quality'].copy()\n",
    "y2 = wine['quality'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42d0e8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  \n",
       "0      8.8  \n",
       "1      9.5  \n",
       "2     10.1  \n",
       "3      9.9  \n",
       "4      9.9  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d4bc6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6\n",
       "1    6\n",
       "2    6\n",
       "3    6\n",
       "4    6\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "441fdadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbs = RobustScaler()\n",
    "X_scaled = rbs.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2fdee19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          3      4      5      6      7      8      9\n",
       "0     False  False  False   True  False  False  False\n",
       "1     False  False  False   True  False  False  False\n",
       "2     False  False  False   True  False  False  False\n",
       "3     False  False  False   True  False  False  False\n",
       "4     False  False  False   True  False  False  False\n",
       "...     ...    ...    ...    ...    ...    ...    ...\n",
       "4893  False  False  False   True  False  False  False\n",
       "4894  False  False   True  False  False  False  False\n",
       "4895  False  False  False   True  False  False  False\n",
       "4896  False  False  False  False   True  False  False\n",
       "4897  False  False  False   True  False  False  False\n",
       "\n",
       "[4898 rows x 7 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y = pd.get_dummies(y)\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dac84769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dbdd39ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_scaled, y, test_size=0.4, stratify=y, random_state=10)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=0.5, stratify=y_valid, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ece8259",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2938, 11) (980, 11) (980, 11)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_valid.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebc4e59a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       6\n",
       "1       6\n",
       "2       6\n",
       "3       6\n",
       "4       6\n",
       "       ..\n",
       "4893    6\n",
       "4894    5\n",
       "4895    6\n",
       "4896    7\n",
       "4897    6\n",
       "Name: quality, Length: 4898, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "730f0f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f8d7fd91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ca1c93eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniforge3/envs/dml/lib/python3.9/site-packages/imblearn/over_sampling/_adasyn.py:156: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "adasyn = ADASYN(random_state=10, n_neighbors=2, n_jobs=-1)\n",
    "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5704d53d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.400000</td>\n",
       "      <td>-0.181818</td>\n",
       "      <td>-1.166667</td>\n",
       "      <td>-0.353659</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.521739</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>-0.381496</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>-0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>-0.727273</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.134146</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>1.308966</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>2.214286</td>\n",
       "      <td>-0.684211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.727273</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>-0.217391</td>\n",
       "      <td>-0.355932</td>\n",
       "      <td>0.073101</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.263158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>-0.341463</td>\n",
       "      <td>-1.214286</td>\n",
       "      <td>-0.478261</td>\n",
       "      <td>-0.576271</td>\n",
       "      <td>-0.785837</td>\n",
       "      <td>-0.157895</td>\n",
       "      <td>-0.642857</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.916667</td>\n",
       "      <td>-0.280488</td>\n",
       "      <td>-1.214286</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>-0.135593</td>\n",
       "      <td>-0.651057</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>-0.642857</td>\n",
       "      <td>0.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9290</th>\n",
       "      <td>1.437013</td>\n",
       "      <td>-0.047538</td>\n",
       "      <td>0.702604</td>\n",
       "      <td>0.126134</td>\n",
       "      <td>-0.716468</td>\n",
       "      <td>-0.282941</td>\n",
       "      <td>-0.040431</td>\n",
       "      <td>-0.003261</td>\n",
       "      <td>0.319006</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.561075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9291</th>\n",
       "      <td>1.727996</td>\n",
       "      <td>-0.000856</td>\n",
       "      <td>0.830979</td>\n",
       "      <td>0.305650</td>\n",
       "      <td>-0.667564</td>\n",
       "      <td>-0.275499</td>\n",
       "      <td>-0.083948</td>\n",
       "      <td>0.248944</td>\n",
       "      <td>0.246936</td>\n",
       "      <td>-0.023361</td>\n",
       "      <td>0.371891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9292</th>\n",
       "      <td>0.523141</td>\n",
       "      <td>-0.135237</td>\n",
       "      <td>0.610880</td>\n",
       "      <td>-0.383995</td>\n",
       "      <td>-0.838843</td>\n",
       "      <td>-0.259792</td>\n",
       "      <td>-0.028155</td>\n",
       "      <td>-0.743358</td>\n",
       "      <td>0.647672</td>\n",
       "      <td>-0.038370</td>\n",
       "      <td>1.159199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9293</th>\n",
       "      <td>0.487635</td>\n",
       "      <td>-0.113718</td>\n",
       "      <td>0.739095</td>\n",
       "      <td>-0.381109</td>\n",
       "      <td>-0.830389</td>\n",
       "      <td>-0.239209</td>\n",
       "      <td>-0.080310</td>\n",
       "      <td>-0.750117</td>\n",
       "      <td>0.703734</td>\n",
       "      <td>-0.089093</td>\n",
       "      <td>1.184116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9294</th>\n",
       "      <td>1.154337</td>\n",
       "      <td>-0.092887</td>\n",
       "      <td>0.577894</td>\n",
       "      <td>-0.048257</td>\n",
       "      <td>-0.763977</td>\n",
       "      <td>-0.290170</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>-0.248265</td>\n",
       "      <td>0.389019</td>\n",
       "      <td>0.024846</td>\n",
       "      <td>0.744859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9295 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0         -1.400000         -0.181818    -1.166667       -0.353659   0.500000   \n",
       "1          0.300000         -0.727273     0.583333        1.134146   0.571429   \n",
       "2          0.000000         -0.727273    -0.333333        0.560976  -0.285714   \n",
       "3          0.300000         -0.454545     0.416667       -0.341463  -1.214286   \n",
       "4         -0.200000          0.000000    -0.916667       -0.280488  -1.214286   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "9290       1.437013         -0.047538     0.702604        0.126134  -0.716468   \n",
       "9291       1.727996         -0.000856     0.830979        0.305650  -0.667564   \n",
       "9292       0.523141         -0.135237     0.610880       -0.383995  -0.838843   \n",
       "9293       0.487635         -0.113718     0.739095       -0.381109  -0.830389   \n",
       "9294       1.154337         -0.092887     0.577894       -0.048257  -0.763977   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide   density        pH  \\\n",
       "0               -0.521739              0.186441 -0.381496  0.315789   \n",
       "1                0.608696              0.372881  1.308966  0.894737   \n",
       "2               -0.217391             -0.355932  0.073101 -0.368421   \n",
       "3               -0.478261             -0.576271 -0.785837 -0.157895   \n",
       "4                0.608696             -0.135593 -0.651057  0.210526   \n",
       "...                   ...                   ...       ...       ...   \n",
       "9290            -0.282941             -0.040431 -0.003261  0.319006   \n",
       "9291            -0.275499             -0.083948  0.248944  0.246936   \n",
       "9292            -0.259792             -0.028155 -0.743358  0.647672   \n",
       "9293            -0.239209             -0.080310 -0.750117  0.703734   \n",
       "9294            -0.290170              0.001844 -0.248265  0.389019   \n",
       "\n",
       "      sulphates   alcohol  \n",
       "0     -0.071429 -0.052632  \n",
       "1      2.214286 -0.684211  \n",
       "2     -0.142857  0.263158  \n",
       "3     -0.642857  0.526316  \n",
       "4     -0.642857  0.473684  \n",
       "...         ...       ...  \n",
       "9290   0.001091  0.561075  \n",
       "9291  -0.023361  0.371891  \n",
       "9292  -0.038370  1.159199  \n",
       "9293  -0.089093  1.184116  \n",
       "9294   0.024846  0.744859  \n",
       "\n",
       "[9295 rows x 11 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_adasyn = pd.DataFrame(X_train_adasyn, columns=X.columns)\n",
    "X_train_adasyn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52920ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quality\n",
       "5    1410\n",
       "4    1346\n",
       "8    1345\n",
       "3    1320\n",
       "6    1318\n",
       "9    1317\n",
       "7    1239\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_adasyn.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99d6327d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9290</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9291</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9292</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9293</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9294</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9295 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          3      4      5      6      7      8      9\n",
       "0     False  False   True  False  False  False  False\n",
       "1     False  False   True  False  False  False  False\n",
       "2     False  False  False  False   True  False  False\n",
       "3     False  False  False  False   True  False  False\n",
       "4     False  False  False  False   True  False  False\n",
       "...     ...    ...    ...    ...    ...    ...    ...\n",
       "9290  False  False  False  False  False  False   True\n",
       "9291  False  False  False  False  False  False   True\n",
       "9292  False  False  False  False  False  False   True\n",
       "9293  False  False  False  False  False  False   True\n",
       "9294  False  False  False  False  False  False   True\n",
       "\n",
       "[9295 rows x 7 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_adasyn = pd.get_dummies(y_train_adasyn)\n",
    "y_train_adasyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4bb7a743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4331</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2323</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1557</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2731</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3438</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>980 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          3      4      5      6      7      8      9\n",
       "4331  False  False   True  False  False  False  False\n",
       "438   False  False  False  False   True  False  False\n",
       "198   False  False   True  False  False  False  False\n",
       "2323  False  False  False   True  False  False  False\n",
       "1557  False  False  False   True  False  False  False\n",
       "...     ...    ...    ...    ...    ...    ...    ...\n",
       "456   False  False  False  False   True  False  False\n",
       "2731  False  False   True  False  False  False  False\n",
       "3438  False  False   True  False  False  False  False\n",
       "60    False  False  False   True  False  False  False\n",
       "786   False  False  False   True  False  False  False\n",
       "\n",
       "[980 rows x 7 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid = pd.get_dummies(y_valid)\n",
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b4fb2518",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3107</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2806</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3650</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4362</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4804</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>980 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          3      4      5      6      7      8      9\n",
       "3107  False  False  False   True  False  False  False\n",
       "1049  False  False  False   True  False  False  False\n",
       "2806  False  False   True  False  False  False  False\n",
       "3650  False   True  False  False  False  False  False\n",
       "4362  False  False  False   True  False  False  False\n",
       "...     ...    ...    ...    ...    ...    ...    ...\n",
       "2129  False  False   True  False  False  False  False\n",
       "479   False  False  False   True  False  False  False\n",
       "4804  False   True  False  False  False  False  False\n",
       "2380  False   True  False  False  False  False  False\n",
       "1052  False  False   True  False  False  False  False\n",
       "\n",
       "[980 rows x 7 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = pd.get_dummies(y_test)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d30503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e7b77fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 32)                384       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 7)                 119       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,223\n",
      "Trainable params: 5,223\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f5033d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 10:03:47.522265: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-09-11 10:03:47.591633: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-09-11 10:03:47.591716: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15603 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 349ms/step - loss: 0.0795 - accuracy: 0.9758 - val_loss: 9.4823 - val_accuracy: 0.5755\n",
      "Epoch 2/10000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0029 - accuracy: 0.9996"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 10:03:47.842885: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-09-11 10:03:47.877051: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-09-11 10:03:47.877131: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15603 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0189 - accuracy: 0.9941 - val_loss: 9.7251 - val_accuracy: 0.5653\n",
      "Epoch 3/10000\n",
      "2/2 [==============================] - 0s 179ms/step - loss: 0.0791 - accuracy: 0.9713 - val_loss: 9.4388 - val_accuracy: 0.5867\n",
      "Epoch 4/10000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0041 - accuracy: 0.9992 - val_loss: 9.5729 - val_accuracy: 0.5735\n",
      "Epoch 5/10000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.0316 - accuracy: 0.9889 - val_loss: 9.6884 - val_accuracy: 0.5704\n",
      "Epoch 6/10000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0233 - accuracy: 0.9926 - val_loss: 9.5889 - val_accuracy: 0.5735\n",
      "Epoch 7/10000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0042 - accuracy: 0.9992 - val_loss: 9.5849 - val_accuracy: 0.5745\n",
      "Epoch 8/10000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0065 - accuracy: 0.9988 - val_loss: 9.6167 - val_accuracy: 0.5724\n",
      "Epoch 9/10000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0138 - accuracy: 0.9956 - val_loss: 9.5632 - val_accuracy: 0.5755\n",
      "Epoch 10/10000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0076 - accuracy: 0.9981 - val_loss: 9.4867 - val_accuracy: 0.5786\n",
      "Epoch 11/10000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.0028 - accuracy: 0.9998 - val_loss: 9.4575 - val_accuracy: 0.5796\n",
      "Epoch 12/10000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 9.4634 - val_accuracy: 0.5806\n",
      "Epoch 13/10000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 9.4808 - val_accuracy: 0.5806\n",
      "Epoch 14/10000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0053 - accuracy: 0.9989 - val_loss: 9.4841 - val_accuracy: 0.5796\n",
      "Epoch 15/10000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0043 - accuracy: 0.9991 - val_loss: 9.4763 - val_accuracy: 0.5735\n",
      "Epoch 16/10000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 9.4702 - val_accuracy: 0.5735\n",
      "Epoch 17/10000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 9.4768 - val_accuracy: 0.5745\n",
      "Epoch 18/10000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 9.4900 - val_accuracy: 0.5745\n",
      "Epoch 19/10000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0019 - accuracy: 0.9999 - val_loss: 9.5035 - val_accuracy: 0.5735\n",
      "Epoch 20/10000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.0022 - accuracy: 0.9999 - val_loss: 9.5132 - val_accuracy: 0.5735\n",
      "Epoch 21/10000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 9.5187 - val_accuracy: 0.5745\n",
      "Epoch 22/10000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 9.5223 - val_accuracy: 0.5776\n",
      "Epoch 23/10000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 9.5242 - val_accuracy: 0.5796\n",
      "Epoch 24/10000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 9.5250 - val_accuracy: 0.5786\n",
      "Epoch 25/10000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 9.5239 - val_accuracy: 0.5755\n",
      "Epoch 26/10000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 9.5214 - val_accuracy: 0.5755\n",
      "Epoch 27/10000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 9.5178 - val_accuracy: 0.5755\n",
      "Epoch 28/10000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 9.5133 - val_accuracy: 0.5765\n",
      "Epoch 29/10000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 9.5089 - val_accuracy: 0.5755\n",
      "Epoch 30/10000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 9.5063 - val_accuracy: 0.5765\n",
      "Epoch 31/10000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 9.5050 - val_accuracy: 0.5755\n",
      "Epoch 32/10000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 9.5054 - val_accuracy: 0.5755\n",
      "Epoch 33/10000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 9.5072 - val_accuracy: 0.5755\n",
      "Epoch 34/10000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 9.5095 - val_accuracy: 0.5755\n",
      "Epoch 35/10000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 9.5113 - val_accuracy: 0.5755\n",
      "Epoch 36/10000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 9.5122 - val_accuracy: 0.5755\n",
      "Epoch 37/10000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 9.5119 - val_accuracy: 0.5765\n",
      "Epoch 38/10000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 9.5103 - val_accuracy: 0.5755\n",
      "Epoch 39/10000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5084 - val_accuracy: 0.5755\n",
      "Epoch 40/10000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5062 - val_accuracy: 0.5776\n",
      "Epoch 41/10000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5041 - val_accuracy: 0.5776\n",
      "Epoch 42/10000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5022 - val_accuracy: 0.5776\n",
      "Epoch 43/10000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 9.5008 - val_accuracy: 0.5776\n",
      "Epoch 44/10000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5001 - val_accuracy: 0.5765\n",
      "Epoch 45/10000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5000 - val_accuracy: 0.5765\n",
      "Epoch 46/10000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5008 - val_accuracy: 0.5765\n",
      "Epoch 47/10000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5016 - val_accuracy: 0.5765\n",
      "Epoch 48/10000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5024 - val_accuracy: 0.5765\n",
      "Epoch 49/10000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 9.5031 - val_accuracy: 0.5755\n",
      "Epoch 50/10000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5033 - val_accuracy: 0.5765\n",
      "Epoch 51/10000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5031 - val_accuracy: 0.5765\n",
      "Epoch 52/10000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5033 - val_accuracy: 0.5765\n",
      "Epoch 53/10000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5032 - val_accuracy: 0.5765\n",
      "Epoch 54/10000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5035 - val_accuracy: 0.5765\n",
      "Epoch 55/10000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 9.5034 - val_accuracy: 0.5755\n",
      "Epoch 56/10000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 9.5033 - val_accuracy: 0.5765\n",
      "Epoch 57/10000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5032 - val_accuracy: 0.5765\n",
      "Epoch 58/10000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5034 - val_accuracy: 0.5765\n",
      "Epoch 59/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5036 - val_accuracy: 0.5765\n",
      "Epoch 60/10000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5035 - val_accuracy: 0.5755\n",
      "Epoch 61/10000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 9.5036 - val_accuracy: 0.5755\n",
      "Epoch 62/10000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 9.5035 - val_accuracy: 0.5755\n",
      "Epoch 63/10000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5037 - val_accuracy: 0.5765\n",
      "Epoch 64/10000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5040 - val_accuracy: 0.5765\n",
      "Epoch 65/10000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5043 - val_accuracy: 0.5765\n",
      "Epoch 66/10000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5047 - val_accuracy: 0.5765\n",
      "Epoch 67/10000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5049 - val_accuracy: 0.5755\n",
      "Epoch 68/10000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 9.5049 - val_accuracy: 0.5755\n",
      "Epoch 69/10000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 9.5050 - val_accuracy: 0.5765\n",
      "Epoch 70/10000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5055 - val_accuracy: 0.5765\n",
      "Epoch 71/10000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 9.5055 - val_accuracy: 0.5765\n",
      "Epoch 72/10000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 9.5057 - val_accuracy: 0.5755\n",
      "Epoch 73/10000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5059 - val_accuracy: 0.5755\n",
      "Epoch 74/10000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 9.5062 - val_accuracy: 0.5755\n",
      "Epoch 75/10000\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 9.5064 - val_accuracy: 0.5755\n",
      "Epoch 76/10000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5071 - val_accuracy: 0.5765\n",
      "Epoch 77/10000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5078 - val_accuracy: 0.5765\n",
      "Epoch 78/10000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5082 - val_accuracy: 0.5765\n",
      "Epoch 79/10000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5085 - val_accuracy: 0.5765\n",
      "Epoch 80/10000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 9.5087 - val_accuracy: 0.5755\n",
      "Epoch 81/10000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 9.5088 - val_accuracy: 0.5755\n",
      "Epoch 82/10000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 9.5093 - val_accuracy: 0.5755\n",
      "Epoch 83/10000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5095 - val_accuracy: 0.5765\n",
      "Epoch 84/10000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5102 - val_accuracy: 0.5765\n",
      "Epoch 85/10000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5109 - val_accuracy: 0.5765\n",
      "Epoch 86/10000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5114 - val_accuracy: 0.5765\n",
      "Epoch 87/10000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5120 - val_accuracy: 0.5755\n",
      "Epoch 88/10000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5123 - val_accuracy: 0.5755\n",
      "Epoch 89/10000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5123 - val_accuracy: 0.5765\n",
      "Epoch 90/10000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5123 - val_accuracy: 0.5765\n",
      "Epoch 91/10000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5124 - val_accuracy: 0.5765\n",
      "Epoch 92/10000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5126 - val_accuracy: 0.5765\n",
      "Epoch 93/10000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5128 - val_accuracy: 0.5765\n",
      "Epoch 94/10000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5134 - val_accuracy: 0.5776\n",
      "Epoch 95/10000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5139 - val_accuracy: 0.5765\n",
      "Epoch 96/10000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5144 - val_accuracy: 0.5755\n",
      "Epoch 97/10000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5151 - val_accuracy: 0.5755\n",
      "Epoch 98/10000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5159 - val_accuracy: 0.5765\n",
      "Epoch 99/10000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5164 - val_accuracy: 0.5765\n",
      "Epoch 100/10000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 9.5165 - val_accuracy: 0.5765\n",
      "Epoch 101/10000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.5162 - val_accuracy: 0.5755\n",
      "Epoch 102/10000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5166 - val_accuracy: 0.5755\n",
      "Epoch 103/10000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5168 - val_accuracy: 0.5755\n",
      "Epoch 104/10000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5174 - val_accuracy: 0.5755\n",
      "Epoch 105/10000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5181 - val_accuracy: 0.5755\n",
      "Epoch 106/10000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5180 - val_accuracy: 0.5755\n",
      "Epoch 107/10000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5184 - val_accuracy: 0.5765\n",
      "Epoch 108/10000\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5186 - val_accuracy: 0.5765\n",
      "Epoch 109/10000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5194 - val_accuracy: 0.5755\n",
      "Epoch 110/10000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5201 - val_accuracy: 0.5755\n",
      "Epoch 111/10000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5211 - val_accuracy: 0.5755\n",
      "Epoch 112/10000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5217 - val_accuracy: 0.5755\n",
      "Epoch 113/10000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5223 - val_accuracy: 0.5755\n",
      "Epoch 114/10000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5226 - val_accuracy: 0.5755\n",
      "Epoch 115/10000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5227 - val_accuracy: 0.5765\n",
      "Epoch 116/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5226 - val_accuracy: 0.5765\n",
      "Epoch 117/10000\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5231 - val_accuracy: 0.5755\n",
      "Epoch 118/10000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5231 - val_accuracy: 0.5755\n",
      "Epoch 119/10000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5238 - val_accuracy: 0.5755\n",
      "Epoch 120/10000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5246 - val_accuracy: 0.5765\n",
      "Epoch 121/10000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5249 - val_accuracy: 0.5765\n",
      "Epoch 122/10000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5254 - val_accuracy: 0.5765\n",
      "Epoch 123/10000\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5261 - val_accuracy: 0.5745\n",
      "Epoch 124/10000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5271 - val_accuracy: 0.5745\n",
      "Epoch 125/10000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5280 - val_accuracy: 0.5755\n",
      "Epoch 126/10000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5283 - val_accuracy: 0.5765\n",
      "Epoch 127/10000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5286 - val_accuracy: 0.5755\n",
      "Epoch 128/10000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5288 - val_accuracy: 0.5765\n",
      "Epoch 129/10000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5288 - val_accuracy: 0.5745\n",
      "Epoch 130/10000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5293 - val_accuracy: 0.5745\n",
      "Epoch 131/10000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5303 - val_accuracy: 0.5755\n",
      "Epoch 132/10000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5312 - val_accuracy: 0.5765\n",
      "Epoch 133/10000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5317 - val_accuracy: 0.5765\n",
      "Epoch 134/10000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5320 - val_accuracy: 0.5745\n",
      "Epoch 135/10000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5325 - val_accuracy: 0.5755\n",
      "Epoch 136/10000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5329 - val_accuracy: 0.5755\n",
      "Epoch 137/10000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5330 - val_accuracy: 0.5755\n",
      "Epoch 138/10000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5332 - val_accuracy: 0.5765\n",
      "Epoch 139/10000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5337 - val_accuracy: 0.5755\n",
      "Epoch 140/10000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5345 - val_accuracy: 0.5755\n",
      "Epoch 141/10000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5350 - val_accuracy: 0.5745\n",
      "Epoch 142/10000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5354 - val_accuracy: 0.5745\n",
      "Epoch 143/10000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5359 - val_accuracy: 0.5755\n",
      "Epoch 144/10000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5362 - val_accuracy: 0.5765\n",
      "Epoch 145/10000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5366 - val_accuracy: 0.5765\n",
      "Epoch 146/10000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5371 - val_accuracy: 0.5755\n",
      "Epoch 147/10000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5380 - val_accuracy: 0.5755\n",
      "Epoch 148/10000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5391 - val_accuracy: 0.5755\n",
      "Epoch 149/10000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5394 - val_accuracy: 0.5765\n",
      "Epoch 150/10000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5392 - val_accuracy: 0.5755\n",
      "Epoch 151/10000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5395 - val_accuracy: 0.5755\n",
      "Epoch 152/10000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5397 - val_accuracy: 0.5765\n",
      "Epoch 153/10000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5396 - val_accuracy: 0.5755\n",
      "Epoch 154/10000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5405 - val_accuracy: 0.5755\n",
      "Epoch 155/10000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5419 - val_accuracy: 0.5755\n",
      "Epoch 156/10000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5428 - val_accuracy: 0.5745\n",
      "Epoch 157/10000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5434 - val_accuracy: 0.5765\n",
      "Epoch 158/10000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5434 - val_accuracy: 0.5765\n",
      "Epoch 159/10000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5435 - val_accuracy: 0.5755\n",
      "Epoch 160/10000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5437 - val_accuracy: 0.5755\n",
      "Epoch 161/10000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5438 - val_accuracy: 0.5755\n",
      "Epoch 162/10000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5444 - val_accuracy: 0.5755\n",
      "Epoch 163/10000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5449 - val_accuracy: 0.5765\n",
      "Epoch 164/10000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5458 - val_accuracy: 0.5755\n",
      "Epoch 165/10000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5467 - val_accuracy: 0.5755\n",
      "Epoch 166/10000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5470 - val_accuracy: 0.5765\n",
      "Epoch 167/10000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5479 - val_accuracy: 0.5765\n",
      "Epoch 168/10000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5482 - val_accuracy: 0.5745\n",
      "Epoch 169/10000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5483 - val_accuracy: 0.5755\n",
      "Epoch 170/10000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5481 - val_accuracy: 0.5755\n",
      "Epoch 171/10000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5481 - val_accuracy: 0.5755\n",
      "Epoch 172/10000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5490 - val_accuracy: 0.5755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/10000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5500 - val_accuracy: 0.5755\n",
      "Epoch 174/10000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5503 - val_accuracy: 0.5755\n",
      "Epoch 175/10000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5512 - val_accuracy: 0.5755\n",
      "Epoch 176/10000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5517 - val_accuracy: 0.5755\n",
      "Epoch 177/10000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5520 - val_accuracy: 0.5765\n",
      "Epoch 178/10000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5525 - val_accuracy: 0.5755\n",
      "Epoch 179/10000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5530 - val_accuracy: 0.5745\n",
      "Epoch 180/10000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5531 - val_accuracy: 0.5745\n",
      "Epoch 181/10000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5532 - val_accuracy: 0.5755\n",
      "Epoch 182/10000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5538 - val_accuracy: 0.5755\n",
      "Epoch 183/10000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5548 - val_accuracy: 0.5765\n",
      "Epoch 184/10000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5554 - val_accuracy: 0.5765\n",
      "Epoch 185/10000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5556 - val_accuracy: 0.5745\n",
      "Epoch 186/10000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5559 - val_accuracy: 0.5745\n",
      "Epoch 187/10000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5565 - val_accuracy: 0.5745\n",
      "Epoch 188/10000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5572 - val_accuracy: 0.5765\n",
      "Epoch 189/10000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5574 - val_accuracy: 0.5765\n",
      "Epoch 190/10000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5572 - val_accuracy: 0.5755\n",
      "Epoch 191/10000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5576 - val_accuracy: 0.5755\n",
      "Epoch 192/10000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5589 - val_accuracy: 0.5755\n",
      "Epoch 193/10000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5598 - val_accuracy: 0.5765\n",
      "Epoch 194/10000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5602 - val_accuracy: 0.5765\n",
      "Epoch 195/10000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5602 - val_accuracy: 0.5755\n",
      "Epoch 196/10000\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5605 - val_accuracy: 0.5755\n",
      "Epoch 197/10000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5611 - val_accuracy: 0.5755\n",
      "Epoch 198/10000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5620 - val_accuracy: 0.5755\n",
      "Epoch 199/10000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5628 - val_accuracy: 0.5755\n",
      "Epoch 200/10000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5640 - val_accuracy: 0.5755\n",
      "Epoch 201/10000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5641 - val_accuracy: 0.5765\n",
      "Epoch 202/10000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5639 - val_accuracy: 0.5765\n",
      "Epoch 203/10000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5635 - val_accuracy: 0.5755\n",
      "Epoch 204/10000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5637 - val_accuracy: 0.5755\n",
      "Epoch 205/10000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5648 - val_accuracy: 0.5765\n",
      "Epoch 206/10000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5661 - val_accuracy: 0.5755\n",
      "Epoch 207/10000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5668 - val_accuracy: 0.5755\n",
      "Epoch 208/10000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5671 - val_accuracy: 0.5755\n",
      "Epoch 209/10000\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5676 - val_accuracy: 0.5765\n",
      "Epoch 210/10000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5681 - val_accuracy: 0.5755\n",
      "Epoch 211/10000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5682 - val_accuracy: 0.5745\n",
      "Epoch 212/10000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5687 - val_accuracy: 0.5765\n",
      "Epoch 213/10000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5699 - val_accuracy: 0.5765\n",
      "Epoch 214/10000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5703 - val_accuracy: 0.5755\n",
      "Epoch 215/10000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5711 - val_accuracy: 0.5755\n",
      "Epoch 216/10000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5710 - val_accuracy: 0.5755\n",
      "Epoch 217/10000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5712 - val_accuracy: 0.5765\n",
      "Epoch 218/10000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5720 - val_accuracy: 0.5765\n",
      "Epoch 219/10000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5725 - val_accuracy: 0.5755\n",
      "Epoch 220/10000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5731 - val_accuracy: 0.5765\n",
      "Epoch 221/10000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5733 - val_accuracy: 0.5745\n",
      "Epoch 222/10000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5733 - val_accuracy: 0.5745\n",
      "Epoch 223/10000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5729 - val_accuracy: 0.5755\n",
      "Epoch 224/10000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5732 - val_accuracy: 0.5755\n",
      "Epoch 225/10000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5749 - val_accuracy: 0.5765\n",
      "Epoch 226/10000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5768 - val_accuracy: 0.5765\n",
      "Epoch 227/10000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5776 - val_accuracy: 0.5745\n",
      "Epoch 228/10000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5778 - val_accuracy: 0.5745\n",
      "Epoch 229/10000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5781 - val_accuracy: 0.5745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230/10000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5783 - val_accuracy: 0.5745\n",
      "Epoch 231/10000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5789 - val_accuracy: 0.5755\n",
      "Epoch 232/10000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5799 - val_accuracy: 0.5765\n",
      "Epoch 233/10000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5804 - val_accuracy: 0.5765\n",
      "Epoch 234/10000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5810 - val_accuracy: 0.5755\n",
      "Epoch 235/10000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5815 - val_accuracy: 0.5745\n",
      "Epoch 236/10000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 9.5819 - val_accuracy: 0.5745\n",
      "Epoch 237/10000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 9.5826 - val_accuracy: 0.5745\n",
      "Epoch 238/10000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5828 - val_accuracy: 0.5765\n",
      "Epoch 239/10000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5832 - val_accuracy: 0.5765\n",
      "Epoch 240/10000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5836 - val_accuracy: 0.5745\n",
      "Epoch 241/10000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5846 - val_accuracy: 0.5745\n",
      "Epoch 242/10000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5854 - val_accuracy: 0.5765\n",
      "Epoch 243/10000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5846 - val_accuracy: 0.5765\n",
      "Epoch 244/10000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5846 - val_accuracy: 0.5755\n",
      "Epoch 245/10000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5856 - val_accuracy: 0.5745\n",
      "Epoch 246/10000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5867 - val_accuracy: 0.5745\n",
      "Epoch 247/10000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5870 - val_accuracy: 0.5755\n",
      "Epoch 248/10000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5868 - val_accuracy: 0.5745\n",
      "Epoch 249/10000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5874 - val_accuracy: 0.5745\n",
      "Epoch 250/10000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5886 - val_accuracy: 0.5745\n",
      "Epoch 251/10000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 9.5899 - val_accuracy: 0.5765\n",
      "Epoch 252/10000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5904 - val_accuracy: 0.5765\n",
      "Epoch 253/10000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 9.5906 - val_accuracy: 0.5745\n",
      "Epoch 254/10000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5910 - val_accuracy: 0.5745\n",
      "Epoch 255/10000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5911 - val_accuracy: 0.5755\n",
      "Epoch 256/10000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5927 - val_accuracy: 0.5765\n",
      "Epoch 257/10000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5923 - val_accuracy: 0.5765\n",
      "Epoch 258/10000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 9.5932 - val_accuracy: 0.5745\n",
      "Epoch 259/10000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5938 - val_accuracy: 0.5745\n",
      "Epoch 260/10000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5944 - val_accuracy: 0.5755\n",
      "Epoch 261/10000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5950 - val_accuracy: 0.5755\n",
      "Epoch 262/10000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5954 - val_accuracy: 0.5745\n",
      "Epoch 263/10000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 9.5957 - val_accuracy: 0.5745\n",
      "Epoch 264/10000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 9.5965 - val_accuracy: 0.5765\n",
      "Epoch 265/10000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5972 - val_accuracy: 0.5765\n",
      "Epoch 266/10000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5983 - val_accuracy: 0.5765\n",
      "Epoch 267/10000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5986 - val_accuracy: 0.5755\n",
      "Epoch 268/10000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.5998 - val_accuracy: 0.5745\n",
      "Epoch 269/10000\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.6002 - val_accuracy: 0.5745\n",
      "Epoch 270/10000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 9.6006 - val_accuracy: 0.5745\n",
      "Epoch 271/10000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.6005 - val_accuracy: 0.5765\n",
      "Epoch 272/10000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5996 - val_accuracy: 0.5765\n",
      "Epoch 273/10000\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.5992 - val_accuracy: 0.5745\n",
      "Epoch 274/10000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.6003 - val_accuracy: 0.5745\n",
      "Epoch 275/10000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 9.9101e-04 - accuracy: 1.0000 - val_loss: 9.6027 - val_accuracy: 0.5765\n",
      "Epoch 276/10000\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 9.6047 - val_accuracy: 0.5755\n",
      "Epoch 277/10000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.6051 - val_accuracy: 0.5745\n",
      "Epoch 278/10000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 9.6042 - val_accuracy: 0.5745\n",
      "Epoch 279/10000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 9.6024 - val_accuracy: 0.5745\n",
      "Epoch 280/10000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.6024 - val_accuracy: 0.5755\n",
      "Epoch 281/10000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.6021 - val_accuracy: 0.5745\n",
      "Epoch 282/10000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 9.9748e-04 - accuracy: 1.0000 - val_loss: 9.6041 - val_accuracy: 0.5745\n",
      "Epoch 283/10000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 9.6065 - val_accuracy: 0.5745\n",
      "Epoch 284/10000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 9.6080 - val_accuracy: 0.5745\n",
      "Epoch 285/10000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.6098 - val_accuracy: 0.5755\n",
      "Epoch 286/10000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.6098 - val_accuracy: 0.5755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 287/10000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.6097 - val_accuracy: 0.5745\n",
      "Epoch 288/10000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.6091 - val_accuracy: 0.5755\n",
      "Epoch 289/10000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 9.9952e-04 - accuracy: 1.0000 - val_loss: 9.6085 - val_accuracy: 0.5755\n",
      "Epoch 290/10000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.6091 - val_accuracy: 0.5755\n",
      "Epoch 291/10000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.6109 - val_accuracy: 0.5755\n",
      "Epoch 292/10000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.9801e-04 - accuracy: 1.0000 - val_loss: 9.6120 - val_accuracy: 0.5755\n",
      "Epoch 293/10000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 9.6127 - val_accuracy: 0.5755\n",
      "Epoch 294/10000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./model/white_wine\u001b[39m\u001b[38;5;132;01m{epoch:04d}\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;132;01m{val_loss:.4f}\u001b[39;00m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m model_save \u001b[38;5;241m=\u001b[39m ModelCheckpoint(filepath\u001b[38;5;241m=\u001b[39mfilepath, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_adasyn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_adasyn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_save\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/dml/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/dml/lib/python3.9/site-packages/keras/engine/training.py:1606\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1593\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m   1594\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1604\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution,\n\u001b[1;32m   1605\u001b[0m     )\n\u001b[0;32m-> 1606\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1619\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1620\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1621\u001b[0m }\n\u001b[1;32m   1622\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/miniforge3/envs/dml/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/dml/lib/python3.9/site-packages/keras/engine/training.py:1939\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_test_counter\u001b[38;5;241m.\u001b[39massign(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1938\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_test_begin()\n\u001b[0;32m-> 1939\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():  \u001b[38;5;66;03m# Single epoch.\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n",
      "File \u001b[0;32m~/miniforge3/envs/dml/lib/python3.9/site-packages/keras/engine/data_adapter.py:1307\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[0;32m-> 1307\u001b[0m     data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epochs):\n\u001b[1;32m   1309\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/dml/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:499\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    502\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/dml/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:696\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    694\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 696\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/dml/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:721\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(ds_variant):\n\u001b[1;32m    717\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    718\u001b[0m       gen_dataset_ops\u001b[38;5;241m.\u001b[39manonymous_iterator_v3(\n\u001b[1;32m    719\u001b[0m           output_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types,\n\u001b[1;32m    720\u001b[0m           output_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_shapes))\n\u001b[0;32m--> 721\u001b[0m   \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/dml/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3409\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   3408\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3409\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3410\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3412\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "early_stop = EarlyStopping(patience=1000) \n",
    "filepath = \"./model/white_wine{epoch:04d}__{val_loss:.4f}.keras\"\n",
    "model_save = ModelCheckpoint(filepath=filepath, save_best_only=True)\n",
    "history = model.fit(X_train_adasyn, y_train_adasyn, epochs=10000, batch_size=5000, validation_data=(X_valid, y_valid),\n",
    "                   callbacks=[early_stop, model_save])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "467b2741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4331</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2323</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1557</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2731</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3438</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>980 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          3      4      5      6      7      8      9\n",
       "4331  False  False   True  False  False  False  False\n",
       "438   False  False  False  False   True  False  False\n",
       "198   False  False   True  False  False  False  False\n",
       "2323  False  False  False   True  False  False  False\n",
       "1557  False  False  False   True  False  False  False\n",
       "...     ...    ...    ...    ...    ...    ...    ...\n",
       "456   False  False  False  False   True  False  False\n",
       "2731  False  False   True  False  False  False  False\n",
       "3438  False  False   True  False  False  False  False\n",
       "60    False  False  False   True  False  False  False\n",
       "786   False  False  False   True  False  False  False\n",
       "\n",
       "[980 rows x 7 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c8985709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 15:29:21.262548: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "wine_best_model = load_model(\"./model/white_wine0032__1.2257.keras\")\n",
    "wine_pred = wine_best_model.predict(X_test)\n",
    "wine_pred = pd.DataFrame(wine_pred, columns=y_valid.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "eedd45a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.787430e-06</td>\n",
       "      <td>0.323064</td>\n",
       "      <td>0.238568</td>\n",
       "      <td>0.299787</td>\n",
       "      <td>0.138471</td>\n",
       "      <td>1.057107e-04</td>\n",
       "      <td>7.054573e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.297148e-06</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.002891</td>\n",
       "      <td>0.081570</td>\n",
       "      <td>0.341317</td>\n",
       "      <td>5.741845e-01</td>\n",
       "      <td>1.723967e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.422047e-08</td>\n",
       "      <td>0.015389</td>\n",
       "      <td>0.540318</td>\n",
       "      <td>0.387200</td>\n",
       "      <td>0.057090</td>\n",
       "      <td>3.441946e-06</td>\n",
       "      <td>1.173062e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.363896e-07</td>\n",
       "      <td>0.042890</td>\n",
       "      <td>0.544707</td>\n",
       "      <td>0.275783</td>\n",
       "      <td>0.136594</td>\n",
       "      <td>2.560246e-05</td>\n",
       "      <td>1.144879e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.377284e-08</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>0.010333</td>\n",
       "      <td>0.257113</td>\n",
       "      <td>0.587828</td>\n",
       "      <td>1.436569e-01</td>\n",
       "      <td>1.238069e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>8.633584e-08</td>\n",
       "      <td>0.388208</td>\n",
       "      <td>0.069424</td>\n",
       "      <td>0.135844</td>\n",
       "      <td>0.227480</td>\n",
       "      <td>1.790447e-01</td>\n",
       "      <td>2.416760e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>1.238788e-01</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.201254</td>\n",
       "      <td>0.170278</td>\n",
       "      <td>0.357444</td>\n",
       "      <td>1.461935e-01</td>\n",
       "      <td>5.791647e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>7.931296e-04</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.150577</td>\n",
       "      <td>0.541969</td>\n",
       "      <td>0.277148</td>\n",
       "      <td>2.929746e-02</td>\n",
       "      <td>6.766827e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>6.334407e-14</td>\n",
       "      <td>0.631184</td>\n",
       "      <td>0.201823</td>\n",
       "      <td>0.163198</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>2.136742e-08</td>\n",
       "      <td>8.654434e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>1.557601e-04</td>\n",
       "      <td>0.354448</td>\n",
       "      <td>0.260442</td>\n",
       "      <td>0.220301</td>\n",
       "      <td>0.127878</td>\n",
       "      <td>3.677411e-02</td>\n",
       "      <td>1.532565e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>980 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                3         4         5         6         7             8  \\\n",
       "0    4.787430e-06  0.323064  0.238568  0.299787  0.138471  1.057107e-04   \n",
       "1    1.297148e-06  0.000035  0.002891  0.081570  0.341317  5.741845e-01   \n",
       "2    1.422047e-08  0.015389  0.540318  0.387200  0.057090  3.441946e-06   \n",
       "3    8.363896e-07  0.042890  0.544707  0.275783  0.136594  2.560246e-05   \n",
       "4    3.377284e-08  0.001069  0.010333  0.257113  0.587828  1.436569e-01   \n",
       "..            ...       ...       ...       ...       ...           ...   \n",
       "975  8.633584e-08  0.388208  0.069424  0.135844  0.227480  1.790447e-01   \n",
       "976  1.238788e-01  0.000373  0.201254  0.170278  0.357444  1.461935e-01   \n",
       "977  7.931296e-04  0.000209  0.150577  0.541969  0.277148  2.929746e-02   \n",
       "978  6.334407e-14  0.631184  0.201823  0.163198  0.003795  2.136742e-08   \n",
       "979  1.557601e-04  0.354448  0.260442  0.220301  0.127878  3.677411e-02   \n",
       "\n",
       "                9  \n",
       "0    7.054573e-07  \n",
       "1    1.723967e-06  \n",
       "2    1.173062e-10  \n",
       "3    1.144879e-08  \n",
       "4    1.238069e-07  \n",
       "..            ...  \n",
       "975  2.416760e-08  \n",
       "976  5.791647e-04  \n",
       "977  6.766827e-06  \n",
       "978  8.654434e-16  \n",
       "979  1.532565e-06  \n",
       "\n",
       "[980 rows x 7 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "5d94df4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      6\n",
       "1      6\n",
       "2      5\n",
       "3      4\n",
       "4      6\n",
       "      ..\n",
       "975    5\n",
       "976    6\n",
       "977    4\n",
       "978    4\n",
       "979    5\n",
       "Length: 980, dtype: int64"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_class = y_test.idxmax(axis=1)\n",
    "y_test_class = y_test_class.reset_index(drop=True)\n",
    "y_test_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5a2c8723",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      4\n",
       "1      8\n",
       "2      5\n",
       "3      5\n",
       "4      7\n",
       "      ..\n",
       "975    4\n",
       "976    7\n",
       "977    6\n",
       "978    4\n",
       "979    4\n",
       "Length: 980, dtype: int64"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_pred_class = wine_pred.idxmax(axis=1)\n",
    "wine_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b9f7e3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quality\n",
       "6    2198\n",
       "5    1457\n",
       "7     880\n",
       "8     175\n",
       "4     163\n",
       "3      20\n",
       "9       5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "6aa3cd66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       0.10      0.28      0.15        32\n",
      "           5       0.55      0.58      0.56       292\n",
      "           6       0.58      0.35      0.43       440\n",
      "           7       0.35      0.39      0.37       176\n",
      "           8       0.17      0.54      0.26        35\n",
      "           9       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.43       980\n",
      "   macro avg       0.25      0.30      0.25       980\n",
      "weighted avg       0.50      0.43      0.44       980\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniforge3/envs/dml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/user/miniforge3/envs/dml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/user/miniforge3/envs/dml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_class, wine_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ae909a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "339801f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, ..., 3, 4, 3])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y2_labeled = le.fit_transform(y2)\n",
    "y2_labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aa7541",
   "metadata": {},
   "source": [
    "XGB로 비교분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4599382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_valid2, y_train2, y_valid2 = train_test_split(X_scaled, y2_labeled, test_size=0.4, stratify=y2_labeled, random_state=10)\n",
    "X_valid2, X_test2, y_valid2, y_test2 = train_test_split(X_valid2, y_valid2, test_size=0.5, stratify=y_valid2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "41ce8aea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniforge3/envs/dml/lib/python3.9/site-packages/imblearn/over_sampling/_adasyn.py:156: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "adasyn = ADASYN(random_state=10, n_neighbors=2, n_jobs=-1)\n",
    "X_train2_adasyn, y_train2_adasyn = adasyn.fit_resample(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cdad0bf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.400000</td>\n",
       "      <td>-0.181818</td>\n",
       "      <td>-1.166667</td>\n",
       "      <td>-0.353659</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.521739</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>-0.381496</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>-0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>-0.727273</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.134146</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>1.308966</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>2.214286</td>\n",
       "      <td>-0.684211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.727273</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>-0.217391</td>\n",
       "      <td>-0.355932</td>\n",
       "      <td>0.073101</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.263158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>-0.341463</td>\n",
       "      <td>-1.214286</td>\n",
       "      <td>-0.478261</td>\n",
       "      <td>-0.576271</td>\n",
       "      <td>-0.785837</td>\n",
       "      <td>-0.157895</td>\n",
       "      <td>-0.642857</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.916667</td>\n",
       "      <td>-0.280488</td>\n",
       "      <td>-1.214286</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>-0.135593</td>\n",
       "      <td>-0.651057</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>-0.642857</td>\n",
       "      <td>0.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9290</th>\n",
       "      <td>1.437013</td>\n",
       "      <td>-0.047538</td>\n",
       "      <td>0.702604</td>\n",
       "      <td>0.126134</td>\n",
       "      <td>-0.716468</td>\n",
       "      <td>-0.282941</td>\n",
       "      <td>-0.040431</td>\n",
       "      <td>-0.003261</td>\n",
       "      <td>0.319006</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.561075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9291</th>\n",
       "      <td>1.727996</td>\n",
       "      <td>-0.000856</td>\n",
       "      <td>0.830979</td>\n",
       "      <td>0.305650</td>\n",
       "      <td>-0.667564</td>\n",
       "      <td>-0.275499</td>\n",
       "      <td>-0.083948</td>\n",
       "      <td>0.248944</td>\n",
       "      <td>0.246936</td>\n",
       "      <td>-0.023361</td>\n",
       "      <td>0.371891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9292</th>\n",
       "      <td>0.523141</td>\n",
       "      <td>-0.135237</td>\n",
       "      <td>0.610880</td>\n",
       "      <td>-0.383995</td>\n",
       "      <td>-0.838843</td>\n",
       "      <td>-0.259792</td>\n",
       "      <td>-0.028155</td>\n",
       "      <td>-0.743358</td>\n",
       "      <td>0.647672</td>\n",
       "      <td>-0.038370</td>\n",
       "      <td>1.159199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9293</th>\n",
       "      <td>0.487635</td>\n",
       "      <td>-0.113718</td>\n",
       "      <td>0.739095</td>\n",
       "      <td>-0.381109</td>\n",
       "      <td>-0.830389</td>\n",
       "      <td>-0.239209</td>\n",
       "      <td>-0.080310</td>\n",
       "      <td>-0.750117</td>\n",
       "      <td>0.703734</td>\n",
       "      <td>-0.089093</td>\n",
       "      <td>1.184116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9294</th>\n",
       "      <td>1.154337</td>\n",
       "      <td>-0.092887</td>\n",
       "      <td>0.577894</td>\n",
       "      <td>-0.048257</td>\n",
       "      <td>-0.763977</td>\n",
       "      <td>-0.290170</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>-0.248265</td>\n",
       "      <td>0.389019</td>\n",
       "      <td>0.024846</td>\n",
       "      <td>0.744859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9295 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0         -1.400000         -0.181818    -1.166667       -0.353659   0.500000   \n",
       "1          0.300000         -0.727273     0.583333        1.134146   0.571429   \n",
       "2          0.000000         -0.727273    -0.333333        0.560976  -0.285714   \n",
       "3          0.300000         -0.454545     0.416667       -0.341463  -1.214286   \n",
       "4         -0.200000          0.000000    -0.916667       -0.280488  -1.214286   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "9290       1.437013         -0.047538     0.702604        0.126134  -0.716468   \n",
       "9291       1.727996         -0.000856     0.830979        0.305650  -0.667564   \n",
       "9292       0.523141         -0.135237     0.610880       -0.383995  -0.838843   \n",
       "9293       0.487635         -0.113718     0.739095       -0.381109  -0.830389   \n",
       "9294       1.154337         -0.092887     0.577894       -0.048257  -0.763977   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide   density        pH  \\\n",
       "0               -0.521739              0.186441 -0.381496  0.315789   \n",
       "1                0.608696              0.372881  1.308966  0.894737   \n",
       "2               -0.217391             -0.355932  0.073101 -0.368421   \n",
       "3               -0.478261             -0.576271 -0.785837 -0.157895   \n",
       "4                0.608696             -0.135593 -0.651057  0.210526   \n",
       "...                   ...                   ...       ...       ...   \n",
       "9290            -0.282941             -0.040431 -0.003261  0.319006   \n",
       "9291            -0.275499             -0.083948  0.248944  0.246936   \n",
       "9292            -0.259792             -0.028155 -0.743358  0.647672   \n",
       "9293            -0.239209             -0.080310 -0.750117  0.703734   \n",
       "9294            -0.290170              0.001844 -0.248265  0.389019   \n",
       "\n",
       "      sulphates   alcohol  \n",
       "0     -0.071429 -0.052632  \n",
       "1      2.214286 -0.684211  \n",
       "2     -0.142857  0.263158  \n",
       "3     -0.642857  0.526316  \n",
       "4     -0.642857  0.473684  \n",
       "...         ...       ...  \n",
       "9290   0.001091  0.561075  \n",
       "9291  -0.023361  0.371891  \n",
       "9292  -0.038370  1.159199  \n",
       "9293  -0.089093  1.184116  \n",
       "9294   0.024846  0.744859  \n",
       "\n",
       "[9295 rows x 11 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2_adasyn = pd.DataFrame(X_train2_adasyn, columns=X.columns)\n",
    "X_train2_adasyn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a072eb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1410\n",
       "1    1346\n",
       "5    1345\n",
       "0    1320\n",
       "3    1318\n",
       "6    1317\n",
       "4    1239\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train2_adasyn = pd.Series(y_train2_adasyn)\n",
    "y_train2_adasyn.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "506df56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "58ab6927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cd0297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "26cd5b67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.14      0.25      0.18         4\n",
      "           4       0.46      0.39      0.43        33\n",
      "           5       0.66      0.62      0.64       291\n",
      "           6       0.64      0.64      0.64       440\n",
      "           7       0.55      0.61      0.58       176\n",
      "           8       0.43      0.46      0.44        35\n",
      "           9       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.61       980\n",
      "   macro avg       0.41      0.42      0.41       980\n",
      "weighted avg       0.61      0.61      0.61       980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(max_depth= 5, n_estimators=1000, random_state=10, n_jobs=-1)\n",
    "xgb.fit(X_train2_adasyn, y_train2_adasyn)\n",
    "xgb_pred = xgb.predict(X_valid2)\n",
    "print(classification_report(le.inverse_transform(y_valid2), le.inverse_transform(xgb_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdfbcd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a5a7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a2ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1bd2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6aa7c419",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4898 entries, 0 to 4897\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         4898 non-null   float64\n",
      " 1   volatile acidity      4898 non-null   float64\n",
      " 2   citric acid           4898 non-null   float64\n",
      " 3   residual sugar        4898 non-null   float64\n",
      " 4   chlorides             4898 non-null   float64\n",
      " 5   free sulfur dioxide   4898 non-null   float64\n",
      " 6   total sulfur dioxide  4898 non-null   float64\n",
      " 7   density               4898 non-null   float64\n",
      " 8   pH                    4898 non-null   float64\n",
      " 9   sulphates             4898 non-null   float64\n",
      " 10  alcohol               4898 non-null   float64\n",
      " 11  quality               4898 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 459.3 KB\n"
     ]
    }
   ],
   "source": [
    "wine.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c9c720bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wine.drop('quality', axis=1).copy()\n",
    "y = wine['quality'].copy()\n",
    "y2 = wine['quality'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2917b8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f52d6467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          3      4      5      6      7      8      9\n",
       "0     False  False  False   True  False  False  False\n",
       "1     False  False  False   True  False  False  False\n",
       "2     False  False  False   True  False  False  False\n",
       "3     False  False  False   True  False  False  False\n",
       "4     False  False  False   True  False  False  False\n",
       "...     ...    ...    ...    ...    ...    ...    ...\n",
       "4893  False  False  False   True  False  False  False\n",
       "4894  False  False   True  False  False  False  False\n",
       "4895  False  False  False   True  False  False  False\n",
       "4896  False  False  False  False   True  False  False\n",
       "4897  False  False  False   True  False  False  False\n",
       "\n",
       "[4898 rows x 7 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1ceababf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ffccb7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RobustScaler()\n",
    "X_scaled = rs.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "92ccf6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bcd4e768",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_scaled, y, test_size=0.4, stratify=y ,random_state=10)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=0.5, stratify=y_valid, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4e54f012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7fab3f85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2938, 11) (980, 11) (980, 11)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_valid.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ac86ea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3df4f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Dense(64, input_dim=11, activation='relu'))\n",
    "model1.add(Dense(32, activation='relu'))\n",
    "model1.add(Dense(64, activation='relu'))\n",
    "model1.add(Dense(32, activation='relu'))\n",
    "model1.add(Dense(16, activation='relu'))\n",
    "model1.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "31235ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 64)                768       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 7)                 119       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,687\n",
      "Trainable params: 7,687\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a7582cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2938, 11) (2938, 7)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d029f0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.6961 - accuracy: 0.7264"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 13:25:53.871275: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-09-10 13:25:53.958331: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-09-10 13:25:53.958403: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15603 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-09-10 13:25:54.220448: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-09-10 13:25:54.263323: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-09-10 13:25:54.263456: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15603 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6953 - accuracy: 0.7267 - val_loss: 1.0956 - val_accuracy: 0.5704\n",
      "Epoch 2/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.6792 - accuracy: 0.7291 - val_loss: 1.1067 - val_accuracy: 0.5724\n",
      "Epoch 3/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.6700 - accuracy: 0.7338 - val_loss: 1.1088 - val_accuracy: 0.5776\n",
      "Epoch 4/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.6666 - accuracy: 0.7410 - val_loss: 1.1085 - val_accuracy: 0.5816\n",
      "Epoch 5/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.6607 - accuracy: 0.7471 - val_loss: 1.1134 - val_accuracy: 0.5827\n",
      "Epoch 6/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.6562 - accuracy: 0.7495 - val_loss: 1.1138 - val_accuracy: 0.5837\n",
      "Epoch 7/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.6513 - accuracy: 0.7488 - val_loss: 1.1176 - val_accuracy: 0.5867\n",
      "Epoch 8/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.6497 - accuracy: 0.7526 - val_loss: 1.1186 - val_accuracy: 0.5918\n",
      "Epoch 9/10000\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.6454 - accuracy: 0.7570 - val_loss: 1.1258 - val_accuracy: 0.5827\n",
      "Epoch 10/10000\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.6437 - accuracy: 0.7543 - val_loss: 1.1324 - val_accuracy: 0.5776\n",
      "Epoch 11/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.6399 - accuracy: 0.7549 - val_loss: 1.1368 - val_accuracy: 0.5816\n",
      "Epoch 12/10000\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.6364 - accuracy: 0.7583 - val_loss: 1.1352 - val_accuracy: 0.5847\n",
      "Epoch 13/10000\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.6337 - accuracy: 0.7573 - val_loss: 1.1404 - val_accuracy: 0.5816\n",
      "Epoch 14/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.6293 - accuracy: 0.7645 - val_loss: 1.1492 - val_accuracy: 0.5796\n",
      "Epoch 15/10000\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 0.6277 - accuracy: 0.7641 - val_loss: 1.1456 - val_accuracy: 0.5786\n",
      "Epoch 16/10000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.6241 - accuracy: 0.7638 - val_loss: 1.1547 - val_accuracy: 0.5765\n",
      "Epoch 17/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.6202 - accuracy: 0.7655 - val_loss: 1.1590 - val_accuracy: 0.5796\n",
      "Epoch 18/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.6186 - accuracy: 0.7662 - val_loss: 1.1582 - val_accuracy: 0.5857\n",
      "Epoch 19/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.6162 - accuracy: 0.7675 - val_loss: 1.1730 - val_accuracy: 0.5796\n",
      "Epoch 20/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.6159 - accuracy: 0.7692 - val_loss: 1.1623 - val_accuracy: 0.5806\n",
      "Epoch 21/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.6093 - accuracy: 0.7733 - val_loss: 1.1763 - val_accuracy: 0.5745\n",
      "Epoch 22/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.6075 - accuracy: 0.7754 - val_loss: 1.1785 - val_accuracy: 0.5776\n",
      "Epoch 23/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.6053 - accuracy: 0.7675 - val_loss: 1.1753 - val_accuracy: 0.5857\n",
      "Epoch 24/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.6022 - accuracy: 0.7740 - val_loss: 1.1810 - val_accuracy: 0.5704\n",
      "Epoch 25/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.6000 - accuracy: 0.7754 - val_loss: 1.1858 - val_accuracy: 0.5796\n",
      "Epoch 26/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.5950 - accuracy: 0.7774 - val_loss: 1.1869 - val_accuracy: 0.5837\n",
      "Epoch 27/10000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.5936 - accuracy: 0.7774 - val_loss: 1.1945 - val_accuracy: 0.5724\n",
      "Epoch 28/10000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.5909 - accuracy: 0.7791 - val_loss: 1.1923 - val_accuracy: 0.5745\n",
      "Epoch 29/10000\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.5918 - accuracy: 0.7798 - val_loss: 1.1974 - val_accuracy: 0.5755\n",
      "Epoch 30/10000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.5819 - accuracy: 0.7832 - val_loss: 1.1995 - val_accuracy: 0.5765\n",
      "Epoch 31/10000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.5792 - accuracy: 0.7866 - val_loss: 1.2013 - val_accuracy: 0.5745\n",
      "Epoch 32/10000\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.5799 - accuracy: 0.7866 - val_loss: 1.2114 - val_accuracy: 0.5806\n",
      "Epoch 33/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.5735 - accuracy: 0.7890 - val_loss: 1.2056 - val_accuracy: 0.5867\n",
      "Epoch 34/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.5702 - accuracy: 0.7859 - val_loss: 1.2117 - val_accuracy: 0.5714\n",
      "Epoch 35/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.5670 - accuracy: 0.7903 - val_loss: 1.2301 - val_accuracy: 0.5724\n",
      "Epoch 36/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.5687 - accuracy: 0.7832 - val_loss: 1.2214 - val_accuracy: 0.5827\n",
      "Epoch 37/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.5647 - accuracy: 0.7903 - val_loss: 1.2258 - val_accuracy: 0.5765\n",
      "Epoch 38/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.5633 - accuracy: 0.7897 - val_loss: 1.2378 - val_accuracy: 0.5806\n",
      "Epoch 39/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.5588 - accuracy: 0.7914 - val_loss: 1.2363 - val_accuracy: 0.5786\n",
      "Epoch 40/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.5560 - accuracy: 0.7937 - val_loss: 1.2380 - val_accuracy: 0.5776\n",
      "Epoch 41/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.5551 - accuracy: 0.7931 - val_loss: 1.2516 - val_accuracy: 0.5714\n",
      "Epoch 42/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.5601 - accuracy: 0.7856 - val_loss: 1.2524 - val_accuracy: 0.5796\n",
      "Epoch 43/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.5500 - accuracy: 0.7965 - val_loss: 1.2550 - val_accuracy: 0.5776\n",
      "Epoch 44/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.5446 - accuracy: 0.7951 - val_loss: 1.2512 - val_accuracy: 0.5765\n",
      "Epoch 45/10000\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.5411 - accuracy: 0.7961 - val_loss: 1.2608 - val_accuracy: 0.5745\n",
      "Epoch 46/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.5400 - accuracy: 0.7988 - val_loss: 1.2651 - val_accuracy: 0.5806\n",
      "Epoch 47/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.5372 - accuracy: 0.7971 - val_loss: 1.2811 - val_accuracy: 0.5755\n",
      "Epoch 48/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.5368 - accuracy: 0.7944 - val_loss: 1.2678 - val_accuracy: 0.5755\n",
      "Epoch 49/10000\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.5312 - accuracy: 0.8039 - val_loss: 1.2724 - val_accuracy: 0.5724\n",
      "Epoch 50/10000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.5301 - accuracy: 0.7999 - val_loss: 1.2810 - val_accuracy: 0.5765\n",
      "Epoch 51/10000\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.5244 - accuracy: 0.8016 - val_loss: 1.2834 - val_accuracy: 0.5755\n",
      "Epoch 52/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.5254 - accuracy: 0.8029 - val_loss: 1.2911 - val_accuracy: 0.5735\n",
      "Epoch 53/10000\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.5205 - accuracy: 0.8087 - val_loss: 1.2895 - val_accuracy: 0.5776\n",
      "Epoch 54/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.5203 - accuracy: 0.8067 - val_loss: 1.3032 - val_accuracy: 0.5704\n",
      "Epoch 55/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.5177 - accuracy: 0.8074 - val_loss: 1.3012 - val_accuracy: 0.5724\n",
      "Epoch 56/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.5148 - accuracy: 0.8131 - val_loss: 1.3035 - val_accuracy: 0.5694\n",
      "Epoch 57/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.5106 - accuracy: 0.8111 - val_loss: 1.3102 - val_accuracy: 0.5694\n",
      "Epoch 58/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.5052 - accuracy: 0.8111 - val_loss: 1.3158 - val_accuracy: 0.5765\n",
      "Epoch 59/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.5043 - accuracy: 0.8118 - val_loss: 1.3200 - val_accuracy: 0.5745\n",
      "Epoch 60/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.5006 - accuracy: 0.8142 - val_loss: 1.3282 - val_accuracy: 0.5653\n",
      "Epoch 61/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.4983 - accuracy: 0.8182 - val_loss: 1.3253 - val_accuracy: 0.5796\n",
      "Epoch 62/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.4966 - accuracy: 0.8196 - val_loss: 1.3376 - val_accuracy: 0.5694\n",
      "Epoch 63/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.4926 - accuracy: 0.8159 - val_loss: 1.3374 - val_accuracy: 0.5765\n",
      "Epoch 64/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.4914 - accuracy: 0.8206 - val_loss: 1.3432 - val_accuracy: 0.5745\n",
      "Epoch 65/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.4881 - accuracy: 0.8203 - val_loss: 1.3543 - val_accuracy: 0.5704\n",
      "Epoch 66/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.4870 - accuracy: 0.8206 - val_loss: 1.3499 - val_accuracy: 0.5714\n",
      "Epoch 67/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.4842 - accuracy: 0.8172 - val_loss: 1.3560 - val_accuracy: 0.5786\n",
      "Epoch 68/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.4801 - accuracy: 0.8295 - val_loss: 1.3624 - val_accuracy: 0.5806\n",
      "Epoch 69/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.4843 - accuracy: 0.8216 - val_loss: 1.3699 - val_accuracy: 0.5724\n",
      "Epoch 70/10000\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.4757 - accuracy: 0.8220 - val_loss: 1.3695 - val_accuracy: 0.5776\n",
      "Epoch 71/10000\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.4785 - accuracy: 0.8216 - val_loss: 1.3789 - val_accuracy: 0.5755\n",
      "Epoch 72/10000\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.4737 - accuracy: 0.8240 - val_loss: 1.3742 - val_accuracy: 0.5745\n",
      "Epoch 73/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.4731 - accuracy: 0.8237 - val_loss: 1.3832 - val_accuracy: 0.5735\n",
      "Epoch 74/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.4699 - accuracy: 0.8285 - val_loss: 1.3949 - val_accuracy: 0.5714\n",
      "Epoch 75/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.4661 - accuracy: 0.8315 - val_loss: 1.3969 - val_accuracy: 0.5694\n",
      "Epoch 76/10000\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.4634 - accuracy: 0.8308 - val_loss: 1.3998 - val_accuracy: 0.5765\n",
      "Epoch 77/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.4644 - accuracy: 0.8336 - val_loss: 1.4096 - val_accuracy: 0.5745\n",
      "Epoch 78/10000\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.4641 - accuracy: 0.8271 - val_loss: 1.4158 - val_accuracy: 0.5684\n",
      "Epoch 79/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.4574 - accuracy: 0.8336 - val_loss: 1.4287 - val_accuracy: 0.5663\n",
      "Epoch 80/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.4600 - accuracy: 0.8288 - val_loss: 1.4142 - val_accuracy: 0.5816\n",
      "Epoch 81/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.4534 - accuracy: 0.8380 - val_loss: 1.4252 - val_accuracy: 0.5786\n",
      "Epoch 82/10000\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.4537 - accuracy: 0.8424 - val_loss: 1.4198 - val_accuracy: 0.5755\n",
      "Epoch 83/10000\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.4491 - accuracy: 0.8373 - val_loss: 1.4506 - val_accuracy: 0.5643\n",
      "Epoch 84/10000\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.4450 - accuracy: 0.8397 - val_loss: 1.4563 - val_accuracy: 0.5776\n",
      "Epoch 85/10000\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.4464 - accuracy: 0.8414 - val_loss: 1.4455 - val_accuracy: 0.5684\n",
      "Epoch 86/10000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.4423 - accuracy: 0.8434 - val_loss: 1.4435 - val_accuracy: 0.5796\n",
      "Epoch 87/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.4347 - accuracy: 0.8472 - val_loss: 1.4703 - val_accuracy: 0.5663\n",
      "Epoch 88/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.4380 - accuracy: 0.8465 - val_loss: 1.4643 - val_accuracy: 0.5724\n",
      "Epoch 89/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.4334 - accuracy: 0.8509 - val_loss: 1.4662 - val_accuracy: 0.5776\n",
      "Epoch 90/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.4317 - accuracy: 0.8462 - val_loss: 1.4762 - val_accuracy: 0.5684\n",
      "Epoch 91/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.4285 - accuracy: 0.8489 - val_loss: 1.4785 - val_accuracy: 0.5694\n",
      "Epoch 92/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.4248 - accuracy: 0.8536 - val_loss: 1.4793 - val_accuracy: 0.5755\n",
      "Epoch 93/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.4254 - accuracy: 0.8492 - val_loss: 1.5008 - val_accuracy: 0.5755\n",
      "Epoch 94/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.4248 - accuracy: 0.8513 - val_loss: 1.4801 - val_accuracy: 0.5765\n",
      "Epoch 95/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.4243 - accuracy: 0.8475 - val_loss: 1.4966 - val_accuracy: 0.5776\n",
      "Epoch 96/10000\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.4213 - accuracy: 0.8523 - val_loss: 1.5162 - val_accuracy: 0.5714\n",
      "Epoch 97/10000\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.4192 - accuracy: 0.8533 - val_loss: 1.5254 - val_accuracy: 0.5735\n",
      "Epoch 98/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.4146 - accuracy: 0.8499 - val_loss: 1.5130 - val_accuracy: 0.5755\n",
      "Epoch 99/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.4096 - accuracy: 0.8577 - val_loss: 1.5108 - val_accuracy: 0.5704\n",
      "Epoch 100/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.4073 - accuracy: 0.8584 - val_loss: 1.5171 - val_accuracy: 0.5745\n",
      "Epoch 101/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.4031 - accuracy: 0.8608 - val_loss: 1.5343 - val_accuracy: 0.5673\n",
      "Epoch 102/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.4017 - accuracy: 0.8604 - val_loss: 1.5314 - val_accuracy: 0.5857\n",
      "Epoch 103/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.4028 - accuracy: 0.8601 - val_loss: 1.5297 - val_accuracy: 0.5724\n",
      "Epoch 104/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.4030 - accuracy: 0.8577 - val_loss: 1.5562 - val_accuracy: 0.5704\n",
      "Epoch 105/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.3992 - accuracy: 0.8611 - val_loss: 1.5591 - val_accuracy: 0.5714\n",
      "Epoch 106/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.4009 - accuracy: 0.8632 - val_loss: 1.5551 - val_accuracy: 0.5745\n",
      "Epoch 107/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.3938 - accuracy: 0.8659 - val_loss: 1.5633 - val_accuracy: 0.5776\n",
      "Epoch 108/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.3995 - accuracy: 0.8618 - val_loss: 1.5636 - val_accuracy: 0.5786\n",
      "Epoch 109/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.3929 - accuracy: 0.8659 - val_loss: 1.5779 - val_accuracy: 0.5694\n",
      "Epoch 110/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.3864 - accuracy: 0.8673 - val_loss: 1.5727 - val_accuracy: 0.5816\n",
      "Epoch 111/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.3807 - accuracy: 0.8700 - val_loss: 1.5750 - val_accuracy: 0.5837\n",
      "Epoch 112/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.3803 - accuracy: 0.8696 - val_loss: 1.5926 - val_accuracy: 0.5827\n",
      "Epoch 113/10000\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.3784 - accuracy: 0.8700 - val_loss: 1.5922 - val_accuracy: 0.5765\n",
      "Epoch 114/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.3767 - accuracy: 0.8696 - val_loss: 1.5968 - val_accuracy: 0.5735\n",
      "Epoch 115/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 51ms/step - loss: 0.3712 - accuracy: 0.8775 - val_loss: 1.6128 - val_accuracy: 0.5755\n",
      "Epoch 116/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.3719 - accuracy: 0.8713 - val_loss: 1.6118 - val_accuracy: 0.5786\n",
      "Epoch 117/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.3705 - accuracy: 0.8764 - val_loss: 1.6069 - val_accuracy: 0.5786\n",
      "Epoch 118/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.3730 - accuracy: 0.8662 - val_loss: 1.6230 - val_accuracy: 0.5765\n",
      "Epoch 119/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.3696 - accuracy: 0.8730 - val_loss: 1.6389 - val_accuracy: 0.5735\n",
      "Epoch 120/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.3678 - accuracy: 0.8734 - val_loss: 1.6422 - val_accuracy: 0.5684\n",
      "Epoch 121/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.3651 - accuracy: 0.8720 - val_loss: 1.6560 - val_accuracy: 0.5776\n",
      "Epoch 122/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.3627 - accuracy: 0.8775 - val_loss: 1.6561 - val_accuracy: 0.5694\n",
      "Epoch 123/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.3576 - accuracy: 0.8819 - val_loss: 1.6584 - val_accuracy: 0.5755\n",
      "Epoch 124/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.3558 - accuracy: 0.8809 - val_loss: 1.6715 - val_accuracy: 0.5735\n",
      "Epoch 125/10000\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.3608 - accuracy: 0.8764 - val_loss: 1.6954 - val_accuracy: 0.5755\n",
      "Epoch 126/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.3551 - accuracy: 0.8843 - val_loss: 1.6685 - val_accuracy: 0.5765\n",
      "Epoch 127/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.3530 - accuracy: 0.8781 - val_loss: 1.6755 - val_accuracy: 0.5745\n",
      "Epoch 128/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.3487 - accuracy: 0.8812 - val_loss: 1.6701 - val_accuracy: 0.5704\n",
      "Epoch 129/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.3458 - accuracy: 0.8819 - val_loss: 1.6904 - val_accuracy: 0.5745\n",
      "Epoch 130/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.3430 - accuracy: 0.8887 - val_loss: 1.7143 - val_accuracy: 0.5745\n",
      "Epoch 131/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.3416 - accuracy: 0.8897 - val_loss: 1.7143 - val_accuracy: 0.5796\n",
      "Epoch 132/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.3397 - accuracy: 0.8880 - val_loss: 1.6983 - val_accuracy: 0.5796\n",
      "Epoch 133/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.3390 - accuracy: 0.8870 - val_loss: 1.7056 - val_accuracy: 0.5765\n",
      "Epoch 134/10000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.3387 - accuracy: 0.8819 - val_loss: 1.7317 - val_accuracy: 0.5714\n",
      "Epoch 135/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.3398 - accuracy: 0.8860 - val_loss: 1.7413 - val_accuracy: 0.5786\n",
      "Epoch 136/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.3382 - accuracy: 0.8843 - val_loss: 1.7562 - val_accuracy: 0.5714\n",
      "Epoch 137/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.3352 - accuracy: 0.8867 - val_loss: 1.7308 - val_accuracy: 0.5776\n",
      "Epoch 138/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.3317 - accuracy: 0.8914 - val_loss: 1.7389 - val_accuracy: 0.5776\n",
      "Epoch 139/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.3277 - accuracy: 0.8918 - val_loss: 1.7342 - val_accuracy: 0.5796\n",
      "Epoch 140/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.3290 - accuracy: 0.8873 - val_loss: 1.7502 - val_accuracy: 0.5827\n",
      "Epoch 141/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.3296 - accuracy: 0.8890 - val_loss: 1.7533 - val_accuracy: 0.5724\n",
      "Epoch 142/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.3225 - accuracy: 0.8935 - val_loss: 1.7670 - val_accuracy: 0.5745\n",
      "Epoch 143/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.3271 - accuracy: 0.8952 - val_loss: 1.7588 - val_accuracy: 0.5806\n",
      "Epoch 144/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.3232 - accuracy: 0.8972 - val_loss: 1.7863 - val_accuracy: 0.5735\n",
      "Epoch 145/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.3196 - accuracy: 0.8952 - val_loss: 1.7999 - val_accuracy: 0.5827\n",
      "Epoch 146/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.3146 - accuracy: 0.9003 - val_loss: 1.8113 - val_accuracy: 0.5735\n",
      "Epoch 147/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.3164 - accuracy: 0.8993 - val_loss: 1.8124 - val_accuracy: 0.5806\n",
      "Epoch 148/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.3136 - accuracy: 0.8975 - val_loss: 1.8201 - val_accuracy: 0.5704\n",
      "Epoch 149/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.3126 - accuracy: 0.8982 - val_loss: 1.8430 - val_accuracy: 0.5806\n",
      "Epoch 150/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.3102 - accuracy: 0.8999 - val_loss: 1.8291 - val_accuracy: 0.5816\n",
      "Epoch 151/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.3148 - accuracy: 0.8962 - val_loss: 1.8322 - val_accuracy: 0.5867\n",
      "Epoch 152/10000\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.3077 - accuracy: 0.9033 - val_loss: 1.8352 - val_accuracy: 0.5827\n",
      "Epoch 153/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.3040 - accuracy: 0.9013 - val_loss: 1.8505 - val_accuracy: 0.5786\n",
      "Epoch 154/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.3001 - accuracy: 0.9098 - val_loss: 1.8628 - val_accuracy: 0.5837\n",
      "Epoch 155/10000\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.3046 - accuracy: 0.9033 - val_loss: 1.8608 - val_accuracy: 0.5806\n",
      "Epoch 156/10000\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.3003 - accuracy: 0.9050 - val_loss: 1.8491 - val_accuracy: 0.5755\n",
      "Epoch 157/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.2993 - accuracy: 0.9027 - val_loss: 1.8660 - val_accuracy: 0.5724\n",
      "Epoch 158/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.2961 - accuracy: 0.9050 - val_loss: 1.8698 - val_accuracy: 0.5776\n",
      "Epoch 159/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.2981 - accuracy: 0.9057 - val_loss: 1.8788 - val_accuracy: 0.5776\n",
      "Epoch 160/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.2905 - accuracy: 0.9084 - val_loss: 1.8961 - val_accuracy: 0.5806\n",
      "Epoch 161/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.2864 - accuracy: 0.9108 - val_loss: 1.8957 - val_accuracy: 0.5827\n",
      "Epoch 162/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.2846 - accuracy: 0.9125 - val_loss: 1.9036 - val_accuracy: 0.5776\n",
      "Epoch 163/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.2817 - accuracy: 0.9163 - val_loss: 1.9104 - val_accuracy: 0.5806\n",
      "Epoch 164/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.2847 - accuracy: 0.9095 - val_loss: 1.9484 - val_accuracy: 0.5765\n",
      "Epoch 165/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.2888 - accuracy: 0.9084 - val_loss: 1.9512 - val_accuracy: 0.5867\n",
      "Epoch 166/10000\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.2895 - accuracy: 0.9044 - val_loss: 1.9552 - val_accuracy: 0.5745\n",
      "Epoch 167/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.2864 - accuracy: 0.9091 - val_loss: 1.9730 - val_accuracy: 0.5776\n",
      "Epoch 168/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.2841 - accuracy: 0.9067 - val_loss: 1.9628 - val_accuracy: 0.5755\n",
      "Epoch 169/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.2810 - accuracy: 0.9074 - val_loss: 1.9621 - val_accuracy: 0.5837\n",
      "Epoch 170/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.2752 - accuracy: 0.9183 - val_loss: 1.9768 - val_accuracy: 0.5878\n",
      "Epoch 171/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.2797 - accuracy: 0.9067 - val_loss: 1.9881 - val_accuracy: 0.5694\n",
      "Epoch 172/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 47ms/step - loss: 0.2751 - accuracy: 0.9183 - val_loss: 1.9710 - val_accuracy: 0.5878\n",
      "Epoch 173/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.2759 - accuracy: 0.9091 - val_loss: 2.0008 - val_accuracy: 0.5714\n",
      "Epoch 174/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.2734 - accuracy: 0.9180 - val_loss: 1.9920 - val_accuracy: 0.5867\n",
      "Epoch 175/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.2669 - accuracy: 0.9190 - val_loss: 2.0175 - val_accuracy: 0.5704\n",
      "Epoch 176/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.2662 - accuracy: 0.9197 - val_loss: 2.0045 - val_accuracy: 0.5888\n",
      "Epoch 177/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.2677 - accuracy: 0.9183 - val_loss: 2.0250 - val_accuracy: 0.5806\n",
      "Epoch 178/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.2617 - accuracy: 0.9200 - val_loss: 2.0505 - val_accuracy: 0.5857\n",
      "Epoch 179/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.2598 - accuracy: 0.9217 - val_loss: 2.0431 - val_accuracy: 0.5847\n",
      "Epoch 180/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.2592 - accuracy: 0.9190 - val_loss: 2.0531 - val_accuracy: 0.5847\n",
      "Epoch 181/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.2563 - accuracy: 0.9224 - val_loss: 2.0763 - val_accuracy: 0.5816\n",
      "Epoch 182/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.2540 - accuracy: 0.9261 - val_loss: 2.0546 - val_accuracy: 0.5847\n",
      "Epoch 183/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.2497 - accuracy: 0.9251 - val_loss: 2.0757 - val_accuracy: 0.5806\n",
      "Epoch 184/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.2480 - accuracy: 0.9258 - val_loss: 2.0467 - val_accuracy: 0.5806\n",
      "Epoch 185/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.2509 - accuracy: 0.9272 - val_loss: 2.0782 - val_accuracy: 0.5827\n",
      "Epoch 186/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.2469 - accuracy: 0.9265 - val_loss: 2.0867 - val_accuracy: 0.5847\n",
      "Epoch 187/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.2512 - accuracy: 0.9238 - val_loss: 2.0884 - val_accuracy: 0.5898\n",
      "Epoch 188/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.2460 - accuracy: 0.9272 - val_loss: 2.1016 - val_accuracy: 0.5806\n",
      "Epoch 189/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.2425 - accuracy: 0.9275 - val_loss: 2.1125 - val_accuracy: 0.5847\n",
      "Epoch 190/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.2425 - accuracy: 0.9299 - val_loss: 2.1146 - val_accuracy: 0.5776\n",
      "Epoch 191/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.2425 - accuracy: 0.9231 - val_loss: 2.1054 - val_accuracy: 0.5837\n",
      "Epoch 192/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.2415 - accuracy: 0.9265 - val_loss: 2.1530 - val_accuracy: 0.5806\n",
      "Epoch 193/10000\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.2398 - accuracy: 0.9323 - val_loss: 2.1295 - val_accuracy: 0.5837\n",
      "Epoch 194/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.2360 - accuracy: 0.9295 - val_loss: 2.1825 - val_accuracy: 0.5816\n",
      "Epoch 195/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.2359 - accuracy: 0.9299 - val_loss: 2.1562 - val_accuracy: 0.5847\n",
      "Epoch 196/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.2351 - accuracy: 0.9282 - val_loss: 2.1748 - val_accuracy: 0.5827\n",
      "Epoch 197/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.2309 - accuracy: 0.9323 - val_loss: 2.1829 - val_accuracy: 0.5857\n",
      "Epoch 198/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.2295 - accuracy: 0.9370 - val_loss: 2.2057 - val_accuracy: 0.5837\n",
      "Epoch 199/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.2337 - accuracy: 0.9295 - val_loss: 2.2089 - val_accuracy: 0.5755\n",
      "Epoch 200/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.2324 - accuracy: 0.9295 - val_loss: 2.2089 - val_accuracy: 0.5867\n",
      "Epoch 201/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.2262 - accuracy: 0.9353 - val_loss: 2.2076 - val_accuracy: 0.5867\n",
      "Epoch 202/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.2233 - accuracy: 0.9343 - val_loss: 2.2284 - val_accuracy: 0.5857\n",
      "Epoch 203/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.2203 - accuracy: 0.9374 - val_loss: 2.2445 - val_accuracy: 0.5878\n",
      "Epoch 204/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.2210 - accuracy: 0.9350 - val_loss: 2.2410 - val_accuracy: 0.5857\n",
      "Epoch 205/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.2193 - accuracy: 0.9364 - val_loss: 2.2621 - val_accuracy: 0.5816\n",
      "Epoch 206/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.2168 - accuracy: 0.9370 - val_loss: 2.2553 - val_accuracy: 0.5827\n",
      "Epoch 207/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.2144 - accuracy: 0.9387 - val_loss: 2.2900 - val_accuracy: 0.5857\n",
      "Epoch 208/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.2158 - accuracy: 0.9353 - val_loss: 2.3062 - val_accuracy: 0.5857\n",
      "Epoch 209/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.2178 - accuracy: 0.9340 - val_loss: 2.2916 - val_accuracy: 0.5888\n",
      "Epoch 210/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.2131 - accuracy: 0.9387 - val_loss: 2.3344 - val_accuracy: 0.5929\n",
      "Epoch 211/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.2152 - accuracy: 0.9381 - val_loss: 2.2938 - val_accuracy: 0.5867\n",
      "Epoch 212/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.2120 - accuracy: 0.9387 - val_loss: 2.3136 - val_accuracy: 0.5837\n",
      "Epoch 213/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.2111 - accuracy: 0.9381 - val_loss: 2.3482 - val_accuracy: 0.5867\n",
      "Epoch 214/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.2080 - accuracy: 0.9391 - val_loss: 2.3377 - val_accuracy: 0.5878\n",
      "Epoch 215/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.2039 - accuracy: 0.9411 - val_loss: 2.3407 - val_accuracy: 0.5857\n",
      "Epoch 216/10000\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.2064 - accuracy: 0.9384 - val_loss: 2.3495 - val_accuracy: 0.5827\n",
      "Epoch 217/10000\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.2061 - accuracy: 0.9411 - val_loss: 2.3728 - val_accuracy: 0.5867\n",
      "Epoch 218/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.2017 - accuracy: 0.9408 - val_loss: 2.3659 - val_accuracy: 0.5857\n",
      "Epoch 219/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.2022 - accuracy: 0.9404 - val_loss: 2.3656 - val_accuracy: 0.5878\n",
      "Epoch 220/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.1978 - accuracy: 0.9408 - val_loss: 2.3980 - val_accuracy: 0.5827\n",
      "Epoch 221/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1962 - accuracy: 0.9459 - val_loss: 2.3790 - val_accuracy: 0.5847\n",
      "Epoch 222/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1980 - accuracy: 0.9415 - val_loss: 2.3912 - val_accuracy: 0.5867\n",
      "Epoch 223/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.1962 - accuracy: 0.9421 - val_loss: 2.4084 - val_accuracy: 0.5908\n",
      "Epoch 224/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.1973 - accuracy: 0.9387 - val_loss: 2.4190 - val_accuracy: 0.5857\n",
      "Epoch 225/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1950 - accuracy: 0.9432 - val_loss: 2.4255 - val_accuracy: 0.5847\n",
      "Epoch 226/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.1925 - accuracy: 0.9469 - val_loss: 2.4546 - val_accuracy: 0.5847\n",
      "Epoch 227/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1976 - accuracy: 0.9391 - val_loss: 2.5036 - val_accuracy: 0.5888\n",
      "Epoch 228/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.1953 - accuracy: 0.9432 - val_loss: 2.4659 - val_accuracy: 0.5908\n",
      "Epoch 229/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 46ms/step - loss: 0.1979 - accuracy: 0.9418 - val_loss: 2.4489 - val_accuracy: 0.5888\n",
      "Epoch 230/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.1897 - accuracy: 0.9449 - val_loss: 2.4769 - val_accuracy: 0.5939\n",
      "Epoch 231/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.1913 - accuracy: 0.9442 - val_loss: 2.4951 - val_accuracy: 0.5908\n",
      "Epoch 232/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.1921 - accuracy: 0.9438 - val_loss: 2.4813 - val_accuracy: 0.5878\n",
      "Epoch 233/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1882 - accuracy: 0.9445 - val_loss: 2.4835 - val_accuracy: 0.5837\n",
      "Epoch 234/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.1846 - accuracy: 0.9489 - val_loss: 2.4790 - val_accuracy: 0.5888\n",
      "Epoch 235/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.1853 - accuracy: 0.9476 - val_loss: 2.5038 - val_accuracy: 0.5949\n",
      "Epoch 236/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.1815 - accuracy: 0.9466 - val_loss: 2.5262 - val_accuracy: 0.5908\n",
      "Epoch 237/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1805 - accuracy: 0.9503 - val_loss: 2.5091 - val_accuracy: 0.5898\n",
      "Epoch 238/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.1804 - accuracy: 0.9506 - val_loss: 2.5476 - val_accuracy: 0.5878\n",
      "Epoch 239/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1813 - accuracy: 0.9483 - val_loss: 2.5466 - val_accuracy: 0.5898\n",
      "Epoch 240/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.1837 - accuracy: 0.9513 - val_loss: 2.5616 - val_accuracy: 0.5857\n",
      "Epoch 241/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.1799 - accuracy: 0.9496 - val_loss: 2.5689 - val_accuracy: 0.5898\n",
      "Epoch 242/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.1778 - accuracy: 0.9513 - val_loss: 2.6061 - val_accuracy: 0.5918\n",
      "Epoch 243/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1799 - accuracy: 0.9445 - val_loss: 2.5751 - val_accuracy: 0.5898\n",
      "Epoch 244/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.1742 - accuracy: 0.9520 - val_loss: 2.6079 - val_accuracy: 0.5878\n",
      "Epoch 245/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.1731 - accuracy: 0.9486 - val_loss: 2.6036 - val_accuracy: 0.5929\n",
      "Epoch 246/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.1713 - accuracy: 0.9537 - val_loss: 2.6242 - val_accuracy: 0.5939\n",
      "Epoch 247/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.1671 - accuracy: 0.9551 - val_loss: 2.6247 - val_accuracy: 0.5918\n",
      "Epoch 248/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1684 - accuracy: 0.9547 - val_loss: 2.6361 - val_accuracy: 0.5918\n",
      "Epoch 249/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.1674 - accuracy: 0.9537 - val_loss: 2.6698 - val_accuracy: 0.5878\n",
      "Epoch 250/10000\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.1665 - accuracy: 0.9554 - val_loss: 2.6302 - val_accuracy: 0.5888\n",
      "Epoch 251/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.1634 - accuracy: 0.9558 - val_loss: 2.6489 - val_accuracy: 0.5959\n",
      "Epoch 252/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1615 - accuracy: 0.9578 - val_loss: 2.6783 - val_accuracy: 0.5837\n",
      "Epoch 253/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.1639 - accuracy: 0.9564 - val_loss: 2.6850 - val_accuracy: 0.5908\n",
      "Epoch 254/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.1601 - accuracy: 0.9585 - val_loss: 2.7025 - val_accuracy: 0.5898\n",
      "Epoch 255/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.1613 - accuracy: 0.9588 - val_loss: 2.7010 - val_accuracy: 0.5918\n",
      "Epoch 256/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1646 - accuracy: 0.9551 - val_loss: 2.6916 - val_accuracy: 0.5939\n",
      "Epoch 257/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.1602 - accuracy: 0.9571 - val_loss: 2.7090 - val_accuracy: 0.5929\n",
      "Epoch 258/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1606 - accuracy: 0.9537 - val_loss: 2.7100 - val_accuracy: 0.5918\n",
      "Epoch 259/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.1567 - accuracy: 0.9605 - val_loss: 2.7497 - val_accuracy: 0.5918\n",
      "Epoch 260/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.1561 - accuracy: 0.9592 - val_loss: 2.7296 - val_accuracy: 0.5949\n",
      "Epoch 261/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.1564 - accuracy: 0.9578 - val_loss: 2.7822 - val_accuracy: 0.5816\n",
      "Epoch 262/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.1622 - accuracy: 0.9547 - val_loss: 2.7397 - val_accuracy: 0.5929\n",
      "Epoch 263/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.1561 - accuracy: 0.9571 - val_loss: 2.7862 - val_accuracy: 0.5959\n",
      "Epoch 264/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1527 - accuracy: 0.9595 - val_loss: 2.8014 - val_accuracy: 0.5929\n",
      "Epoch 265/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.1533 - accuracy: 0.9578 - val_loss: 2.8163 - val_accuracy: 0.5939\n",
      "Epoch 266/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.1537 - accuracy: 0.9605 - val_loss: 2.8447 - val_accuracy: 0.5939\n",
      "Epoch 267/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1478 - accuracy: 0.9612 - val_loss: 2.8522 - val_accuracy: 0.5898\n",
      "Epoch 268/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.1467 - accuracy: 0.9632 - val_loss: 2.8310 - val_accuracy: 0.5908\n",
      "Epoch 269/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.1476 - accuracy: 0.9615 - val_loss: 2.8403 - val_accuracy: 0.5939\n",
      "Epoch 270/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1425 - accuracy: 0.9636 - val_loss: 2.8599 - val_accuracy: 0.5918\n",
      "Epoch 271/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.1414 - accuracy: 0.9670 - val_loss: 2.8998 - val_accuracy: 0.5939\n",
      "Epoch 272/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.1417 - accuracy: 0.9656 - val_loss: 2.8877 - val_accuracy: 0.5969\n",
      "Epoch 273/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.1440 - accuracy: 0.9609 - val_loss: 2.9298 - val_accuracy: 0.5980\n",
      "Epoch 274/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.1413 - accuracy: 0.9646 - val_loss: 2.9310 - val_accuracy: 0.5959\n",
      "Epoch 275/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.1428 - accuracy: 0.9578 - val_loss: 2.9182 - val_accuracy: 0.5929\n",
      "Epoch 276/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.1403 - accuracy: 0.9639 - val_loss: 2.9158 - val_accuracy: 0.5908\n",
      "Epoch 277/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.1381 - accuracy: 0.9653 - val_loss: 2.9280 - val_accuracy: 0.5908\n",
      "Epoch 278/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1402 - accuracy: 0.9626 - val_loss: 2.9328 - val_accuracy: 0.5867\n",
      "Epoch 279/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.1443 - accuracy: 0.9595 - val_loss: 2.9726 - val_accuracy: 0.5888\n",
      "Epoch 280/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.1374 - accuracy: 0.9677 - val_loss: 2.9512 - val_accuracy: 0.5939\n",
      "Epoch 281/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.1378 - accuracy: 0.9636 - val_loss: 2.9502 - val_accuracy: 0.5878\n",
      "Epoch 282/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.1364 - accuracy: 0.9649 - val_loss: 2.9523 - val_accuracy: 0.5949\n",
      "Epoch 283/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.1372 - accuracy: 0.9670 - val_loss: 2.9544 - val_accuracy: 0.5939\n",
      "Epoch 284/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.1346 - accuracy: 0.9670 - val_loss: 2.9897 - val_accuracy: 0.5949\n",
      "Epoch 285/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.1305 - accuracy: 0.9680 - val_loss: 3.0174 - val_accuracy: 0.5949\n",
      "Epoch 286/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 49ms/step - loss: 0.1307 - accuracy: 0.9656 - val_loss: 3.0159 - val_accuracy: 0.5918\n",
      "Epoch 287/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1259 - accuracy: 0.9697 - val_loss: 3.0116 - val_accuracy: 0.5888\n",
      "Epoch 288/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.1269 - accuracy: 0.9704 - val_loss: 3.0346 - val_accuracy: 0.5837\n",
      "Epoch 289/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.1347 - accuracy: 0.9629 - val_loss: 3.0262 - val_accuracy: 0.5867\n",
      "Epoch 290/10000\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.1387 - accuracy: 0.9636 - val_loss: 3.0635 - val_accuracy: 0.5898\n",
      "Epoch 291/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.1320 - accuracy: 0.9660 - val_loss: 3.0543 - val_accuracy: 0.5857\n",
      "Epoch 292/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1308 - accuracy: 0.9663 - val_loss: 3.1053 - val_accuracy: 0.5847\n",
      "Epoch 293/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.1297 - accuracy: 0.9680 - val_loss: 3.0622 - val_accuracy: 0.5929\n",
      "Epoch 294/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.1268 - accuracy: 0.9680 - val_loss: 3.0999 - val_accuracy: 0.5837\n",
      "Epoch 295/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.1316 - accuracy: 0.9632 - val_loss: 3.1145 - val_accuracy: 0.6010\n",
      "Epoch 296/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.1331 - accuracy: 0.9639 - val_loss: 3.1414 - val_accuracy: 0.5929\n",
      "Epoch 297/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.1348 - accuracy: 0.9636 - val_loss: 3.1549 - val_accuracy: 0.6000\n",
      "Epoch 298/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.1371 - accuracy: 0.9622 - val_loss: 3.1260 - val_accuracy: 0.5980\n",
      "Epoch 299/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.1414 - accuracy: 0.9602 - val_loss: 3.1765 - val_accuracy: 0.5857\n",
      "Epoch 300/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.1334 - accuracy: 0.9639 - val_loss: 3.1931 - val_accuracy: 0.5939\n",
      "Epoch 301/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.1229 - accuracy: 0.9694 - val_loss: 3.1659 - val_accuracy: 0.5888\n",
      "Epoch 302/10000\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.1185 - accuracy: 0.9694 - val_loss: 3.1376 - val_accuracy: 0.5878\n",
      "Epoch 303/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.1161 - accuracy: 0.9724 - val_loss: 3.1796 - val_accuracy: 0.5827\n",
      "Epoch 304/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.1141 - accuracy: 0.9724 - val_loss: 3.1514 - val_accuracy: 0.5898\n",
      "Epoch 305/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.1152 - accuracy: 0.9721 - val_loss: 3.2080 - val_accuracy: 0.5939\n",
      "Epoch 306/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.1159 - accuracy: 0.9724 - val_loss: 3.2137 - val_accuracy: 0.5837\n",
      "Epoch 307/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.1108 - accuracy: 0.9735 - val_loss: 3.2072 - val_accuracy: 0.5908\n",
      "Epoch 308/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.1090 - accuracy: 0.9772 - val_loss: 3.2339 - val_accuracy: 0.5939\n",
      "Epoch 309/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.1098 - accuracy: 0.9769 - val_loss: 3.2589 - val_accuracy: 0.5918\n",
      "Epoch 310/10000\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.1090 - accuracy: 0.9782 - val_loss: 3.2590 - val_accuracy: 0.5969\n",
      "Epoch 311/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.1071 - accuracy: 0.9762 - val_loss: 3.2943 - val_accuracy: 0.5918\n",
      "Epoch 312/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1071 - accuracy: 0.9758 - val_loss: 3.3073 - val_accuracy: 0.5898\n",
      "Epoch 313/10000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.1079 - accuracy: 0.9762 - val_loss: 3.2914 - val_accuracy: 0.5918\n",
      "Epoch 314/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.1072 - accuracy: 0.9782 - val_loss: 3.3222 - val_accuracy: 0.5929\n",
      "Epoch 315/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.1078 - accuracy: 0.9741 - val_loss: 3.2984 - val_accuracy: 0.5888\n",
      "Epoch 316/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.1134 - accuracy: 0.9745 - val_loss: 3.2943 - val_accuracy: 0.5898\n",
      "Epoch 317/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.1137 - accuracy: 0.9735 - val_loss: 3.3134 - val_accuracy: 0.5908\n",
      "Epoch 318/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.1096 - accuracy: 0.9731 - val_loss: 3.3476 - val_accuracy: 0.5888\n",
      "Epoch 319/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.1091 - accuracy: 0.9748 - val_loss: 3.3599 - val_accuracy: 0.5918\n",
      "Epoch 320/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1103 - accuracy: 0.9769 - val_loss: 3.3591 - val_accuracy: 0.5980\n",
      "Epoch 321/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.1082 - accuracy: 0.9741 - val_loss: 3.3923 - val_accuracy: 0.5949\n",
      "Epoch 322/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.1082 - accuracy: 0.9769 - val_loss: 3.3705 - val_accuracy: 0.5949\n",
      "Epoch 323/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1037 - accuracy: 0.9765 - val_loss: 3.3861 - val_accuracy: 0.5939\n",
      "Epoch 324/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.1065 - accuracy: 0.9765 - val_loss: 3.4030 - val_accuracy: 0.5918\n",
      "Epoch 325/10000\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.1030 - accuracy: 0.9765 - val_loss: 3.4347 - val_accuracy: 0.5939\n",
      "Epoch 326/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0998 - accuracy: 0.9803 - val_loss: 3.4500 - val_accuracy: 0.5898\n",
      "Epoch 327/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0972 - accuracy: 0.9806 - val_loss: 3.4365 - val_accuracy: 0.5908\n",
      "Epoch 328/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.1000 - accuracy: 0.9786 - val_loss: 3.4811 - val_accuracy: 0.5888\n",
      "Epoch 329/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.1029 - accuracy: 0.9748 - val_loss: 3.4956 - val_accuracy: 0.5939\n",
      "Epoch 330/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.1003 - accuracy: 0.9772 - val_loss: 3.5066 - val_accuracy: 0.5888\n",
      "Epoch 331/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0979 - accuracy: 0.9786 - val_loss: 3.4861 - val_accuracy: 0.5939\n",
      "Epoch 332/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0927 - accuracy: 0.9813 - val_loss: 3.5114 - val_accuracy: 0.5939\n",
      "Epoch 333/10000\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 0.0924 - accuracy: 0.9813 - val_loss: 3.5364 - val_accuracy: 0.5949\n",
      "Epoch 334/10000\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0907 - accuracy: 0.9806 - val_loss: 3.5237 - val_accuracy: 0.5949\n",
      "Epoch 335/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0889 - accuracy: 0.9826 - val_loss: 3.5422 - val_accuracy: 0.5929\n",
      "Epoch 336/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0883 - accuracy: 0.9813 - val_loss: 3.5454 - val_accuracy: 0.5929\n",
      "Epoch 337/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0918 - accuracy: 0.9796 - val_loss: 3.5506 - val_accuracy: 0.5929\n",
      "Epoch 338/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.0877 - accuracy: 0.9816 - val_loss: 3.5772 - val_accuracy: 0.5959\n",
      "Epoch 339/10000\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.0866 - accuracy: 0.9813 - val_loss: 3.5834 - val_accuracy: 0.5918\n",
      "Epoch 340/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0859 - accuracy: 0.9833 - val_loss: 3.5784 - val_accuracy: 0.5929\n",
      "Epoch 341/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0851 - accuracy: 0.9823 - val_loss: 3.6148 - val_accuracy: 0.5908\n",
      "Epoch 342/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0834 - accuracy: 0.9847 - val_loss: 3.6043 - val_accuracy: 0.5929\n",
      "Epoch 343/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0833 - accuracy: 0.9830 - val_loss: 3.6147 - val_accuracy: 0.5959\n",
      "Epoch 344/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0853 - accuracy: 0.9830 - val_loss: 3.6352 - val_accuracy: 0.5990\n",
      "Epoch 345/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0842 - accuracy: 0.9840 - val_loss: 3.6378 - val_accuracy: 0.6000\n",
      "Epoch 346/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0818 - accuracy: 0.9837 - val_loss: 3.6576 - val_accuracy: 0.5929\n",
      "Epoch 347/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0816 - accuracy: 0.9816 - val_loss: 3.6955 - val_accuracy: 0.5939\n",
      "Epoch 348/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0821 - accuracy: 0.9806 - val_loss: 3.6917 - val_accuracy: 0.5969\n",
      "Epoch 349/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0821 - accuracy: 0.9833 - val_loss: 3.6719 - val_accuracy: 0.5949\n",
      "Epoch 350/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0822 - accuracy: 0.9816 - val_loss: 3.7037 - val_accuracy: 0.5969\n",
      "Epoch 351/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0856 - accuracy: 0.9830 - val_loss: 3.6864 - val_accuracy: 0.5878\n",
      "Epoch 352/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0851 - accuracy: 0.9826 - val_loss: 3.6926 - val_accuracy: 0.5980\n",
      "Epoch 353/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0871 - accuracy: 0.9806 - val_loss: 3.6982 - val_accuracy: 0.5949\n",
      "Epoch 354/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0926 - accuracy: 0.9789 - val_loss: 3.6923 - val_accuracy: 0.5969\n",
      "Epoch 355/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0908 - accuracy: 0.9789 - val_loss: 3.7001 - val_accuracy: 0.5939\n",
      "Epoch 356/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0899 - accuracy: 0.9809 - val_loss: 3.7241 - val_accuracy: 0.5908\n",
      "Epoch 357/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0926 - accuracy: 0.9769 - val_loss: 3.7085 - val_accuracy: 0.5959\n",
      "Epoch 358/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0860 - accuracy: 0.9813 - val_loss: 3.7486 - val_accuracy: 0.5888\n",
      "Epoch 359/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0852 - accuracy: 0.9813 - val_loss: 3.7527 - val_accuracy: 0.5878\n",
      "Epoch 360/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0868 - accuracy: 0.9789 - val_loss: 3.7681 - val_accuracy: 0.5980\n",
      "Epoch 361/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0841 - accuracy: 0.9833 - val_loss: 3.8407 - val_accuracy: 0.5949\n",
      "Epoch 362/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0853 - accuracy: 0.9803 - val_loss: 3.8684 - val_accuracy: 0.5878\n",
      "Epoch 363/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0904 - accuracy: 0.9820 - val_loss: 3.8625 - val_accuracy: 0.5949\n",
      "Epoch 364/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0895 - accuracy: 0.9758 - val_loss: 3.8416 - val_accuracy: 0.5918\n",
      "Epoch 365/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0833 - accuracy: 0.9833 - val_loss: 3.8424 - val_accuracy: 0.5878\n",
      "Epoch 366/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0798 - accuracy: 0.9830 - val_loss: 3.8324 - val_accuracy: 0.5908\n",
      "Epoch 367/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0772 - accuracy: 0.9837 - val_loss: 3.8594 - val_accuracy: 0.5898\n",
      "Epoch 368/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0773 - accuracy: 0.9833 - val_loss: 3.8572 - val_accuracy: 0.5949\n",
      "Epoch 369/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0760 - accuracy: 0.9850 - val_loss: 3.8703 - val_accuracy: 0.5929\n",
      "Epoch 370/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0738 - accuracy: 0.9843 - val_loss: 3.8844 - val_accuracy: 0.5898\n",
      "Epoch 371/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0723 - accuracy: 0.9850 - val_loss: 3.8756 - val_accuracy: 0.5929\n",
      "Epoch 372/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0730 - accuracy: 0.9857 - val_loss: 3.9333 - val_accuracy: 0.5908\n",
      "Epoch 373/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0719 - accuracy: 0.9857 - val_loss: 3.9056 - val_accuracy: 0.5939\n",
      "Epoch 374/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0713 - accuracy: 0.9860 - val_loss: 3.9042 - val_accuracy: 0.5929\n",
      "Epoch 375/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0733 - accuracy: 0.9830 - val_loss: 3.9554 - val_accuracy: 0.5918\n",
      "Epoch 376/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0748 - accuracy: 0.9871 - val_loss: 3.9636 - val_accuracy: 0.5949\n",
      "Epoch 377/10000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0708 - accuracy: 0.9843 - val_loss: 3.9668 - val_accuracy: 0.5949\n",
      "Epoch 378/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0725 - accuracy: 0.9864 - val_loss: 3.9865 - val_accuracy: 0.5980\n",
      "Epoch 379/10000\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.0803 - accuracy: 0.9833 - val_loss: 3.9992 - val_accuracy: 0.5959\n",
      "Epoch 380/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0772 - accuracy: 0.9823 - val_loss: 3.9957 - val_accuracy: 0.6010\n",
      "Epoch 381/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0747 - accuracy: 0.9830 - val_loss: 4.0360 - val_accuracy: 0.5959\n",
      "Epoch 382/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0711 - accuracy: 0.9843 - val_loss: 4.0234 - val_accuracy: 0.5949\n",
      "Epoch 383/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0724 - accuracy: 0.9867 - val_loss: 4.0445 - val_accuracy: 0.5908\n",
      "Epoch 384/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0706 - accuracy: 0.9854 - val_loss: 4.0543 - val_accuracy: 0.5878\n",
      "Epoch 385/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.0685 - accuracy: 0.9871 - val_loss: 4.0803 - val_accuracy: 0.5949\n",
      "Epoch 386/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0679 - accuracy: 0.9860 - val_loss: 4.0695 - val_accuracy: 0.5918\n",
      "Epoch 387/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0691 - accuracy: 0.9857 - val_loss: 4.0587 - val_accuracy: 0.5969\n",
      "Epoch 388/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0712 - accuracy: 0.9840 - val_loss: 4.1290 - val_accuracy: 0.5980\n",
      "Epoch 389/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0748 - accuracy: 0.9867 - val_loss: 4.0453 - val_accuracy: 0.5888\n",
      "Epoch 390/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0709 - accuracy: 0.9867 - val_loss: 4.1021 - val_accuracy: 0.5949\n",
      "Epoch 391/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0670 - accuracy: 0.9860 - val_loss: 4.1571 - val_accuracy: 0.5929\n",
      "Epoch 392/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0635 - accuracy: 0.9864 - val_loss: 4.1374 - val_accuracy: 0.5959\n",
      "Epoch 393/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.0638 - accuracy: 0.9871 - val_loss: 4.1265 - val_accuracy: 0.6000\n",
      "Epoch 394/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0615 - accuracy: 0.9874 - val_loss: 4.1599 - val_accuracy: 0.5918\n",
      "Epoch 395/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0619 - accuracy: 0.9891 - val_loss: 4.1824 - val_accuracy: 0.5959\n",
      "Epoch 396/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0614 - accuracy: 0.9881 - val_loss: 4.1951 - val_accuracy: 0.5949\n",
      "Epoch 397/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.0596 - accuracy: 0.9888 - val_loss: 4.1884 - val_accuracy: 0.5969\n",
      "Epoch 398/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0586 - accuracy: 0.9894 - val_loss: 4.2235 - val_accuracy: 0.5939\n",
      "Epoch 399/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0554 - accuracy: 0.9898 - val_loss: 4.2134 - val_accuracy: 0.5980\n",
      "Epoch 400/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 62ms/step - loss: 0.0564 - accuracy: 0.9894 - val_loss: 4.2673 - val_accuracy: 0.5939\n",
      "Epoch 401/10000\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.0589 - accuracy: 0.9888 - val_loss: 4.3001 - val_accuracy: 0.5980\n",
      "Epoch 402/10000\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.0586 - accuracy: 0.9894 - val_loss: 4.2624 - val_accuracy: 0.5918\n",
      "Epoch 403/10000\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.0578 - accuracy: 0.9888 - val_loss: 4.2511 - val_accuracy: 0.5939\n",
      "Epoch 404/10000\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.0590 - accuracy: 0.9884 - val_loss: 4.2776 - val_accuracy: 0.5929\n",
      "Epoch 405/10000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0553 - accuracy: 0.9905 - val_loss: 4.2659 - val_accuracy: 0.5929\n",
      "Epoch 406/10000\n",
      "6/6 [==============================] - 0s 86ms/step - loss: 0.0557 - accuracy: 0.9894 - val_loss: 4.2932 - val_accuracy: 0.5939\n",
      "Epoch 407/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0539 - accuracy: 0.9905 - val_loss: 4.3141 - val_accuracy: 0.5969\n",
      "Epoch 408/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0570 - accuracy: 0.9888 - val_loss: 4.3843 - val_accuracy: 0.5969\n",
      "Epoch 409/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0598 - accuracy: 0.9877 - val_loss: 4.3959 - val_accuracy: 0.5969\n",
      "Epoch 410/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0628 - accuracy: 0.9864 - val_loss: 4.3627 - val_accuracy: 0.5908\n",
      "Epoch 411/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0624 - accuracy: 0.9867 - val_loss: 4.3512 - val_accuracy: 0.5969\n",
      "Epoch 412/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0567 - accuracy: 0.9894 - val_loss: 4.3738 - val_accuracy: 0.5969\n",
      "Epoch 413/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0585 - accuracy: 0.9877 - val_loss: 4.4122 - val_accuracy: 0.5939\n",
      "Epoch 414/10000\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.0602 - accuracy: 0.9888 - val_loss: 4.3957 - val_accuracy: 0.5929\n",
      "Epoch 415/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0571 - accuracy: 0.9877 - val_loss: 4.4141 - val_accuracy: 0.5949\n",
      "Epoch 416/10000\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.0558 - accuracy: 0.9898 - val_loss: 4.4164 - val_accuracy: 0.5918\n",
      "Epoch 417/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0558 - accuracy: 0.9884 - val_loss: 4.3723 - val_accuracy: 0.5908\n",
      "Epoch 418/10000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0535 - accuracy: 0.9881 - val_loss: 4.3828 - val_accuracy: 0.5959\n",
      "Epoch 419/10000\n",
      "6/6 [==============================] - 0s 91ms/step - loss: 0.0534 - accuracy: 0.9888 - val_loss: 4.3925 - val_accuracy: 0.5918\n",
      "Epoch 420/10000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0519 - accuracy: 0.9905 - val_loss: 4.4113 - val_accuracy: 0.5959\n",
      "Epoch 421/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0573 - accuracy: 0.9888 - val_loss: 4.4624 - val_accuracy: 0.5949\n",
      "Epoch 422/10000\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 0.0550 - accuracy: 0.9888 - val_loss: 4.4539 - val_accuracy: 0.5980\n",
      "Epoch 423/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0568 - accuracy: 0.9888 - val_loss: 4.4323 - val_accuracy: 0.5888\n",
      "Epoch 424/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0543 - accuracy: 0.9894 - val_loss: 4.4297 - val_accuracy: 0.5969\n",
      "Epoch 425/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0515 - accuracy: 0.9898 - val_loss: 4.4384 - val_accuracy: 0.5908\n",
      "Epoch 426/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0502 - accuracy: 0.9905 - val_loss: 4.4883 - val_accuracy: 0.5959\n",
      "Epoch 427/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0475 - accuracy: 0.9925 - val_loss: 4.5462 - val_accuracy: 0.5929\n",
      "Epoch 428/10000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0483 - accuracy: 0.9905 - val_loss: 4.5251 - val_accuracy: 0.5939\n",
      "Epoch 429/10000\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.0464 - accuracy: 0.9925 - val_loss: 4.5513 - val_accuracy: 0.5949\n",
      "Epoch 430/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0475 - accuracy: 0.9912 - val_loss: 4.5553 - val_accuracy: 0.5898\n",
      "Epoch 431/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0482 - accuracy: 0.9908 - val_loss: 4.5925 - val_accuracy: 0.5908\n",
      "Epoch 432/10000\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.0471 - accuracy: 0.9912 - val_loss: 4.5355 - val_accuracy: 0.5939\n",
      "Epoch 433/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0447 - accuracy: 0.9915 - val_loss: 4.5881 - val_accuracy: 0.5918\n",
      "Epoch 434/10000\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.0442 - accuracy: 0.9922 - val_loss: 4.5985 - val_accuracy: 0.5918\n",
      "Epoch 435/10000\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 0.0433 - accuracy: 0.9922 - val_loss: 4.6022 - val_accuracy: 0.5969\n",
      "Epoch 436/10000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0428 - accuracy: 0.9929 - val_loss: 4.6029 - val_accuracy: 0.5918\n",
      "Epoch 437/10000\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.0426 - accuracy: 0.9922 - val_loss: 4.6203 - val_accuracy: 0.5918\n",
      "Epoch 438/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0431 - accuracy: 0.9918 - val_loss: 4.6285 - val_accuracy: 0.5888\n",
      "Epoch 439/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0453 - accuracy: 0.9908 - val_loss: 4.6580 - val_accuracy: 0.5939\n",
      "Epoch 440/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0481 - accuracy: 0.9908 - val_loss: 4.6008 - val_accuracy: 0.5949\n",
      "Epoch 441/10000\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.0504 - accuracy: 0.9901 - val_loss: 4.6680 - val_accuracy: 0.5949\n",
      "Epoch 442/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0472 - accuracy: 0.9908 - val_loss: 4.6028 - val_accuracy: 0.5939\n",
      "Epoch 443/10000\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 0.0478 - accuracy: 0.9915 - val_loss: 4.6868 - val_accuracy: 0.5918\n",
      "Epoch 444/10000\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.0472 - accuracy: 0.9908 - val_loss: 4.6845 - val_accuracy: 0.5888\n",
      "Epoch 445/10000\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.0471 - accuracy: 0.9908 - val_loss: 4.7205 - val_accuracy: 0.5939\n",
      "Epoch 446/10000\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 0.0470 - accuracy: 0.9912 - val_loss: 4.7196 - val_accuracy: 0.5939\n",
      "Epoch 447/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0451 - accuracy: 0.9918 - val_loss: 4.7237 - val_accuracy: 0.5990\n",
      "Epoch 448/10000\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.0426 - accuracy: 0.9929 - val_loss: 4.7410 - val_accuracy: 0.5918\n",
      "Epoch 449/10000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0422 - accuracy: 0.9929 - val_loss: 4.7618 - val_accuracy: 0.5949\n",
      "Epoch 450/10000\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.0420 - accuracy: 0.9925 - val_loss: 4.7581 - val_accuracy: 0.5980\n",
      "Epoch 451/10000\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.0415 - accuracy: 0.9922 - val_loss: 4.7817 - val_accuracy: 0.5959\n",
      "Epoch 452/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0416 - accuracy: 0.9932 - val_loss: 4.7458 - val_accuracy: 0.5969\n",
      "Epoch 453/10000\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.0424 - accuracy: 0.9922 - val_loss: 4.8074 - val_accuracy: 0.5929\n",
      "Epoch 454/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0412 - accuracy: 0.9912 - val_loss: 4.7982 - val_accuracy: 0.6010\n",
      "Epoch 455/10000\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.0432 - accuracy: 0.9915 - val_loss: 4.8475 - val_accuracy: 0.5929\n",
      "Epoch 456/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0455 - accuracy: 0.9908 - val_loss: 4.8298 - val_accuracy: 0.5969\n",
      "Epoch 457/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0448 - accuracy: 0.9918 - val_loss: 4.8663 - val_accuracy: 0.5959\n",
      "Epoch 458/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0456 - accuracy: 0.9915 - val_loss: 4.9075 - val_accuracy: 0.5969\n",
      "Epoch 459/10000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0525 - accuracy: 0.9891 - val_loss: 4.8521 - val_accuracy: 0.5929\n",
      "Epoch 460/10000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0477 - accuracy: 0.9912 - val_loss: 4.8502 - val_accuracy: 0.6020\n",
      "Epoch 461/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0467 - accuracy: 0.9901 - val_loss: 4.8796 - val_accuracy: 0.5929\n",
      "Epoch 462/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0409 - accuracy: 0.9942 - val_loss: 4.9165 - val_accuracy: 0.5959\n",
      "Epoch 463/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0435 - accuracy: 0.9918 - val_loss: 4.9134 - val_accuracy: 0.5939\n",
      "Epoch 464/10000\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.0380 - accuracy: 0.9929 - val_loss: 4.9217 - val_accuracy: 0.6000\n",
      "Epoch 465/10000\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.0396 - accuracy: 0.9922 - val_loss: 4.9441 - val_accuracy: 0.5949\n",
      "Epoch 466/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0375 - accuracy: 0.9939 - val_loss: 4.8783 - val_accuracy: 0.5969\n",
      "Epoch 467/10000\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0369 - accuracy: 0.9925 - val_loss: 4.9490 - val_accuracy: 0.5959\n",
      "Epoch 468/10000\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.0356 - accuracy: 0.9935 - val_loss: 4.9603 - val_accuracy: 0.5969\n",
      "Epoch 469/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0351 - accuracy: 0.9925 - val_loss: 4.9572 - val_accuracy: 0.5949\n",
      "Epoch 470/10000\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.0348 - accuracy: 0.9932 - val_loss: 5.0297 - val_accuracy: 0.5949\n",
      "Epoch 471/10000\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.0343 - accuracy: 0.9942 - val_loss: 5.0392 - val_accuracy: 0.5949\n",
      "Epoch 472/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0351 - accuracy: 0.9925 - val_loss: 5.0259 - val_accuracy: 0.5959\n",
      "Epoch 473/10000\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.0353 - accuracy: 0.9915 - val_loss: 5.0567 - val_accuracy: 0.5918\n",
      "Epoch 474/10000\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.0354 - accuracy: 0.9939 - val_loss: 5.0799 - val_accuracy: 0.5939\n",
      "Epoch 475/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0337 - accuracy: 0.9935 - val_loss: 5.0163 - val_accuracy: 0.5959\n",
      "Epoch 476/10000\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.0335 - accuracy: 0.9935 - val_loss: 5.0343 - val_accuracy: 0.5980\n",
      "Epoch 477/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0344 - accuracy: 0.9935 - val_loss: 5.0954 - val_accuracy: 0.5939\n",
      "Epoch 478/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0331 - accuracy: 0.9946 - val_loss: 5.0318 - val_accuracy: 0.5959\n",
      "Epoch 479/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0356 - accuracy: 0.9932 - val_loss: 5.0539 - val_accuracy: 0.5980\n",
      "Epoch 480/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0494 - accuracy: 0.9908 - val_loss: 5.0311 - val_accuracy: 0.5908\n",
      "Epoch 481/10000\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.0519 - accuracy: 0.9884 - val_loss: 5.0039 - val_accuracy: 0.5867\n",
      "Epoch 482/10000\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0660 - accuracy: 0.9837 - val_loss: 5.0843 - val_accuracy: 0.5827\n",
      "Epoch 483/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0713 - accuracy: 0.9813 - val_loss: 5.0823 - val_accuracy: 0.5918\n",
      "Epoch 484/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0634 - accuracy: 0.9816 - val_loss: 4.9946 - val_accuracy: 0.5908\n",
      "Epoch 485/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0701 - accuracy: 0.9789 - val_loss: 5.1018 - val_accuracy: 0.5929\n",
      "Epoch 486/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0735 - accuracy: 0.9786 - val_loss: 5.0142 - val_accuracy: 0.5918\n",
      "Epoch 487/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0796 - accuracy: 0.9775 - val_loss: 5.1721 - val_accuracy: 0.5806\n",
      "Epoch 488/10000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0994 - accuracy: 0.9687 - val_loss: 5.0211 - val_accuracy: 0.5806\n",
      "Epoch 489/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.0789 - accuracy: 0.9762 - val_loss: 4.9739 - val_accuracy: 0.5857\n",
      "Epoch 490/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0864 - accuracy: 0.9731 - val_loss: 5.0865 - val_accuracy: 0.5857\n",
      "Epoch 491/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0698 - accuracy: 0.9803 - val_loss: 5.1050 - val_accuracy: 0.5888\n",
      "Epoch 492/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0759 - accuracy: 0.9755 - val_loss: 5.1528 - val_accuracy: 0.5837\n",
      "Epoch 493/10000\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.0762 - accuracy: 0.9779 - val_loss: 5.2123 - val_accuracy: 0.5857\n",
      "Epoch 494/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0668 - accuracy: 0.9799 - val_loss: 5.2373 - val_accuracy: 0.5847\n",
      "Epoch 495/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0617 - accuracy: 0.9850 - val_loss: 5.1445 - val_accuracy: 0.5837\n",
      "Epoch 496/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0540 - accuracy: 0.9871 - val_loss: 5.0927 - val_accuracy: 0.5959\n",
      "Epoch 497/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0491 - accuracy: 0.9905 - val_loss: 5.1251 - val_accuracy: 0.5888\n",
      "Epoch 498/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0474 - accuracy: 0.9877 - val_loss: 5.1041 - val_accuracy: 0.5990\n",
      "Epoch 499/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0465 - accuracy: 0.9888 - val_loss: 5.0952 - val_accuracy: 0.5888\n",
      "Epoch 500/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0445 - accuracy: 0.9912 - val_loss: 5.1238 - val_accuracy: 0.5929\n",
      "Epoch 501/10000\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.0393 - accuracy: 0.9932 - val_loss: 5.2207 - val_accuracy: 0.5949\n",
      "Epoch 502/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0351 - accuracy: 0.9949 - val_loss: 5.1722 - val_accuracy: 0.5990\n",
      "Epoch 503/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0349 - accuracy: 0.9939 - val_loss: 5.1573 - val_accuracy: 0.6000\n",
      "Epoch 504/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0332 - accuracy: 0.9932 - val_loss: 5.1775 - val_accuracy: 0.5959\n",
      "Epoch 505/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0328 - accuracy: 0.9942 - val_loss: 5.2321 - val_accuracy: 0.5918\n",
      "Epoch 506/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0326 - accuracy: 0.9935 - val_loss: 5.2670 - val_accuracy: 0.5949\n",
      "Epoch 507/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0323 - accuracy: 0.9939 - val_loss: 5.3442 - val_accuracy: 0.5908\n",
      "Epoch 508/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0398 - accuracy: 0.9925 - val_loss: 5.2821 - val_accuracy: 0.5969\n",
      "Epoch 509/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0399 - accuracy: 0.9915 - val_loss: 5.2367 - val_accuracy: 0.5939\n",
      "Epoch 510/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0353 - accuracy: 0.9925 - val_loss: 5.2133 - val_accuracy: 0.5949\n",
      "Epoch 511/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0339 - accuracy: 0.9939 - val_loss: 5.2152 - val_accuracy: 0.5939\n",
      "Epoch 512/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0329 - accuracy: 0.9939 - val_loss: 5.3016 - val_accuracy: 0.5918\n",
      "Epoch 513/10000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0295 - accuracy: 0.9946 - val_loss: 5.3050 - val_accuracy: 0.5959\n",
      "Epoch 514/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0285 - accuracy: 0.9946 - val_loss: 5.3216 - val_accuracy: 0.5939\n",
      "Epoch 515/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0289 - accuracy: 0.9949 - val_loss: 5.2984 - val_accuracy: 0.6020\n",
      "Epoch 516/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0300 - accuracy: 0.9935 - val_loss: 5.3174 - val_accuracy: 0.5949\n",
      "Epoch 517/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0296 - accuracy: 0.9946 - val_loss: 5.3697 - val_accuracy: 0.5980\n",
      "Epoch 518/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0276 - accuracy: 0.9946 - val_loss: 5.3823 - val_accuracy: 0.5908\n",
      "Epoch 519/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0291 - accuracy: 0.9952 - val_loss: 5.3465 - val_accuracy: 0.5980\n",
      "Epoch 520/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0268 - accuracy: 0.9956 - val_loss: 5.3838 - val_accuracy: 0.5939\n",
      "Epoch 521/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0241 - accuracy: 0.9952 - val_loss: 5.3883 - val_accuracy: 0.5980\n",
      "Epoch 522/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0243 - accuracy: 0.9946 - val_loss: 5.3674 - val_accuracy: 0.5949\n",
      "Epoch 523/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0250 - accuracy: 0.9949 - val_loss: 5.4149 - val_accuracy: 0.5949\n",
      "Epoch 524/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0238 - accuracy: 0.9956 - val_loss: 5.3926 - val_accuracy: 0.5990\n",
      "Epoch 525/10000\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0236 - accuracy: 0.9949 - val_loss: 5.4105 - val_accuracy: 0.6000\n",
      "Epoch 526/10000\n",
      "6/6 [==============================] - 0s 84ms/step - loss: 0.0246 - accuracy: 0.9942 - val_loss: 5.4304 - val_accuracy: 0.5939\n",
      "Epoch 527/10000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0247 - accuracy: 0.9956 - val_loss: 5.4132 - val_accuracy: 0.5969\n",
      "Epoch 528/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0261 - accuracy: 0.9949 - val_loss: 5.4936 - val_accuracy: 0.5939\n",
      "Epoch 529/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0252 - accuracy: 0.9952 - val_loss: 5.4556 - val_accuracy: 0.5939\n",
      "Epoch 530/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0248 - accuracy: 0.9956 - val_loss: 5.4363 - val_accuracy: 0.5949\n",
      "Epoch 531/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0237 - accuracy: 0.9946 - val_loss: 5.4783 - val_accuracy: 0.5949\n",
      "Epoch 532/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0252 - accuracy: 0.9946 - val_loss: 5.4683 - val_accuracy: 0.5949\n",
      "Epoch 533/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0231 - accuracy: 0.9963 - val_loss: 5.4475 - val_accuracy: 0.5929\n",
      "Epoch 534/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0259 - accuracy: 0.9949 - val_loss: 5.4490 - val_accuracy: 0.5969\n",
      "Epoch 535/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0349 - accuracy: 0.9929 - val_loss: 5.4804 - val_accuracy: 0.5959\n",
      "Epoch 536/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0549 - accuracy: 0.9860 - val_loss: 5.3952 - val_accuracy: 0.5867\n",
      "Epoch 537/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0630 - accuracy: 0.9826 - val_loss: 5.4638 - val_accuracy: 0.5837\n",
      "Epoch 538/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.1087 - accuracy: 0.9649 - val_loss: 5.5518 - val_accuracy: 0.5827\n",
      "Epoch 539/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0809 - accuracy: 0.9741 - val_loss: 5.3632 - val_accuracy: 0.5959\n",
      "Epoch 540/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0609 - accuracy: 0.9830 - val_loss: 5.4404 - val_accuracy: 0.5888\n",
      "Epoch 541/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0585 - accuracy: 0.9847 - val_loss: 5.4331 - val_accuracy: 0.5949\n",
      "Epoch 542/10000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0536 - accuracy: 0.9850 - val_loss: 5.3521 - val_accuracy: 0.5929\n",
      "Epoch 543/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0440 - accuracy: 0.9881 - val_loss: 5.3907 - val_accuracy: 0.5837\n",
      "Epoch 544/10000\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.0440 - accuracy: 0.9901 - val_loss: 5.3598 - val_accuracy: 0.5878\n",
      "Epoch 545/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0322 - accuracy: 0.9929 - val_loss: 5.4913 - val_accuracy: 0.5918\n",
      "Epoch 546/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0296 - accuracy: 0.9949 - val_loss: 5.5015 - val_accuracy: 0.5898\n",
      "Epoch 547/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0258 - accuracy: 0.9966 - val_loss: 5.4891 - val_accuracy: 0.5929\n",
      "Epoch 548/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0248 - accuracy: 0.9956 - val_loss: 5.5464 - val_accuracy: 0.5990\n",
      "Epoch 549/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0235 - accuracy: 0.9966 - val_loss: 5.5113 - val_accuracy: 0.5939\n",
      "Epoch 550/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0221 - accuracy: 0.9959 - val_loss: 5.5534 - val_accuracy: 0.5929\n",
      "Epoch 551/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.0225 - accuracy: 0.9946 - val_loss: 5.5664 - val_accuracy: 0.5959\n",
      "Epoch 552/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0241 - accuracy: 0.9956 - val_loss: 5.5513 - val_accuracy: 0.5980\n",
      "Epoch 553/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0226 - accuracy: 0.9959 - val_loss: 5.5663 - val_accuracy: 0.5949\n",
      "Epoch 554/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0221 - accuracy: 0.9946 - val_loss: 5.5927 - val_accuracy: 0.5929\n",
      "Epoch 555/10000\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.0249 - accuracy: 0.9949 - val_loss: 5.6127 - val_accuracy: 0.5918\n",
      "Epoch 556/10000\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.0220 - accuracy: 0.9963 - val_loss: 5.5630 - val_accuracy: 0.5969\n",
      "Epoch 557/10000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0238 - accuracy: 0.9949 - val_loss: 5.6577 - val_accuracy: 0.5959\n",
      "Epoch 558/10000\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.0216 - accuracy: 0.9963 - val_loss: 5.5791 - val_accuracy: 0.5939\n",
      "Epoch 559/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0209 - accuracy: 0.9966 - val_loss: 5.6015 - val_accuracy: 0.5980\n",
      "Epoch 560/10000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0226 - accuracy: 0.9942 - val_loss: 5.6274 - val_accuracy: 0.5980\n",
      "Epoch 561/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0203 - accuracy: 0.9959 - val_loss: 5.6313 - val_accuracy: 0.6000\n",
      "Epoch 562/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0196 - accuracy: 0.9963 - val_loss: 5.6261 - val_accuracy: 0.5959\n",
      "Epoch 563/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0197 - accuracy: 0.9963 - val_loss: 5.6738 - val_accuracy: 0.5929\n",
      "Epoch 564/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0198 - accuracy: 0.9963 - val_loss: 5.6606 - val_accuracy: 0.5980\n",
      "Epoch 565/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0197 - accuracy: 0.9969 - val_loss: 5.6922 - val_accuracy: 0.5959\n",
      "Epoch 566/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0231 - accuracy: 0.9956 - val_loss: 5.6285 - val_accuracy: 0.5929\n",
      "Epoch 567/10000\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0221 - accuracy: 0.9963 - val_loss: 5.7111 - val_accuracy: 0.5929\n",
      "Epoch 568/10000\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.0202 - accuracy: 0.9956 - val_loss: 5.6935 - val_accuracy: 0.5939\n",
      "Epoch 569/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0202 - accuracy: 0.9959 - val_loss: 5.6871 - val_accuracy: 0.5980\n",
      "Epoch 570/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0205 - accuracy: 0.9952 - val_loss: 5.7153 - val_accuracy: 0.5959\n",
      "Epoch 571/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0197 - accuracy: 0.9956 - val_loss: 5.7020 - val_accuracy: 0.5929\n",
      "Epoch 572/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0210 - accuracy: 0.9959 - val_loss: 5.7194 - val_accuracy: 0.5959\n",
      "Epoch 573/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0215 - accuracy: 0.9963 - val_loss: 5.7395 - val_accuracy: 0.5929\n",
      "Epoch 574/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0228 - accuracy: 0.9949 - val_loss: 5.7470 - val_accuracy: 0.5929\n",
      "Epoch 575/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0197 - accuracy: 0.9966 - val_loss: 5.7246 - val_accuracy: 0.5969\n",
      "Epoch 576/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0195 - accuracy: 0.9959 - val_loss: 5.7421 - val_accuracy: 0.5959\n",
      "Epoch 577/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0211 - accuracy: 0.9956 - val_loss: 5.7712 - val_accuracy: 0.5918\n",
      "Epoch 578/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0188 - accuracy: 0.9966 - val_loss: 5.7849 - val_accuracy: 0.5959\n",
      "Epoch 579/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0190 - accuracy: 0.9952 - val_loss: 5.7517 - val_accuracy: 0.5939\n",
      "Epoch 580/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0197 - accuracy: 0.9956 - val_loss: 5.8198 - val_accuracy: 0.5939\n",
      "Epoch 581/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0216 - accuracy: 0.9963 - val_loss: 5.7636 - val_accuracy: 0.5918\n",
      "Epoch 582/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0222 - accuracy: 0.9963 - val_loss: 5.7377 - val_accuracy: 0.5918\n",
      "Epoch 583/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0240 - accuracy: 0.9946 - val_loss: 5.7797 - val_accuracy: 0.5898\n",
      "Epoch 584/10000\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.0187 - accuracy: 0.9966 - val_loss: 5.7506 - val_accuracy: 0.5918\n",
      "Epoch 585/10000\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.0191 - accuracy: 0.9959 - val_loss: 5.7972 - val_accuracy: 0.5918\n",
      "Epoch 586/10000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0178 - accuracy: 0.9963 - val_loss: 5.8246 - val_accuracy: 0.5929\n",
      "Epoch 587/10000\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.0167 - accuracy: 0.9969 - val_loss: 5.8074 - val_accuracy: 0.5949\n",
      "Epoch 588/10000\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.0182 - accuracy: 0.9959 - val_loss: 5.8648 - val_accuracy: 0.5949\n",
      "Epoch 589/10000\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.0184 - accuracy: 0.9969 - val_loss: 5.8010 - val_accuracy: 0.5888\n",
      "Epoch 590/10000\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.0190 - accuracy: 0.9956 - val_loss: 5.8160 - val_accuracy: 0.5929\n",
      "Epoch 591/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0213 - accuracy: 0.9956 - val_loss: 5.8873 - val_accuracy: 0.5918\n",
      "Epoch 592/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0214 - accuracy: 0.9959 - val_loss: 5.7829 - val_accuracy: 0.5969\n",
      "Epoch 593/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0210 - accuracy: 0.9956 - val_loss: 5.8580 - val_accuracy: 0.5939\n",
      "Epoch 594/10000\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.0181 - accuracy: 0.9966 - val_loss: 5.8881 - val_accuracy: 0.5969\n",
      "Epoch 595/10000\n",
      "6/6 [==============================] - 0s 89ms/step - loss: 0.0193 - accuracy: 0.9969 - val_loss: 5.7959 - val_accuracy: 0.5980\n",
      "Epoch 596/10000\n",
      "6/6 [==============================] - 0s 89ms/step - loss: 0.0220 - accuracy: 0.9952 - val_loss: 5.8189 - val_accuracy: 0.5969\n",
      "Epoch 597/10000\n",
      "6/6 [==============================] - 0s 91ms/step - loss: 0.0195 - accuracy: 0.9963 - val_loss: 5.9251 - val_accuracy: 0.5918\n",
      "Epoch 598/10000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0187 - accuracy: 0.9963 - val_loss: 5.8737 - val_accuracy: 0.5980\n",
      "Epoch 599/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0179 - accuracy: 0.9963 - val_loss: 5.8925 - val_accuracy: 0.5959\n",
      "Epoch 600/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0205 - accuracy: 0.9963 - val_loss: 5.9422 - val_accuracy: 0.5929\n",
      "Epoch 601/10000\n",
      "6/6 [==============================] - 0s 94ms/step - loss: 0.0200 - accuracy: 0.9973 - val_loss: 5.8759 - val_accuracy: 0.5929\n",
      "Epoch 602/10000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 0.0206 - accuracy: 0.9963 - val_loss: 5.9038 - val_accuracy: 0.5929\n",
      "Epoch 603/10000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 0.0208 - accuracy: 0.9952 - val_loss: 5.8848 - val_accuracy: 0.5959\n",
      "Epoch 604/10000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0172 - accuracy: 0.9976 - val_loss: 5.9472 - val_accuracy: 0.5959\n",
      "Epoch 605/10000\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.0231 - accuracy: 0.9946 - val_loss: 5.9378 - val_accuracy: 0.5969\n",
      "Epoch 606/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0223 - accuracy: 0.9952 - val_loss: 5.9304 - val_accuracy: 0.5898\n",
      "Epoch 607/10000\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.0260 - accuracy: 0.9959 - val_loss: 5.9289 - val_accuracy: 0.5908\n",
      "Epoch 608/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0215 - accuracy: 0.9963 - val_loss: 5.8715 - val_accuracy: 0.5959\n",
      "Epoch 609/10000\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.0225 - accuracy: 0.9959 - val_loss: 5.9557 - val_accuracy: 0.5918\n",
      "Epoch 610/10000\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.0182 - accuracy: 0.9966 - val_loss: 5.9551 - val_accuracy: 0.5959\n",
      "Epoch 611/10000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0178 - accuracy: 0.9966 - val_loss: 5.9204 - val_accuracy: 0.5980\n",
      "Epoch 612/10000\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.0188 - accuracy: 0.9966 - val_loss: 5.9700 - val_accuracy: 0.5959\n",
      "Epoch 613/10000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0172 - accuracy: 0.9963 - val_loss: 5.9481 - val_accuracy: 0.5929\n",
      "Epoch 614/10000\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.0199 - accuracy: 0.9959 - val_loss: 5.9600 - val_accuracy: 0.5878\n",
      "Epoch 615/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0185 - accuracy: 0.9963 - val_loss: 6.0520 - val_accuracy: 0.5949\n",
      "Epoch 616/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0179 - accuracy: 0.9969 - val_loss: 5.9892 - val_accuracy: 0.5949\n",
      "Epoch 617/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0159 - accuracy: 0.9969 - val_loss: 5.9977 - val_accuracy: 0.5918\n",
      "Epoch 618/10000\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.0171 - accuracy: 0.9973 - val_loss: 6.0429 - val_accuracy: 0.5898\n",
      "Epoch 619/10000\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.0168 - accuracy: 0.9966 - val_loss: 6.0585 - val_accuracy: 0.5969\n",
      "Epoch 620/10000\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.0167 - accuracy: 0.9973 - val_loss: 5.9890 - val_accuracy: 0.5949\n",
      "Epoch 621/10000\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.0234 - accuracy: 0.9956 - val_loss: 6.0685 - val_accuracy: 0.5918\n",
      "Epoch 622/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0189 - accuracy: 0.9966 - val_loss: 6.0283 - val_accuracy: 0.5929\n",
      "Epoch 623/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0190 - accuracy: 0.9963 - val_loss: 5.9697 - val_accuracy: 0.5980\n",
      "Epoch 624/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0197 - accuracy: 0.9980 - val_loss: 6.0831 - val_accuracy: 0.5949\n",
      "Epoch 625/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0179 - accuracy: 0.9963 - val_loss: 6.1129 - val_accuracy: 0.5980\n",
      "Epoch 626/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0191 - accuracy: 0.9963 - val_loss: 6.0113 - val_accuracy: 0.5949\n",
      "Epoch 627/10000\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.0172 - accuracy: 0.9963 - val_loss: 6.0674 - val_accuracy: 0.5929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 628/10000\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0185 - accuracy: 0.9966 - val_loss: 6.0786 - val_accuracy: 0.5929\n",
      "Epoch 629/10000\n",
      "6/6 [==============================] - 0s 84ms/step - loss: 0.0192 - accuracy: 0.9959 - val_loss: 6.1194 - val_accuracy: 0.5898\n",
      "Epoch 630/10000\n",
      "6/6 [==============================] - 0s 89ms/step - loss: 0.0173 - accuracy: 0.9969 - val_loss: 6.0696 - val_accuracy: 0.5898\n",
      "Epoch 631/10000\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.0207 - accuracy: 0.9956 - val_loss: 6.0152 - val_accuracy: 0.5969\n",
      "Epoch 632/10000\n",
      "6/6 [==============================] - 0s 84ms/step - loss: 0.0209 - accuracy: 0.9959 - val_loss: 6.0785 - val_accuracy: 0.5969\n",
      "Epoch 633/10000\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.0191 - accuracy: 0.9963 - val_loss: 6.1293 - val_accuracy: 0.5929\n",
      "Epoch 634/10000\n",
      "6/6 [==============================] - 0s 94ms/step - loss: 0.0171 - accuracy: 0.9963 - val_loss: 6.0777 - val_accuracy: 0.5929\n",
      "Epoch 635/10000\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.0191 - accuracy: 0.9969 - val_loss: 6.1064 - val_accuracy: 0.5929\n",
      "Epoch 636/10000\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 0.0186 - accuracy: 0.9963 - val_loss: 6.1408 - val_accuracy: 0.5990\n",
      "Epoch 637/10000\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0194 - accuracy: 0.9959 - val_loss: 6.1525 - val_accuracy: 0.5939\n",
      "Epoch 638/10000\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.0179 - accuracy: 0.9976 - val_loss: 6.1757 - val_accuracy: 0.5939\n",
      "Epoch 639/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0193 - accuracy: 0.9973 - val_loss: 6.1068 - val_accuracy: 0.5939\n",
      "Epoch 640/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0209 - accuracy: 0.9956 - val_loss: 6.1408 - val_accuracy: 0.5949\n",
      "Epoch 641/10000\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.0203 - accuracy: 0.9952 - val_loss: 6.0559 - val_accuracy: 0.5929\n",
      "Epoch 642/10000\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.0325 - accuracy: 0.9942 - val_loss: 6.1397 - val_accuracy: 0.5949\n",
      "Epoch 643/10000\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0367 - accuracy: 0.9905 - val_loss: 6.1452 - val_accuracy: 0.6031\n",
      "Epoch 644/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0491 - accuracy: 0.9847 - val_loss: 6.1611 - val_accuracy: 0.5806\n",
      "Epoch 645/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.0450 - accuracy: 0.9898 - val_loss: 6.1178 - val_accuracy: 0.5949\n",
      "Epoch 646/10000\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0489 - accuracy: 0.9854 - val_loss: 6.1446 - val_accuracy: 0.5827\n",
      "Epoch 647/10000\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0667 - accuracy: 0.9813 - val_loss: 6.0026 - val_accuracy: 0.5929\n",
      "Epoch 648/10000\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.1183 - accuracy: 0.9609 - val_loss: 6.2842 - val_accuracy: 0.5867\n",
      "Epoch 649/10000\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.1165 - accuracy: 0.9636 - val_loss: 6.2312 - val_accuracy: 0.5653\n",
      "Epoch 650/10000\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0885 - accuracy: 0.9707 - val_loss: 6.2810 - val_accuracy: 0.5929\n",
      "Epoch 651/10000\n",
      "6/6 [==============================] - 0s 91ms/step - loss: 0.0767 - accuracy: 0.9758 - val_loss: 6.0639 - val_accuracy: 0.5888\n",
      "Epoch 652/10000\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 0.0810 - accuracy: 0.9735 - val_loss: 6.1465 - val_accuracy: 0.5929\n",
      "Epoch 653/10000\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.0929 - accuracy: 0.9639 - val_loss: 5.9334 - val_accuracy: 0.5745\n",
      "Epoch 654/10000\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0774 - accuracy: 0.9704 - val_loss: 5.9732 - val_accuracy: 0.5878\n",
      "Epoch 655/10000\n",
      "6/6 [==============================] - 1s 98ms/step - loss: 0.0648 - accuracy: 0.9809 - val_loss: 5.9395 - val_accuracy: 0.5908\n",
      "Epoch 656/10000\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 0.0416 - accuracy: 0.9884 - val_loss: 6.0177 - val_accuracy: 0.5816\n",
      "Epoch 657/10000\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.0400 - accuracy: 0.9867 - val_loss: 6.1407 - val_accuracy: 0.5959\n",
      "Epoch 658/10000\n",
      "6/6 [==============================] - 0s 87ms/step - loss: 0.0306 - accuracy: 0.9949 - val_loss: 5.9930 - val_accuracy: 0.5918\n",
      "Epoch 659/10000\n",
      "6/6 [==============================] - 0s 85ms/step - loss: 0.0312 - accuracy: 0.9925 - val_loss: 6.0210 - val_accuracy: 0.5929\n",
      "Epoch 660/10000\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0286 - accuracy: 0.9935 - val_loss: 6.0726 - val_accuracy: 0.6010\n",
      "Epoch 661/10000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0278 - accuracy: 0.9949 - val_loss: 6.1440 - val_accuracy: 0.5888\n",
      "Epoch 662/10000\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.0240 - accuracy: 0.9956 - val_loss: 6.0968 - val_accuracy: 0.5888\n",
      "Epoch 663/10000\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.0222 - accuracy: 0.9959 - val_loss: 6.1178 - val_accuracy: 0.5908\n",
      "Epoch 664/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0209 - accuracy: 0.9963 - val_loss: 6.1636 - val_accuracy: 0.5898\n",
      "Epoch 665/10000\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.0193 - accuracy: 0.9963 - val_loss: 6.1225 - val_accuracy: 0.5980\n",
      "Epoch 666/10000\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0182 - accuracy: 0.9976 - val_loss: 6.1271 - val_accuracy: 0.5949\n",
      "Epoch 667/10000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0183 - accuracy: 0.9973 - val_loss: 6.1392 - val_accuracy: 0.5949\n",
      "Epoch 668/10000\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0173 - accuracy: 0.9966 - val_loss: 6.1772 - val_accuracy: 0.5969\n",
      "Epoch 669/10000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0159 - accuracy: 0.9973 - val_loss: 6.1914 - val_accuracy: 0.5949\n",
      "Epoch 670/10000\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.0172 - accuracy: 0.9963 - val_loss: 6.1401 - val_accuracy: 0.5949\n",
      "Epoch 671/10000\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0152 - accuracy: 0.9973 - val_loss: 6.2369 - val_accuracy: 0.5918\n",
      "Epoch 672/10000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0154 - accuracy: 0.9976 - val_loss: 6.1597 - val_accuracy: 0.5939\n",
      "Epoch 673/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0167 - accuracy: 0.9966 - val_loss: 6.1845 - val_accuracy: 0.5990\n",
      "Epoch 674/10000\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.0158 - accuracy: 0.9969 - val_loss: 6.2219 - val_accuracy: 0.5949\n",
      "Epoch 675/10000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0179 - accuracy: 0.9969 - val_loss: 6.1733 - val_accuracy: 0.5929\n",
      "Epoch 676/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0177 - accuracy: 0.9966 - val_loss: 6.2222 - val_accuracy: 0.5929\n",
      "Epoch 677/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0146 - accuracy: 0.9973 - val_loss: 6.1750 - val_accuracy: 0.5929\n",
      "Epoch 678/10000\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.0153 - accuracy: 0.9973 - val_loss: 6.2296 - val_accuracy: 0.5959\n",
      "Epoch 679/10000\n",
      "6/6 [==============================] - 1s 99ms/step - loss: 0.0143 - accuracy: 0.9973 - val_loss: 6.2283 - val_accuracy: 0.5939\n",
      "Epoch 680/10000\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.0123 - accuracy: 0.9986 - val_loss: 6.2438 - val_accuracy: 0.5929\n",
      "Epoch 681/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 6.2646 - val_accuracy: 0.5949\n",
      "Epoch 682/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.0148 - accuracy: 0.9966 - val_loss: 6.2672 - val_accuracy: 0.5929\n",
      "Epoch 683/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 6.3132 - val_accuracy: 0.5929\n",
      "Epoch 684/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0149 - accuracy: 0.9969 - val_loss: 6.2214 - val_accuracy: 0.5949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 685/10000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0182 - accuracy: 0.9959 - val_loss: 6.3296 - val_accuracy: 0.5949\n",
      "Epoch 686/10000\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0144 - accuracy: 0.9976 - val_loss: 6.2318 - val_accuracy: 0.5969\n",
      "Epoch 687/10000\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.0141 - accuracy: 0.9976 - val_loss: 6.2687 - val_accuracy: 0.5959\n",
      "Epoch 688/10000\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.0138 - accuracy: 0.9983 - val_loss: 6.3293 - val_accuracy: 0.5939\n",
      "Epoch 689/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0142 - accuracy: 0.9976 - val_loss: 6.2018 - val_accuracy: 0.5878\n",
      "Epoch 690/10000\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 0.0186 - accuracy: 0.9969 - val_loss: 6.3459 - val_accuracy: 0.5949\n",
      "Epoch 691/10000\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 0.0162 - accuracy: 0.9976 - val_loss: 6.2805 - val_accuracy: 0.5969\n",
      "Epoch 692/10000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0146 - accuracy: 0.9973 - val_loss: 6.2704 - val_accuracy: 0.5949\n",
      "Epoch 693/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0140 - accuracy: 0.9976 - val_loss: 6.4091 - val_accuracy: 0.5898\n",
      "Epoch 694/10000\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.0161 - accuracy: 0.9963 - val_loss: 6.2582 - val_accuracy: 0.5949\n",
      "Epoch 695/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0153 - accuracy: 0.9973 - val_loss: 6.3465 - val_accuracy: 0.5969\n",
      "Epoch 696/10000\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.0130 - accuracy: 0.9969 - val_loss: 6.3241 - val_accuracy: 0.5918\n",
      "Epoch 697/10000\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.0134 - accuracy: 0.9973 - val_loss: 6.3278 - val_accuracy: 0.5959\n",
      "Epoch 698/10000\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.0129 - accuracy: 0.9969 - val_loss: 6.3317 - val_accuracy: 0.5969\n",
      "Epoch 699/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0131 - accuracy: 0.9980 - val_loss: 6.3041 - val_accuracy: 0.5949\n",
      "Epoch 700/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0149 - accuracy: 0.9976 - val_loss: 6.4130 - val_accuracy: 0.5929\n",
      "Epoch 701/10000\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0150 - accuracy: 0.9973 - val_loss: 6.2856 - val_accuracy: 0.5929\n",
      "Epoch 702/10000\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.0157 - accuracy: 0.9973 - val_loss: 6.3441 - val_accuracy: 0.5929\n",
      "Epoch 703/10000\n",
      "6/6 [==============================] - 0s 87ms/step - loss: 0.0176 - accuracy: 0.9973 - val_loss: 6.4176 - val_accuracy: 0.5949\n",
      "Epoch 704/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0180 - accuracy: 0.9966 - val_loss: 6.2718 - val_accuracy: 0.5908\n",
      "Epoch 705/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0155 - accuracy: 0.9973 - val_loss: 6.4048 - val_accuracy: 0.5888\n",
      "Epoch 706/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0128 - accuracy: 0.9966 - val_loss: 6.3843 - val_accuracy: 0.5959\n",
      "Epoch 707/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0122 - accuracy: 0.9980 - val_loss: 6.4006 - val_accuracy: 0.5898\n",
      "Epoch 708/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0121 - accuracy: 0.9980 - val_loss: 6.3592 - val_accuracy: 0.5990\n",
      "Epoch 709/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0136 - accuracy: 0.9973 - val_loss: 6.4155 - val_accuracy: 0.5939\n",
      "Epoch 710/10000\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 0.0118 - accuracy: 0.9973 - val_loss: 6.4017 - val_accuracy: 0.5959\n",
      "Epoch 711/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0136 - accuracy: 0.9966 - val_loss: 6.4411 - val_accuracy: 0.5949\n",
      "Epoch 712/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0148 - accuracy: 0.9963 - val_loss: 6.3601 - val_accuracy: 0.5959\n",
      "Epoch 713/10000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0129 - accuracy: 0.9973 - val_loss: 6.4619 - val_accuracy: 0.5908\n",
      "Epoch 714/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0124 - accuracy: 0.9976 - val_loss: 6.3643 - val_accuracy: 0.5939\n",
      "Epoch 715/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0150 - accuracy: 0.9973 - val_loss: 6.4943 - val_accuracy: 0.5939\n",
      "Epoch 716/10000\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.0152 - accuracy: 0.9973 - val_loss: 6.4122 - val_accuracy: 0.5990\n",
      "Epoch 717/10000\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.0135 - accuracy: 0.9973 - val_loss: 6.4687 - val_accuracy: 0.5959\n",
      "Epoch 718/10000\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.0142 - accuracy: 0.9976 - val_loss: 6.4621 - val_accuracy: 0.5929\n",
      "Epoch 719/10000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0147 - accuracy: 0.9969 - val_loss: 6.3717 - val_accuracy: 0.5949\n",
      "Epoch 720/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0143 - accuracy: 0.9976 - val_loss: 6.4523 - val_accuracy: 0.5959\n",
      "Epoch 721/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0126 - accuracy: 0.9980 - val_loss: 6.4598 - val_accuracy: 0.5949\n",
      "Epoch 722/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0139 - accuracy: 0.9976 - val_loss: 6.4452 - val_accuracy: 0.5949\n",
      "Epoch 723/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0134 - accuracy: 0.9973 - val_loss: 6.5246 - val_accuracy: 0.5980\n",
      "Epoch 724/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0151 - accuracy: 0.9969 - val_loss: 6.4472 - val_accuracy: 0.6000\n",
      "Epoch 725/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0151 - accuracy: 0.9963 - val_loss: 6.4915 - val_accuracy: 0.5939\n",
      "Epoch 726/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0162 - accuracy: 0.9959 - val_loss: 6.4600 - val_accuracy: 0.5949\n",
      "Epoch 727/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0130 - accuracy: 0.9980 - val_loss: 6.4580 - val_accuracy: 0.5939\n",
      "Epoch 728/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0171 - accuracy: 0.9973 - val_loss: 6.5311 - val_accuracy: 0.5888\n",
      "Epoch 729/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0182 - accuracy: 0.9959 - val_loss: 6.5078 - val_accuracy: 0.5918\n",
      "Epoch 730/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0170 - accuracy: 0.9966 - val_loss: 6.5196 - val_accuracy: 0.5949\n",
      "Epoch 731/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0183 - accuracy: 0.9963 - val_loss: 6.4563 - val_accuracy: 0.5949\n",
      "Epoch 732/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0189 - accuracy: 0.9956 - val_loss: 6.5374 - val_accuracy: 0.5918\n",
      "Epoch 733/10000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0197 - accuracy: 0.9973 - val_loss: 6.5185 - val_accuracy: 0.5878\n",
      "Epoch 734/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0199 - accuracy: 0.9963 - val_loss: 6.4893 - val_accuracy: 0.5898\n",
      "Epoch 735/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0171 - accuracy: 0.9963 - val_loss: 6.4516 - val_accuracy: 0.5888\n",
      "Epoch 736/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0159 - accuracy: 0.9969 - val_loss: 6.6244 - val_accuracy: 0.5918\n",
      "Epoch 737/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0164 - accuracy: 0.9973 - val_loss: 6.4983 - val_accuracy: 0.5929\n",
      "Epoch 738/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0161 - accuracy: 0.9973 - val_loss: 6.4883 - val_accuracy: 0.5929\n",
      "Epoch 739/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0244 - accuracy: 0.9956 - val_loss: 6.6781 - val_accuracy: 0.5878\n",
      "Epoch 740/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0221 - accuracy: 0.9952 - val_loss: 6.6730 - val_accuracy: 0.5878\n",
      "Epoch 741/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0235 - accuracy: 0.9956 - val_loss: 6.5487 - val_accuracy: 0.5918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 742/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0154 - accuracy: 0.9976 - val_loss: 6.4385 - val_accuracy: 0.5867\n",
      "Epoch 743/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0153 - accuracy: 0.9973 - val_loss: 6.5367 - val_accuracy: 0.5949\n",
      "Epoch 744/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0149 - accuracy: 0.9973 - val_loss: 6.4887 - val_accuracy: 0.5939\n",
      "Epoch 745/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0137 - accuracy: 0.9966 - val_loss: 6.5214 - val_accuracy: 0.5888\n",
      "Epoch 746/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0156 - accuracy: 0.9963 - val_loss: 6.6450 - val_accuracy: 0.5908\n",
      "Epoch 747/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0145 - accuracy: 0.9980 - val_loss: 6.5177 - val_accuracy: 0.5929\n",
      "Epoch 748/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0177 - accuracy: 0.9952 - val_loss: 6.5726 - val_accuracy: 0.5898\n",
      "Epoch 749/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0156 - accuracy: 0.9983 - val_loss: 6.6062 - val_accuracy: 0.5969\n",
      "Epoch 750/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0149 - accuracy: 0.9966 - val_loss: 6.6092 - val_accuracy: 0.5857\n",
      "Epoch 751/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0168 - accuracy: 0.9969 - val_loss: 6.5358 - val_accuracy: 0.5969\n",
      "Epoch 752/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0126 - accuracy: 0.9976 - val_loss: 6.4819 - val_accuracy: 0.5980\n",
      "Epoch 753/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0130 - accuracy: 0.9976 - val_loss: 6.6025 - val_accuracy: 0.5888\n",
      "Epoch 754/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0138 - accuracy: 0.9973 - val_loss: 6.5471 - val_accuracy: 0.5908\n",
      "Epoch 755/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0151 - accuracy: 0.9969 - val_loss: 6.6123 - val_accuracy: 0.5959\n",
      "Epoch 756/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0164 - accuracy: 0.9966 - val_loss: 6.7230 - val_accuracy: 0.5898\n",
      "Epoch 757/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0186 - accuracy: 0.9976 - val_loss: 6.5461 - val_accuracy: 0.5939\n",
      "Epoch 758/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0136 - accuracy: 0.9980 - val_loss: 6.6269 - val_accuracy: 0.5867\n",
      "Epoch 759/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0129 - accuracy: 0.9973 - val_loss: 6.6381 - val_accuracy: 0.6010\n",
      "Epoch 760/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0133 - accuracy: 0.9976 - val_loss: 6.6872 - val_accuracy: 0.5969\n",
      "Epoch 761/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0121 - accuracy: 0.9980 - val_loss: 6.5924 - val_accuracy: 0.5888\n",
      "Epoch 762/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0114 - accuracy: 0.9983 - val_loss: 6.6203 - val_accuracy: 0.5980\n",
      "Epoch 763/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0111 - accuracy: 0.9980 - val_loss: 6.6691 - val_accuracy: 0.5990\n",
      "Epoch 764/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0106 - accuracy: 0.9969 - val_loss: 6.6915 - val_accuracy: 0.5888\n",
      "Epoch 765/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0105 - accuracy: 0.9973 - val_loss: 6.6600 - val_accuracy: 0.5990\n",
      "Epoch 766/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0106 - accuracy: 0.9973 - val_loss: 6.6807 - val_accuracy: 0.5959\n",
      "Epoch 767/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0103 - accuracy: 0.9973 - val_loss: 6.6951 - val_accuracy: 0.5898\n",
      "Epoch 768/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0111 - accuracy: 0.9976 - val_loss: 6.7153 - val_accuracy: 0.5959\n",
      "Epoch 769/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0100 - accuracy: 0.9973 - val_loss: 6.7027 - val_accuracy: 0.5929\n",
      "Epoch 770/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0127 - accuracy: 0.9980 - val_loss: 6.7491 - val_accuracy: 0.5939\n",
      "Epoch 771/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0106 - accuracy: 0.9976 - val_loss: 6.7147 - val_accuracy: 0.5969\n",
      "Epoch 772/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0104 - accuracy: 0.9980 - val_loss: 6.6973 - val_accuracy: 0.5939\n",
      "Epoch 773/10000\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.0114 - accuracy: 0.9976 - val_loss: 6.7162 - val_accuracy: 0.5918\n",
      "Epoch 774/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0104 - accuracy: 0.9980 - val_loss: 6.7387 - val_accuracy: 0.5949\n",
      "Epoch 775/10000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0124 - accuracy: 0.9980 - val_loss: 6.6995 - val_accuracy: 0.5918\n",
      "Epoch 776/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0111 - accuracy: 0.9980 - val_loss: 6.7865 - val_accuracy: 0.5990\n",
      "Epoch 777/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0118 - accuracy: 0.9973 - val_loss: 6.7389 - val_accuracy: 0.6010\n",
      "Epoch 778/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0094 - accuracy: 0.9986 - val_loss: 6.7608 - val_accuracy: 0.5929\n",
      "Epoch 779/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0106 - accuracy: 0.9980 - val_loss: 6.7626 - val_accuracy: 0.5990\n",
      "Epoch 780/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0101 - accuracy: 0.9986 - val_loss: 6.7538 - val_accuracy: 0.5908\n",
      "Epoch 781/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0117 - accuracy: 0.9976 - val_loss: 6.7977 - val_accuracy: 0.5908\n",
      "Epoch 782/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0132 - accuracy: 0.9973 - val_loss: 6.8542 - val_accuracy: 0.5918\n",
      "Epoch 783/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0119 - accuracy: 0.9976 - val_loss: 6.7249 - val_accuracy: 0.5939\n",
      "Epoch 784/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0129 - accuracy: 0.9969 - val_loss: 6.7770 - val_accuracy: 0.5878\n",
      "Epoch 785/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0159 - accuracy: 0.9963 - val_loss: 6.8749 - val_accuracy: 0.5959\n",
      "Epoch 786/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0136 - accuracy: 0.9969 - val_loss: 6.7056 - val_accuracy: 0.5959\n",
      "Epoch 787/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0118 - accuracy: 0.9976 - val_loss: 6.7647 - val_accuracy: 0.5929\n",
      "Epoch 788/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0085 - accuracy: 0.9986 - val_loss: 6.7655 - val_accuracy: 0.5949\n",
      "Epoch 789/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0111 - accuracy: 0.9973 - val_loss: 6.7888 - val_accuracy: 0.5918\n",
      "Epoch 790/10000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0102 - accuracy: 0.9980 - val_loss: 6.7942 - val_accuracy: 0.5949\n",
      "Epoch 791/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0129 - accuracy: 0.9980 - val_loss: 6.8311 - val_accuracy: 0.5888\n",
      "Epoch 792/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0110 - accuracy: 0.9983 - val_loss: 6.8560 - val_accuracy: 0.5898\n",
      "Epoch 793/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0131 - accuracy: 0.9980 - val_loss: 6.7765 - val_accuracy: 0.5888\n",
      "Epoch 794/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0149 - accuracy: 0.9973 - val_loss: 6.7696 - val_accuracy: 0.5898\n",
      "Epoch 795/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0119 - accuracy: 0.9983 - val_loss: 6.8965 - val_accuracy: 0.5898\n",
      "Epoch 796/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0119 - accuracy: 0.9983 - val_loss: 6.7931 - val_accuracy: 0.5949\n",
      "Epoch 797/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0143 - accuracy: 0.9973 - val_loss: 6.8564 - val_accuracy: 0.5959\n",
      "Epoch 798/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0155 - accuracy: 0.9966 - val_loss: 6.8579 - val_accuracy: 0.5898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 799/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0153 - accuracy: 0.9966 - val_loss: 6.7942 - val_accuracy: 0.5867\n",
      "Epoch 800/10000\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.0177 - accuracy: 0.9969 - val_loss: 6.8127 - val_accuracy: 0.5908\n",
      "Epoch 801/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0128 - accuracy: 0.9966 - val_loss: 6.9291 - val_accuracy: 0.5908\n",
      "Epoch 802/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.0122 - accuracy: 0.9983 - val_loss: 6.7815 - val_accuracy: 0.5959\n",
      "Epoch 803/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0108 - accuracy: 0.9980 - val_loss: 6.8269 - val_accuracy: 0.5929\n",
      "Epoch 804/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0122 - accuracy: 0.9973 - val_loss: 6.8664 - val_accuracy: 0.5898\n",
      "Epoch 805/10000\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.0124 - accuracy: 0.9976 - val_loss: 6.8435 - val_accuracy: 0.5939\n",
      "Epoch 806/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0101 - accuracy: 0.9986 - val_loss: 6.8624 - val_accuracy: 0.5949\n",
      "Epoch 807/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0092 - accuracy: 0.9983 - val_loss: 6.8721 - val_accuracy: 0.5969\n",
      "Epoch 808/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0106 - accuracy: 0.9980 - val_loss: 6.8375 - val_accuracy: 0.5959\n",
      "Epoch 809/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0124 - accuracy: 0.9980 - val_loss: 6.9814 - val_accuracy: 0.5959\n",
      "Epoch 810/10000\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0091 - accuracy: 0.9980 - val_loss: 6.8851 - val_accuracy: 0.5898\n",
      "Epoch 811/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0095 - accuracy: 0.9980 - val_loss: 6.9332 - val_accuracy: 0.5939\n",
      "Epoch 812/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0101 - accuracy: 0.9983 - val_loss: 6.8675 - val_accuracy: 0.5908\n",
      "Epoch 813/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0102 - accuracy: 0.9973 - val_loss: 6.9409 - val_accuracy: 0.5939\n",
      "Epoch 814/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0077 - accuracy: 0.9993 - val_loss: 6.9314 - val_accuracy: 0.5908\n",
      "Epoch 815/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0090 - accuracy: 0.9983 - val_loss: 6.9513 - val_accuracy: 0.5949\n",
      "Epoch 816/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0075 - accuracy: 0.9983 - val_loss: 6.9402 - val_accuracy: 0.5918\n",
      "Epoch 817/10000\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.0077 - accuracy: 0.9983 - val_loss: 6.8915 - val_accuracy: 0.5908\n",
      "Epoch 818/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0103 - accuracy: 0.9980 - val_loss: 6.9739 - val_accuracy: 0.5898\n",
      "Epoch 819/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0128 - accuracy: 0.9973 - val_loss: 6.9155 - val_accuracy: 0.5867\n",
      "Epoch 820/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0146 - accuracy: 0.9966 - val_loss: 6.9799 - val_accuracy: 0.5898\n",
      "Epoch 821/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0098 - accuracy: 0.9976 - val_loss: 6.9941 - val_accuracy: 0.5908\n",
      "Epoch 822/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0093 - accuracy: 0.9976 - val_loss: 6.9557 - val_accuracy: 0.5908\n",
      "Epoch 823/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0086 - accuracy: 0.9980 - val_loss: 6.9349 - val_accuracy: 0.5918\n",
      "Epoch 824/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0082 - accuracy: 0.9976 - val_loss: 6.9519 - val_accuracy: 0.5908\n",
      "Epoch 825/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0098 - accuracy: 0.9969 - val_loss: 6.9671 - val_accuracy: 0.5918\n",
      "Epoch 826/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0087 - accuracy: 0.9973 - val_loss: 7.0017 - val_accuracy: 0.5949\n",
      "Epoch 827/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0073 - accuracy: 0.9990 - val_loss: 6.9973 - val_accuracy: 0.5888\n",
      "Epoch 828/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0082 - accuracy: 0.9980 - val_loss: 7.0113 - val_accuracy: 0.5929\n",
      "Epoch 829/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0091 - accuracy: 0.9980 - val_loss: 6.9765 - val_accuracy: 0.5949\n",
      "Epoch 830/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0078 - accuracy: 0.9980 - val_loss: 6.9581 - val_accuracy: 0.5847\n",
      "Epoch 831/10000\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.0083 - accuracy: 0.9980 - val_loss: 7.0182 - val_accuracy: 0.5949\n",
      "Epoch 832/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0083 - accuracy: 0.9976 - val_loss: 6.9581 - val_accuracy: 0.5908\n",
      "Epoch 833/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0097 - accuracy: 0.9980 - val_loss: 7.0325 - val_accuracy: 0.5918\n",
      "Epoch 834/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0079 - accuracy: 0.9983 - val_loss: 6.9912 - val_accuracy: 0.5878\n",
      "Epoch 835/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0081 - accuracy: 0.9980 - val_loss: 7.0110 - val_accuracy: 0.5908\n",
      "Epoch 836/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0081 - accuracy: 0.9980 - val_loss: 7.0653 - val_accuracy: 0.5939\n",
      "Epoch 837/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0100 - accuracy: 0.9976 - val_loss: 7.0097 - val_accuracy: 0.5867\n",
      "Epoch 838/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0109 - accuracy: 0.9969 - val_loss: 6.9849 - val_accuracy: 0.5898\n",
      "Epoch 839/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0122 - accuracy: 0.9973 - val_loss: 7.1334 - val_accuracy: 0.5939\n",
      "Epoch 840/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0133 - accuracy: 0.9963 - val_loss: 7.0219 - val_accuracy: 0.5857\n",
      "Epoch 841/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0136 - accuracy: 0.9973 - val_loss: 7.0031 - val_accuracy: 0.5898\n",
      "Epoch 842/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0261 - accuracy: 0.9925 - val_loss: 7.0740 - val_accuracy: 0.5918\n",
      "Epoch 843/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0918 - accuracy: 0.9738 - val_loss: 7.0369 - val_accuracy: 0.5867\n",
      "Epoch 844/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0879 - accuracy: 0.9714 - val_loss: 6.8768 - val_accuracy: 0.5755\n",
      "Epoch 845/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.1225 - accuracy: 0.9585 - val_loss: 7.2106 - val_accuracy: 0.5827\n",
      "Epoch 846/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.1830 - accuracy: 0.9493 - val_loss: 6.9116 - val_accuracy: 0.5673\n",
      "Epoch 847/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.3342 - accuracy: 0.9101 - val_loss: 6.7655 - val_accuracy: 0.5786\n",
      "Epoch 848/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.2603 - accuracy: 0.9224 - val_loss: 6.8695 - val_accuracy: 0.5602\n",
      "Epoch 849/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.1637 - accuracy: 0.9462 - val_loss: 6.5674 - val_accuracy: 0.5847\n",
      "Epoch 850/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1066 - accuracy: 0.9615 - val_loss: 6.3568 - val_accuracy: 0.5949\n",
      "Epoch 851/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0655 - accuracy: 0.9803 - val_loss: 6.4664 - val_accuracy: 0.5816\n",
      "Epoch 852/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0493 - accuracy: 0.9860 - val_loss: 6.6975 - val_accuracy: 0.5847\n",
      "Epoch 853/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0433 - accuracy: 0.9847 - val_loss: 6.5250 - val_accuracy: 0.5847\n",
      "Epoch 854/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0361 - accuracy: 0.9881 - val_loss: 6.6536 - val_accuracy: 0.5847\n",
      "Epoch 855/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0262 - accuracy: 0.9932 - val_loss: 6.6409 - val_accuracy: 0.5837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 856/10000\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.0196 - accuracy: 0.9966 - val_loss: 6.5252 - val_accuracy: 0.5959\n",
      "Epoch 857/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0165 - accuracy: 0.9973 - val_loss: 6.6752 - val_accuracy: 0.5898\n",
      "Epoch 858/10000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0172 - accuracy: 0.9966 - val_loss: 6.5243 - val_accuracy: 0.5939\n",
      "Epoch 859/10000\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.0138 - accuracy: 0.9980 - val_loss: 6.6866 - val_accuracy: 0.5847\n",
      "Epoch 860/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0123 - accuracy: 0.9980 - val_loss: 6.6210 - val_accuracy: 0.5939\n",
      "Epoch 861/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0125 - accuracy: 0.9969 - val_loss: 6.6663 - val_accuracy: 0.5939\n",
      "Epoch 862/10000\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.0106 - accuracy: 0.9980 - val_loss: 6.6418 - val_accuracy: 0.5898\n",
      "Epoch 863/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0100 - accuracy: 0.9976 - val_loss: 6.7140 - val_accuracy: 0.5949\n",
      "Epoch 864/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0100 - accuracy: 0.9980 - val_loss: 6.7033 - val_accuracy: 0.5939\n",
      "Epoch 865/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0103 - accuracy: 0.9973 - val_loss: 6.7479 - val_accuracy: 0.5949\n",
      "Epoch 866/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0102 - accuracy: 0.9980 - val_loss: 6.7159 - val_accuracy: 0.5918\n",
      "Epoch 867/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0094 - accuracy: 0.9983 - val_loss: 6.7553 - val_accuracy: 0.5969\n",
      "Epoch 868/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0090 - accuracy: 0.9986 - val_loss: 6.7176 - val_accuracy: 0.5929\n",
      "Epoch 869/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0092 - accuracy: 0.9983 - val_loss: 6.7782 - val_accuracy: 0.5939\n",
      "Epoch 870/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0101 - accuracy: 0.9980 - val_loss: 6.7525 - val_accuracy: 0.5959\n",
      "Epoch 871/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0094 - accuracy: 0.9983 - val_loss: 6.8005 - val_accuracy: 0.5918\n",
      "Epoch 872/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0098 - accuracy: 0.9986 - val_loss: 6.7721 - val_accuracy: 0.5949\n",
      "Epoch 873/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0086 - accuracy: 0.9983 - val_loss: 6.7679 - val_accuracy: 0.5949\n",
      "Epoch 874/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0093 - accuracy: 0.9983 - val_loss: 6.8146 - val_accuracy: 0.5918\n",
      "Epoch 875/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0079 - accuracy: 0.9986 - val_loss: 6.7828 - val_accuracy: 0.5949\n",
      "Epoch 876/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0084 - accuracy: 0.9983 - val_loss: 6.8270 - val_accuracy: 0.5929\n",
      "Epoch 877/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0082 - accuracy: 0.9983 - val_loss: 6.8104 - val_accuracy: 0.5918\n",
      "Epoch 878/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0082 - accuracy: 0.9986 - val_loss: 6.8327 - val_accuracy: 0.5918\n",
      "Epoch 879/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0080 - accuracy: 0.9980 - val_loss: 6.8419 - val_accuracy: 0.5908\n",
      "Epoch 880/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0078 - accuracy: 0.9986 - val_loss: 6.8492 - val_accuracy: 0.5908\n",
      "Epoch 881/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0073 - accuracy: 0.9990 - val_loss: 6.8584 - val_accuracy: 0.5918\n",
      "Epoch 882/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0077 - accuracy: 0.9986 - val_loss: 6.8471 - val_accuracy: 0.5918\n",
      "Epoch 883/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0089 - accuracy: 0.9969 - val_loss: 6.8964 - val_accuracy: 0.5908\n",
      "Epoch 884/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0091 - accuracy: 0.9983 - val_loss: 6.8471 - val_accuracy: 0.5918\n",
      "Epoch 885/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0102 - accuracy: 0.9980 - val_loss: 6.8967 - val_accuracy: 0.5918\n",
      "Epoch 886/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0079 - accuracy: 0.9976 - val_loss: 6.8709 - val_accuracy: 0.5878\n",
      "Epoch 887/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0081 - accuracy: 0.9980 - val_loss: 6.8828 - val_accuracy: 0.5898\n",
      "Epoch 888/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0076 - accuracy: 0.9980 - val_loss: 6.9002 - val_accuracy: 0.5929\n",
      "Epoch 889/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0078 - accuracy: 0.9980 - val_loss: 6.9089 - val_accuracy: 0.5908\n",
      "Epoch 890/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0072 - accuracy: 0.9983 - val_loss: 6.9356 - val_accuracy: 0.5908\n",
      "Epoch 891/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0083 - accuracy: 0.9973 - val_loss: 6.8992 - val_accuracy: 0.5898\n",
      "Epoch 892/10000\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.0084 - accuracy: 0.9980 - val_loss: 6.9361 - val_accuracy: 0.5908\n",
      "Epoch 893/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0087 - accuracy: 0.9980 - val_loss: 6.8910 - val_accuracy: 0.5908\n",
      "Epoch 894/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0101 - accuracy: 0.9980 - val_loss: 6.9669 - val_accuracy: 0.5918\n",
      "Epoch 895/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0118 - accuracy: 0.9963 - val_loss: 6.8756 - val_accuracy: 0.5929\n",
      "Epoch 896/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0092 - accuracy: 0.9973 - val_loss: 6.9536 - val_accuracy: 0.5888\n",
      "Epoch 897/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0092 - accuracy: 0.9976 - val_loss: 6.8986 - val_accuracy: 0.5878\n",
      "Epoch 898/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0084 - accuracy: 0.9983 - val_loss: 6.9698 - val_accuracy: 0.5878\n",
      "Epoch 899/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0077 - accuracy: 0.9990 - val_loss: 6.9071 - val_accuracy: 0.5908\n",
      "Epoch 900/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0124 - accuracy: 0.9969 - val_loss: 6.9426 - val_accuracy: 0.5939\n",
      "Epoch 901/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0156 - accuracy: 0.9963 - val_loss: 6.8546 - val_accuracy: 0.5898\n",
      "Epoch 902/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0386 - accuracy: 0.9912 - val_loss: 6.9184 - val_accuracy: 0.5929\n",
      "Epoch 903/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0264 - accuracy: 0.9918 - val_loss: 7.0922 - val_accuracy: 0.5867\n",
      "Epoch 904/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0218 - accuracy: 0.9956 - val_loss: 6.8238 - val_accuracy: 0.5959\n",
      "Epoch 905/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0125 - accuracy: 0.9976 - val_loss: 6.8707 - val_accuracy: 0.5898\n",
      "Epoch 906/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0109 - accuracy: 0.9983 - val_loss: 6.8159 - val_accuracy: 0.5867\n",
      "Epoch 907/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0094 - accuracy: 0.9983 - val_loss: 6.8636 - val_accuracy: 0.5878\n",
      "Epoch 908/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0092 - accuracy: 0.9980 - val_loss: 6.9022 - val_accuracy: 0.5878\n",
      "Epoch 909/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0111 - accuracy: 0.9980 - val_loss: 6.8958 - val_accuracy: 0.5959\n",
      "Epoch 910/10000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0096 - accuracy: 0.9980 - val_loss: 6.9612 - val_accuracy: 0.5867\n",
      "Epoch 911/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0096 - accuracy: 0.9980 - val_loss: 6.8872 - val_accuracy: 0.5898\n",
      "Epoch 912/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0079 - accuracy: 0.9986 - val_loss: 6.9394 - val_accuracy: 0.5888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 913/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0080 - accuracy: 0.9980 - val_loss: 6.9283 - val_accuracy: 0.5918\n",
      "Epoch 914/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0082 - accuracy: 0.9983 - val_loss: 6.9797 - val_accuracy: 0.5918\n",
      "Epoch 915/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0080 - accuracy: 0.9983 - val_loss: 6.9663 - val_accuracy: 0.5908\n",
      "Epoch 916/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0068 - accuracy: 0.9990 - val_loss: 7.0047 - val_accuracy: 0.5918\n",
      "Epoch 917/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0075 - accuracy: 0.9973 - val_loss: 6.9707 - val_accuracy: 0.5888\n",
      "Epoch 918/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0066 - accuracy: 0.9986 - val_loss: 7.0108 - val_accuracy: 0.5908\n",
      "Epoch 919/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0088 - accuracy: 0.9976 - val_loss: 6.9969 - val_accuracy: 0.5929\n",
      "Epoch 920/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0103 - accuracy: 0.9973 - val_loss: 6.9935 - val_accuracy: 0.5898\n",
      "Epoch 921/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0085 - accuracy: 0.9973 - val_loss: 7.0578 - val_accuracy: 0.5888\n",
      "Epoch 922/10000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0113 - accuracy: 0.9973 - val_loss: 6.9868 - val_accuracy: 0.5908\n",
      "Epoch 923/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0083 - accuracy: 0.9976 - val_loss: 6.9732 - val_accuracy: 0.5918\n",
      "Epoch 924/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0081 - accuracy: 0.9980 - val_loss: 7.0216 - val_accuracy: 0.5867\n",
      "Epoch 925/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0069 - accuracy: 0.9983 - val_loss: 6.9920 - val_accuracy: 0.5888\n",
      "Epoch 926/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0081 - accuracy: 0.9980 - val_loss: 7.0126 - val_accuracy: 0.5898\n",
      "Epoch 927/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0104 - accuracy: 0.9983 - val_loss: 7.0685 - val_accuracy: 0.5939\n",
      "Epoch 928/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0085 - accuracy: 0.9976 - val_loss: 6.9882 - val_accuracy: 0.5949\n",
      "Epoch 929/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0073 - accuracy: 0.9986 - val_loss: 7.0917 - val_accuracy: 0.5888\n",
      "Epoch 930/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0087 - accuracy: 0.9980 - val_loss: 7.0311 - val_accuracy: 0.5908\n",
      "Epoch 931/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0093 - accuracy: 0.9980 - val_loss: 7.0784 - val_accuracy: 0.5908\n",
      "Epoch 932/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0122 - accuracy: 0.9966 - val_loss: 7.0554 - val_accuracy: 0.5888\n",
      "Epoch 933/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0124 - accuracy: 0.9969 - val_loss: 7.0555 - val_accuracy: 0.5908\n",
      "Epoch 934/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0150 - accuracy: 0.9969 - val_loss: 6.9662 - val_accuracy: 0.5908\n",
      "Epoch 935/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0111 - accuracy: 0.9980 - val_loss: 7.2015 - val_accuracy: 0.5847\n",
      "Epoch 936/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0155 - accuracy: 0.9959 - val_loss: 6.9619 - val_accuracy: 0.5908\n",
      "Epoch 937/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0108 - accuracy: 0.9966 - val_loss: 7.0487 - val_accuracy: 0.5867\n",
      "Epoch 938/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0094 - accuracy: 0.9976 - val_loss: 7.0728 - val_accuracy: 0.5867\n",
      "Epoch 939/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0083 - accuracy: 0.9976 - val_loss: 7.0420 - val_accuracy: 0.5908\n",
      "Epoch 940/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0093 - accuracy: 0.9976 - val_loss: 7.0715 - val_accuracy: 0.5949\n",
      "Epoch 941/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0088 - accuracy: 0.9980 - val_loss: 7.0259 - val_accuracy: 0.5908\n",
      "Epoch 942/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0073 - accuracy: 0.9980 - val_loss: 7.0722 - val_accuracy: 0.5898\n",
      "Epoch 943/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0075 - accuracy: 0.9973 - val_loss: 7.1058 - val_accuracy: 0.5908\n",
      "Epoch 944/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0069 - accuracy: 0.9980 - val_loss: 7.0700 - val_accuracy: 0.5878\n",
      "Epoch 945/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0063 - accuracy: 0.9983 - val_loss: 7.0814 - val_accuracy: 0.5878\n",
      "Epoch 946/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0068 - accuracy: 0.9980 - val_loss: 7.1033 - val_accuracy: 0.5888\n",
      "Epoch 947/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0071 - accuracy: 0.9983 - val_loss: 7.1114 - val_accuracy: 0.5898\n",
      "Epoch 948/10000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0081 - accuracy: 0.9983 - val_loss: 7.0717 - val_accuracy: 0.5918\n",
      "Epoch 949/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.0081 - accuracy: 0.9983 - val_loss: 7.1539 - val_accuracy: 0.5878\n",
      "Epoch 950/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0081 - accuracy: 0.9973 - val_loss: 7.0409 - val_accuracy: 0.5857\n",
      "Epoch 951/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0087 - accuracy: 0.9976 - val_loss: 7.1108 - val_accuracy: 0.5898\n",
      "Epoch 952/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0073 - accuracy: 0.9976 - val_loss: 7.1349 - val_accuracy: 0.5888\n",
      "Epoch 953/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0070 - accuracy: 0.9976 - val_loss: 7.1211 - val_accuracy: 0.5857\n",
      "Epoch 954/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0070 - accuracy: 0.9983 - val_loss: 7.1401 - val_accuracy: 0.5908\n",
      "Epoch 955/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0087 - accuracy: 0.9980 - val_loss: 7.1463 - val_accuracy: 0.5878\n",
      "Epoch 956/10000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0074 - accuracy: 0.9973 - val_loss: 7.1415 - val_accuracy: 0.5857\n",
      "Epoch 957/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0104 - accuracy: 0.9976 - val_loss: 7.1301 - val_accuracy: 0.5878\n",
      "Epoch 958/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0097 - accuracy: 0.9973 - val_loss: 7.1887 - val_accuracy: 0.5898\n",
      "Epoch 959/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.0084 - accuracy: 0.9973 - val_loss: 7.1565 - val_accuracy: 0.5908\n",
      "Epoch 960/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0099 - accuracy: 0.9973 - val_loss: 7.1950 - val_accuracy: 0.5908\n",
      "Epoch 961/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0087 - accuracy: 0.9983 - val_loss: 7.1839 - val_accuracy: 0.5908\n",
      "Epoch 962/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0084 - accuracy: 0.9976 - val_loss: 7.1290 - val_accuracy: 0.5888\n",
      "Epoch 963/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0086 - accuracy: 0.9976 - val_loss: 7.1633 - val_accuracy: 0.5898\n",
      "Epoch 964/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0082 - accuracy: 0.9980 - val_loss: 7.1587 - val_accuracy: 0.5888\n",
      "Epoch 965/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0080 - accuracy: 0.9983 - val_loss: 7.1779 - val_accuracy: 0.5908\n",
      "Epoch 966/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0066 - accuracy: 0.9986 - val_loss: 7.1655 - val_accuracy: 0.5878\n",
      "Epoch 967/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0076 - accuracy: 0.9980 - val_loss: 7.2191 - val_accuracy: 0.5898\n",
      "Epoch 968/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0069 - accuracy: 0.9983 - val_loss: 7.1635 - val_accuracy: 0.5867\n",
      "Epoch 969/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0066 - accuracy: 0.9976 - val_loss: 7.2128 - val_accuracy: 0.5918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 970/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0067 - accuracy: 0.9980 - val_loss: 7.2117 - val_accuracy: 0.5878\n",
      "Epoch 971/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0069 - accuracy: 0.9980 - val_loss: 7.1579 - val_accuracy: 0.5867\n",
      "Epoch 972/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0084 - accuracy: 0.9976 - val_loss: 7.2488 - val_accuracy: 0.5908\n",
      "Epoch 973/10000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.0065 - accuracy: 0.9986 - val_loss: 7.1844 - val_accuracy: 0.5878\n",
      "Epoch 974/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0083 - accuracy: 0.9983 - val_loss: 7.2408 - val_accuracy: 0.5898\n",
      "Epoch 975/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0059 - accuracy: 0.9990 - val_loss: 7.2466 - val_accuracy: 0.5878\n",
      "Epoch 976/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0072 - accuracy: 0.9980 - val_loss: 7.2015 - val_accuracy: 0.5847\n",
      "Epoch 977/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0078 - accuracy: 0.9983 - val_loss: 7.2334 - val_accuracy: 0.5888\n",
      "Epoch 978/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0079 - accuracy: 0.9980 - val_loss: 7.2197 - val_accuracy: 0.5898\n",
      "Epoch 979/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0118 - accuracy: 0.9980 - val_loss: 7.2476 - val_accuracy: 0.5929\n",
      "Epoch 980/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0127 - accuracy: 0.9966 - val_loss: 7.1533 - val_accuracy: 0.5888\n",
      "Epoch 981/10000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0143 - accuracy: 0.9973 - val_loss: 7.3019 - val_accuracy: 0.5878\n",
      "Epoch 982/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0126 - accuracy: 0.9973 - val_loss: 7.1411 - val_accuracy: 0.5837\n",
      "Epoch 983/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0133 - accuracy: 0.9969 - val_loss: 7.1722 - val_accuracy: 0.5878\n",
      "Epoch 984/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0077 - accuracy: 0.9983 - val_loss: 7.2098 - val_accuracy: 0.5898\n",
      "Epoch 985/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0079 - accuracy: 0.9983 - val_loss: 7.2165 - val_accuracy: 0.5857\n",
      "Epoch 986/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0069 - accuracy: 0.9986 - val_loss: 7.2357 - val_accuracy: 0.5908\n",
      "Epoch 987/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0075 - accuracy: 0.9969 - val_loss: 7.2065 - val_accuracy: 0.5867\n",
      "Epoch 988/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0069 - accuracy: 0.9983 - val_loss: 7.2537 - val_accuracy: 0.5878\n",
      "Epoch 989/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0071 - accuracy: 0.9976 - val_loss: 7.2204 - val_accuracy: 0.5878\n",
      "Epoch 990/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0062 - accuracy: 0.9983 - val_loss: 7.2385 - val_accuracy: 0.5867\n",
      "Epoch 991/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0082 - accuracy: 0.9976 - val_loss: 7.2166 - val_accuracy: 0.5847\n",
      "Epoch 992/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0099 - accuracy: 0.9973 - val_loss: 7.3264 - val_accuracy: 0.5898\n",
      "Epoch 993/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0093 - accuracy: 0.9973 - val_loss: 7.1642 - val_accuracy: 0.5939\n",
      "Epoch 994/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0069 - accuracy: 0.9990 - val_loss: 7.3495 - val_accuracy: 0.5878\n",
      "Epoch 995/10000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0095 - accuracy: 0.9969 - val_loss: 7.1912 - val_accuracy: 0.5908\n",
      "Epoch 996/10000\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0093 - accuracy: 0.9976 - val_loss: 7.3980 - val_accuracy: 0.5898\n",
      "Epoch 997/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0094 - accuracy: 0.9969 - val_loss: 7.1949 - val_accuracy: 0.5857\n",
      "Epoch 998/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0064 - accuracy: 0.9986 - val_loss: 7.2917 - val_accuracy: 0.5898\n",
      "Epoch 999/10000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0083 - accuracy: 0.9976 - val_loss: 7.2576 - val_accuracy: 0.5878\n",
      "Epoch 1000/10000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0079 - accuracy: 0.9983 - val_loss: 7.3430 - val_accuracy: 0.5908\n",
      "Epoch 1001/10000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0072 - accuracy: 0.9983 - val_loss: 7.2654 - val_accuracy: 0.5867\n"
     ]
    }
   ],
   "source": [
    "model1.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=1000)\n",
    "modelpath = \"./model/wine_model{epoch:0003d}__{val_loss:.4f}.keras\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=0, save_best_only=True)\n",
    "history = model1.fit(X_train, y_train, epochs=10000, batch_size=500, validation_data=(X_valid, y_valid), callbacks=[early_stop, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6010fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_best_model = load_model(\"./model/wine_model019__1.0356.keras\")\n",
    "pred = wine_best_model.predict(X_test)\n",
    "pred = pd.DataFrame(pred)\n",
    "pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d18cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 행에서 확률이 가장 높은 클래스의 인덱스를 찾기\n",
    "predicted_classes = pred.idxmax(axis=1)\n",
    "\n",
    "# 결과 출력\n",
    "print(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b9f624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 행에서 확률이 가장 높은 클래스의 인덱스를 찾기\n",
    "y_classes = y_test.idxmax(axis=1)\n",
    "\n",
    "# 결과 출력\n",
    "print(y_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dbaa8b1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4590</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3761</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3899</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4609</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>980 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          3      4      5      6      7      8      9\n",
       "1815  False  False  False   True  False  False  False\n",
       "4590  False  False  False   True  False  False  False\n",
       "452   False  False  False  False   True  False  False\n",
       "3761  False  False  False  False   True  False  False\n",
       "3899  False  False   True  False  False  False  False\n",
       "...     ...    ...    ...    ...    ...    ...    ...\n",
       "1731  False  False  False   True  False  False  False\n",
       "4609  False   True  False  False  False  False  False\n",
       "460   False  False   True  False  False  False  False\n",
       "301   False  False  False   True  False  False  False\n",
       "2708  False  False  False   True  False  False  False\n",
       "\n",
       "[980 rows x 7 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f8cea310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.01      0.75      0.01         4\n",
      "           4       0.01      0.03      0.01        33\n",
      "           5       0.00      0.00      0.00       291\n",
      "           6       0.00      0.00      0.00       440\n",
      "           7       0.00      0.00      0.00       176\n",
      "           8       0.00      0.00      0.00        35\n",
      "           9       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.00       980\n",
      "   macro avg       0.00      0.09      0.00       980\n",
      "weighted avg       0.00      0.00      0.00       980\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniforge3/envs/dml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/user/miniforge3/envs/dml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/user/miniforge3/envs/dml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/user/miniforge3/envs/dml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/user/miniforge3/envs/dml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/user/miniforge3/envs/dml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_classes, predicted_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "295c4bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3651c3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8aa24902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, ..., 3, 4, 3])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_labeled = le.fit_transform(y2)\n",
    "y_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a6142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "131bd93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_valid2, y_train2, y_valid2 = train_test_split(X_scaled, y_labeled, test_size=0.4, stratify=y_labeled ,random_state=10)\n",
    "X_valid2, X_test2, y_valid2, y_test2 = train_test_split(X_valid2, y_valid2, test_size=0.5, stratify=y_valid2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f101f8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "86c20e76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b80df151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6, 5, 4, 6, 6, 7, 5, 6, 6, 6, 6, 7, 6, 6, 7, 6, 5, 7, 6, 5, 6,\n",
       "       6, 5, 5, 4, 7, 6, 6, 6, 6, 5, 5, 6, 7, 6, 5, 5, 6, 7, 7, 6, 5, 7,\n",
       "       5, 6, 7, 5, 6, 5, 6, 6, 6, 5, 6, 6, 7, 5, 5, 7, 6, 6, 6, 6, 5, 6,\n",
       "       5, 6, 6, 5, 6, 6, 5, 5, 7, 7, 5, 6, 6, 6, 7, 7, 5, 8, 7, 5, 6, 7,\n",
       "       7, 5, 7, 7, 5, 6, 6, 5, 6, 7, 7, 5, 6, 6, 5, 5, 4, 5, 5, 5, 6, 5,\n",
       "       5, 5, 6, 5, 7, 5, 7, 6, 6, 5, 6, 7, 5, 7, 6, 6, 7, 7, 7, 4, 6, 6,\n",
       "       6, 6, 5, 4, 7, 5, 5, 7, 7, 6, 6, 7, 5, 7, 7, 5, 6, 6, 5, 7, 7, 6,\n",
       "       6, 8, 6, 6, 5, 7, 6, 6, 6, 8, 5, 7, 6, 5, 6, 6, 5, 6, 6, 6, 7, 6,\n",
       "       6, 5, 7, 7, 6, 6, 5, 7, 7, 6, 6, 7, 5, 6, 6, 6, 6, 6, 6, 6, 8, 6,\n",
       "       6, 7, 5, 8, 7, 6, 7, 5, 6, 5, 6, 5, 6, 6, 6, 6, 8, 6, 6, 7, 6, 6,\n",
       "       6, 5, 6, 7, 6, 5, 7, 7, 6, 8, 7, 8, 5, 3, 5, 5, 7, 7, 5, 7, 6, 5,\n",
       "       6, 6, 8, 7, 5, 6, 5, 5, 6, 7, 6, 6, 6, 6, 3, 5, 6, 6, 6, 6, 5, 7,\n",
       "       6, 6, 4, 5, 7, 6, 6, 6, 6, 4, 7, 5, 7, 7, 5, 6, 6, 5, 6, 7, 7, 6,\n",
       "       6, 6, 5, 7, 6, 6, 6, 6, 5, 6, 6, 7, 5, 7, 6, 6, 7, 6, 7, 5, 6, 7,\n",
       "       6, 6, 6, 5, 7, 6, 6, 6, 5, 7, 6, 5, 6, 6, 4, 6, 6, 6, 5, 5, 5, 6,\n",
       "       6, 6, 6, 6, 7, 7, 5, 7, 5, 5, 6, 8, 6, 7, 6, 7, 4, 6, 6, 6, 5, 6,\n",
       "       6, 6, 7, 8, 7, 5, 8, 6, 6, 6, 6, 6, 6, 6, 5, 6, 5, 5, 5, 6, 5, 8,\n",
       "       6, 6, 6, 7, 5, 7, 7, 6, 7, 5, 6, 7, 6, 5, 6, 5, 6, 5, 7, 7, 7, 5,\n",
       "       6, 6, 3, 4, 5, 8, 6, 6, 6, 5, 6, 5, 5, 6, 6, 6, 6, 6, 7, 6, 5, 5,\n",
       "       6, 5, 7, 6, 6, 6, 5, 6, 6, 5, 7, 6, 5, 6, 7, 6, 6, 5, 5, 6, 6, 6,\n",
       "       5, 6, 5, 5, 6, 5, 6, 5, 5, 7, 5, 6, 5, 5, 6, 6, 7, 5, 4, 7, 7, 6,\n",
       "       7, 6, 6, 7, 8, 7, 6, 6, 6, 7, 4, 4, 4, 6, 5, 6, 5, 6, 6, 6, 5, 6,\n",
       "       6, 5, 6, 6, 6, 5, 8, 8, 5, 7, 5, 6, 5, 5, 5, 5, 5, 6, 6, 6, 6, 5,\n",
       "       5, 7, 5, 5, 6, 6, 9, 6, 6, 6, 6, 5, 5, 7, 4, 6, 7, 4, 7, 4, 5, 6,\n",
       "       6, 6, 5, 7, 6, 6, 5, 6, 5, 6, 7, 5, 5, 6, 5, 7, 5, 7, 5, 6, 5, 6,\n",
       "       6, 6, 5, 7, 7, 5, 5, 7, 7, 6, 6, 5, 6, 4, 7, 5, 6, 8, 5, 7, 6, 5,\n",
       "       4, 5, 7, 7, 7, 5, 5, 7, 5, 5, 6, 5, 5, 6, 7, 6, 5, 6, 5, 5, 7, 6,\n",
       "       6, 6, 7, 6, 6, 6, 6, 5, 6, 7, 5, 7, 6, 7, 7, 6, 6, 5, 5, 6, 7, 6,\n",
       "       5, 6, 4, 5, 6, 5, 6, 4, 6, 5, 5, 7, 6, 5, 5, 6, 7, 8, 5, 7, 6, 6,\n",
       "       6, 6, 7, 6, 6, 6, 5, 8, 7, 6, 5, 6, 7, 5, 5, 8, 7, 5, 5, 5, 5, 6,\n",
       "       6, 5, 6, 5, 6, 5, 6, 6, 8, 5, 7, 6, 5, 5, 5, 6, 6, 7, 5, 7, 5, 6,\n",
       "       5, 5, 5, 7, 6, 6, 6, 4, 4, 5, 6, 5, 5, 7, 5, 5, 5, 6, 6, 6, 6, 5,\n",
       "       6, 7, 5, 6, 5, 6, 5, 5, 6, 6, 6, 6, 5, 6, 5, 7, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 7, 6, 6, 5, 5, 6, 6, 4, 4, 6, 5, 5, 5, 8, 4, 6, 5, 6, 6,\n",
       "       5, 5, 5, 6, 6, 5, 5, 8, 5, 7, 5, 8, 5, 6, 5, 5, 6, 8, 6, 6, 6, 5,\n",
       "       6, 5, 5, 6, 5, 6, 6, 5, 6, 6, 6, 6, 6, 7, 5, 6, 6, 6, 5, 5, 7, 6,\n",
       "       6, 7, 6, 5, 5, 6, 6, 5, 6, 5, 7, 5, 5, 6, 6, 6, 5, 6, 7, 6, 6, 7,\n",
       "       6, 8, 7, 6, 6, 5, 6, 7, 6, 6, 6, 5, 5, 6, 5, 4, 7, 6, 5, 6, 5, 5,\n",
       "       5, 7, 6, 4, 6, 5, 7, 6, 6, 6, 6, 6, 7, 5, 6, 5, 5, 6, 5, 5, 6, 3,\n",
       "       5, 6, 8, 7, 5, 7, 6, 6, 5, 6, 5, 5, 5, 6, 6, 6, 6, 5, 7, 5, 7, 6,\n",
       "       5, 5, 7, 8, 6, 8, 6, 5, 6, 5, 6, 8, 6, 5, 5, 5, 7, 5, 6, 7, 6, 5,\n",
       "       7, 6, 6, 6, 6, 6, 6, 5, 6, 5, 5, 6, 5, 7, 5, 8, 6, 6, 6, 4, 7, 6,\n",
       "       7, 7, 6, 5, 5, 6, 7, 6, 6, 6, 6, 7, 5, 6, 8, 6, 8, 6, 4, 5, 6, 7,\n",
       "       6, 7, 5, 5, 8, 6, 6, 6, 7, 6, 6, 6, 5, 6, 6, 5, 6, 5, 5, 6, 6, 7,\n",
       "       6, 5, 6, 7, 6, 6, 7, 5, 6, 4, 4, 5])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "398565c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       0.00      0.00      0.00        32\n",
      "           5       0.32      0.30      0.31       292\n",
      "           6       0.47      0.51      0.49       440\n",
      "           7       0.21      0.21      0.21       176\n",
      "           8       0.10      0.06      0.07        35\n",
      "           9       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.36       980\n",
      "   macro avg       0.16      0.16      0.16       980\n",
      "weighted avg       0.35      0.36      0.35       980\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniforge3/envs/dml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/user/miniforge3/envs/dml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/user/miniforge3/envs/dml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(max_depth=5, n_estimators=1000, n_jobs=-1, random_state=10)\n",
    "xgb.fit(X_train2, y_train2)\n",
    "xgb_pred = xgb.predict(X_valid2)\n",
    "print(classification_report(le.inverse_transform(y_test2), le.inverse_transform(xgb_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221865d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7528f23d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
